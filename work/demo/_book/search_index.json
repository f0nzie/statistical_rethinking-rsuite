[
["index.html", "Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.0.1 This is a love letter Why this? My assumptions about you How to use and understand this project You can do this, too We have updates", " Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.0.1 A Solomon Kurz 2019-06-19 This is a love letter I love McElreath’s Statistical Rethinking text. It’s the entry-level textbook for applied researchers I spent years looking for. McElreath’s freely-available lectures on the book are really great, too. However, I prefer using Bürkner’s brms package when doing Bayesian regression in R. It’s just spectacular. I also prefer plotting with Wickham’s ggplot2, and coding with functions and principles from the tidyverse, which you might learn about here or here. So, this project is an attempt to reexpress the code in McElreath’s textbook. His models are re-fit with brms, the figures are reproduced or reimagined with ggplot2, and the general data wrangling code now predominantly follows the tidyverse style. Why this? I’m not a statistician and I have no formal background in computer science. Though I benefited from a suite of statistics courses in grad school, a large portion of my training has been outside of the classroom, working with messy real-world data, and searching online for help. One of the great resources I happened on was idre, the UCLA Institute for Digital Education, which offers an online portfolio of richly annotated textbook examples. Their online tutorials are among the earliest inspirations for this project. We need more resources like them. With that in mind, one of the strengths of McElreath’s text is its thorough integration with the rethinking package. The rethinking package is a part of the R ecosystem, which is great because R is free and open source. And McElreath has made the source code for rethinking publically available, too. Since he completed his text, many other packages have been developed to help users of the R ecosystem interface with Stan. Of those alternative packages, I think Bürkner’s brms is the best for general-purpose Bayesian data analysis. It’s flexible, uses reasonably-approachable syntax, has sensible defaults, and offers a vast array of post-processing convenience functions. And brms has only gotten better over time. To my knowledge, there are no textbooks on the market that highlight the brms package, which seems like an evil worth correcting. In addition, McElreath’s data wrangling code is based in the base R style and he made most of his figures with base R plots. Though there are benefits to sticking close to base R functions (e.g., less dependencies leading to a lower likelihood that your code will break in the future), there are downsides. For beginners, base R functions can be difficult both to learn and to read. Happily, in recent years Hadley Wickham and others have been developing a group of packages collectively called the tidyverse. These tidyverse packages (e.g., dplyr, tidyr, purrr) were developed according to an underlying philosophy and they are designed to work together coherently and seamlessly. Though not all within the R community share this opinion, I am among those who think the tydyverse style of coding is generally easier to learn and sufficiently powerful that these packages can accommodate the bulk of your data needs. I also find tydyverse-style syntax easier to read. And of course, the widely-used ggplot2 package is part of the tidyverse, too. To be clear, students can get a great education in both Bayesian statistics and programming in R with McElreath’s text just the way it is. Just go slow, work through all the examples, and read the text closely. It’s a pedagogical boon. I could not have done better or even closely so. But what I can offer is a parallel introduction on how to fit the statistical models with the ever-improving and already-quite-impressive brms package. I can throw in examples of how to perform other operations according to the ethic of the tidyverse. And I can also offer glimpses of some of the other great packages in the R + Stan ecosystem, such as loo, bayesplot, and tidybayes. My assumptions about you If you’re looking at this project, I’m guessing you’re either a graduate student, a post-graduate academic, or a researcher of some sort. So I’m presuming you have at least a 101-level foundation in statistics. If you’re rusty, consider checking out Legler and Roback’s free bookdown text, Broadening Your Statistical Horizons before diving into Statistical Rethinking. I’m also assuming you understand the rudiments of R and have at least a vague idea about what the tidyverse is. If you’re totally new to R, consider starting with Peng’s R Programming for Data Science. And the best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s R for Data Science, which I extensively link to throughout this project. That said, you do not need to be totally fluent in statistics or R. Otherwise why would you need this project, anyway? IMO, the most important things are curiosity, a willingness to try, and persistent tinkering. I love this stuff. Hopefully you will, too. How to use and understand this project This project is not meant to stand alone. It’s a supplement to McElreath’s Statistical Rethinking text. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, some of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project will sometimes be blank or omitted, though I do highlight some of the important points in quotes and prose of my own. So I imagine students might reference this project as they progress through McElreath’s text. I also imagine working data analysts might use this project in conjunction with the text as they flip to the specific sections that seem relevant to solving their data challenges. I reproduce the bulk of the figures in the text, too. The plots in the first few chapters are the closest to those in the text. However, I’m passionate about data visualization and like to play around with color palettes, formatting templates, and other conventions quite a bit. As a result, the plots in each chapter have their own look and feel. For more on some of these topics, check out chapters 3, 7, and 28 in R4DS, Healy’s Data Visualization: A practical introduction, or Wilke’s Fundamentals of Data Visualization. In this project, I use a handful of formatting conventions gleaned from R4DS, The tidyverse style guide, and R Markdown: The Definitive Guide. R code blocks and their output appear in a gray background. E.g., 2 + 2 == 5 ## [1] FALSE Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit the package a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidybayes::mode_hdi()). R objects, such as data or function arguments, are in typewriter font atop gray backgrounds (e.g., chimpanzees, .width = .5). You can detect hyperlinks by their typical blue-colored font. In the text, McElreath indexed his models with names like m4.1 (i.e., the first model of Chapter 4). I primarily followed that convention, but replaced the m with a b to stand for the brms package. You can do this, too This project is powered by Yihui Xie’s bookdown package, which makes it easy to turn R markdown files into HTML, PDF, and EPUB. Go here to learn more about bookdown. While you’re at it, also check out Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide. And if you’re unacquainted with GitHub, check out Jenny Bryan’s Happy Git and GitHub for the useR. I’ve even blogged about what it was like putting together the first version of this project. The source code of the project is available here. We have updates I released the initial 0.9.0 version of this project in September 26, 2018. In April 19, 2019 came the 1.0.0 version. Some of the major changes were: All models were refit with the current official version of brms, 2.8.0. Adopting the seed argument within the brm() function made the model results more reproducible. The loo package was updated. As a consequence, our workflow for the WAIC and LOO changed, too. I improved the brms alternative to McElreath’s coeftab() function. I made better use of the tidyverse, especially some of the purrr functions. Particularly in the later chapters, there’s a greater emphasis on functions from the tidybayes package. Chapter 11 contains the updated brms 2.8.0 workflow for making custom distributions, using the beta-binomial model as the example. Chapter 12 received a new bonus section contrasting different methods for working with multilevel posteriors. Chapter 14 received a new bonus section introducing Bayesian meta-analysis and linking it to multilevel and measurement-error models. With the help of others within the community, I corrected many typos and streamlined some of the code (e.g., dropped an unnecessary use of the mi() function in section 14.2.1) And in some cases, I corrected sections that were just plain wrong (e.g., some of my initial attempts in section 3.3 were incorrect). In response to some reader requests, we finally have a PDF version! Making that happen required some formatting adjustments, resulting in version 1.0.1. Noteworthy changes include: Major revisions to the LaTeX syntax underlying many of the in-text equations (e.g., dropping the “eqnarray” environment for &quot;align*&quot;) Adjusting some of the image syntax Updating the reference for the Bayesian \\(R^2\\) (Gelman, Goodrich, Gabry, &amp; Vehtari, 2018) Though we’re into version 1.0.1, there’s room for improvement. There are still two models that need work. The current solution for model 10.6 is wrong, which I try to make clear in the prose. It also appears that the Gaussian process model from section 13.4 is off. Both models are beyond my current skill set and friendly suggestions are welcome. In addition to modeling concerns, typos may yet be looming and I’m sure there are places where the code could be made more streamlined, more elegant, or just more in-line with the tidyverse style. Which is all to say, I hope to release better and more useful updates in the future. Before we move on, I’d like to thank the following for their helpful contributions: Paul-Christian Bürkner (@paul-buerkner), Andrew Collier (@datawookie), Jeff Hammerbacher (@hammer), Matthew Kay (@mjskay), TJ Mahr (@tjmahr), Stijn Masschelein (@stijnmasschelein), Colin Quirk (@colinquirk), Rishi Sadhir (@RishiSadhir), Richard Torkar (@torkar), Aki Vehtari (@avehtari). "],
["the-golem-of-prague.html", "Chapter 1 The Golem of Prague 1 Statistical golems Reference Session info", " Chapter 1 The Golem of Prague Figure 1.1: Rabbi Loew and Golem by Mikoláš Aleš, 1899 As he opened the chapter, McElreath told us that ultimately Judah was forced to destroy the golem, as its combination of extraordinary power with clumsiness eventually led to innocent deaths. Wiping away one letter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned the robot. 1 Statistical golems Scientists also make golems. Our golems rarely have physical form, but they too are often made of clay, living in silicon as computer code. These golems are scientific model. But these golems have real effects on the world, through the predictions they make and the intuitions they challenge or inspire. A concern with truth enlivens these models, but just like a golem or a modern robot, scientific models are neither true nor false, neither prophets nor charlatans. Rather they are constructs engineered for some purpose. These constructs are incredibly powerful, dutifully conducting their programmed calculations. (p. 1, emphasis in the original) There are a lot of great points, themes, methods, and factoids in this text. For me, one of the most powerful themes interlaced throughout the pages is how we should be skeptical of our models. Yes, learn Bayes. Pour over this book. Fit models until late into the night. But please don’t fall into blind love with their elegance and power. If we all knew what we were doing, there’d be no need for science. For more wise deflation along these lines, do check out A personal essay on Bayes factors, Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection and Science, statistics and the problem of “pretty good inference”, a blog, paper and talk by the inimitable Danielle Navarro. Anyway, McElreath left us no code or figures to translate in this chapter. But before you skip off to the next one, why not invest a little time soaking in this chapter’s material by watching McElreath present it? He’s an engaging speaker and the material in his online lectures does not entirely overlap with that in the text. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.1 bookdown_0.11 digest_0.6.19 crayon_1.3.4 ## [5] rprojroot_1.3-2 backports_1.1.4 pacman_0.5.1 magrittr_1.5 ## [9] evaluate_0.14 highr_0.8 stringi_1.4.3 rstudioapi_0.10 ## [13] rmarkdown_1.13 tools_3.6.0 stringr_1.4.0 xfun_0.7 ## [17] yaml_2.2.0 compiler_3.6.0 htmltools_0.3.6 knitr_1.23 "],
["small-worlds-and-large-worlds.html", "Chapter 2 Small Worlds and Large Worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model 2.4 Making the model go Reference Session info", " Chapter 2 Small Worlds and Large Worlds A while back The Oatmeal put together an infographic on Christopher Columbus. I’m no historian and cannot vouch for its accuracy, so make of it what you will. McElreath described the thrust of this chapter this way: In this chapter, you will begin to build Bayesian models. The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20) Indeed. 2.1 The garden of forking data Gelman and Loken wrote a great paper by this name. 2.1.1 Counting possibilities. Throughout this project, we’ll use the tidyverse for data wrangling. library(tidyverse) If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in section 5.6.1 from Grolemund and Wickham’s R for Data Science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2. Other than the pipe, the other big thing to be aware of is tibbles. For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. And importantly, tibbles are just special types of data frames. So whenever we talk about data frames, we’re also talking about tibbles. For more on the topic, check out R4SD, Chapter 10. So, if we’re willing to code the marbles as 0 = “white” 1 = “blue”, we can arrange the possibility data in a tibble as follows. d &lt;- tibble(p_1 = 0, p_2 = rep(1:0, times = c(1, 3)), p_3 = rep(1:0, times = c(2, 2)), p_4 = rep(1:0, times = c(3, 1)), p_5 = 1) head(d) ## # A tibble: 4 x 5 ## p_1 p_2 p_3 p_4 p_5 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1 1 1 1 ## 2 0 0 1 1 1 ## 3 0 0 0 1 1 ## 4 0 0 0 0 1 You might depict the possibility data in a plot. d %&gt;% gather() %&gt;% mutate(x = rep(1:4, times = 5), possibility = rep(1:5, each = 4)) %&gt;% ggplot(aes(x = x, y = possibility, fill = value %&gt;% as.character())) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(.75, 4.25), ylim = c(.75, 5.25)) + theme(legend.position = &quot;none&quot;) As a quick aside, check out Suzan Baert’s blog post Data Wrangling Part 2: Transforming your columns into the right shape for an extensive discussion on dplyr::mutate() and dplyr::gather(). Here’s the basic structure of the possibilities per marble draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) %&gt;% knitr::kable() draw marbles possibilities 1 4 4 2 4 16 3 4 64 If you walk that out a little, you can structure the data required to approach Figure 2.2. ( d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3)), fill = rep(c(&quot;b&quot;, &quot;w&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2))) ) ## # A tibble: 84 x 3 ## position draw fill ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 b ## 2 2 1 w ## 3 3 1 w ## 4 4 1 w ## 5 0.25 2 b ## 6 0.5 2 w ## 7 0.75 2 w ## 8 1 2 w ## 9 1.25 2 b ## 10 1.5 2 w ## # … with 74 more rows See what I did there with the parentheses? If you assign a value to an object in R (e.g., dog &lt;- 1) and just hit return, nothing will immediately pop up in the console. You have to actually execute dog before R will return 1. But if you wrap the code within parentheses (e.g., (dog &lt;- 1)), R will perform the assignment and return the value as if you had executed dog. But we digress. Here’s the initial plot. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) To my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles. # these will connect the dots from the first and second draws ( lines_1 &lt;- tibble(x = rep((1:4), each = 4), xend = ((1:4^2) / 4), y = 1, yend = 2) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1 2 ## 2 1 0.5 1 2 ## 3 1 0.75 1 2 ## 4 1 1 1 2 ## 5 2 1.25 1 2 ## 6 2 1.5 1 2 ## 7 2 1.75 1 2 ## 8 2 2 1 2 ## 9 3 2.25 1 2 ## 10 3 2.5 1 2 ## 11 3 2.75 1 2 ## 12 3 3 1 2 ## 13 4 3.25 1 2 ## 14 4 3.5 1 2 ## 15 4 3.75 1 2 ## 16 4 4 1 2 # these will connect the dots from the second and third draws ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4), xend = (1:4^3) / (4^2), y = 2, yend = 3) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.25 0.0625 2 3 ## 2 0.25 0.125 2 3 ## 3 0.25 0.188 2 3 ## 4 0.25 0.25 2 3 ## 5 0.5 0.312 2 3 ## 6 0.5 0.375 2 3 ## 7 0.5 0.438 2 3 ## 8 0.5 0.5 2 3 ## 9 0.75 0.562 2 3 ## 10 0.75 0.625 2 3 ## # … with 54 more rows We can use the lines_1 and lines_2 data in the plot with two geom_segment() functions. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that. d &lt;- d %&gt;% mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) d ## # A tibble: 84 x 4 ## position draw fill denominator ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.5 1 b 0.5 ## 2 1.5 1 w 0.5 ## 3 2.5 1 w 0.5 ## 4 3.5 1 w 0.5 ## 5 0.125 2 b 0.125 ## 6 0.375 2 w 0.125 ## 7 0.625 2 w 0.125 ## 8 0.875 2 w 0.125 ## 9 1.12 2 b 0.125 ## 10 1.38 2 w 0.125 ## # … with 74 more rows We’ll follow the same logic for the lines_1 and lines_2 data. ( lines_1 &lt;- lines_1 %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 0.125 1 2 ## 2 0.5 0.375 1 2 ## 3 0.5 0.625 1 2 ## 4 0.5 0.875 1 2 ## 5 1.5 1.12 1 2 ## 6 1.5 1.38 1 2 ## 7 1.5 1.62 1 2 ## 8 1.5 1.88 1 2 ## 9 2.5 2.12 1 2 ## 10 2.5 2.38 1 2 ## 11 2.5 2.62 1 2 ## 12 2.5 2.88 1 2 ## 13 3.5 3.12 1 2 ## 14 3.5 3.38 1 2 ## 15 3.5 3.62 1 2 ## 16 3.5 3.88 1 2 ( lines_2 &lt;- lines_2 %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.0312 2 3 ## 2 0.125 0.0938 2 3 ## 3 0.125 0.156 2 3 ## 4 0.125 0.219 2 3 ## 5 0.375 0.281 2 3 ## 6 0.375 0.344 2 3 ## 7 0.375 0.406 2 3 ## 8 0.375 0.469 2 3 ## 9 0.625 0.531 2 3 ## 10 0.625 0.594 2 3 ## # … with 54 more rows Now the plot’s looking closer. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) For the final step, we’ll use coord_polar() to change the coordinate system, giving the plot a mandala-like feel. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 4) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() To make our version of Figure 2.3, we’ll have to add an index to tell us which paths remain logically valid after each choice. We’ll call the index remain. lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0:1, times = c(1, 3)), rep(0, times = 4 * 3))) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d &lt;- d %&gt;% mutate(remain = c(rep(1:0, times = c(1, 3)), rep(0:1, times = c(1, 3)), rep(0, times = 4 * 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) # finally, the plot: d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, alpha = remain %&gt;% as.character()), shape = 21, size = 4) + # it&#39;s the alpha parameter that makes elements semitransparent scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() Letting “w” = a white dot and “b” = a blue dot, we might recreate the table in the middle of page 23 like so. # if we make two custom functions, here, it will simplify the code within `mutate()`, below n_blue &lt;- function(x){ rowSums(x == &quot;b&quot;) } n_white &lt;- function(x){ rowSums(x == &quot;w&quot;) } t &lt;- # for the first four columns, `p_` indexes position tibble(p_1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), p_2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), p_3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), p_4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(`draw 1: blue` = n_blue(.), `draw 2: white` = n_white(.), `draw 3: blue` = n_blue(.)) %&gt;% mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 draw 1: blue draw 2: white draw 3: blue ways to produce w w w w 0 4 0 0 b w w w 1 3 1 3 b b w w 2 2 2 8 b b b w 3 1 3 9 b b b b 4 0 4 0 We’ll need new data for Figure 2.4. Here’s the initial primary data, d. d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3))) ( d &lt;- d %&gt;% bind_rows( d, d ) %&gt;% # here are the fill colors mutate(fill = c(rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), each = 2) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 1)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)))) %&gt;% # now we need to shift the positions over in accordance with draw, like before mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(position = ifelse(pie_index == &quot;a&quot;, position, ifelse(pie_index == &quot;b&quot;, position + 4, position + 4 * 2))) ) ## # A tibble: 252 x 5 ## position draw fill denominator pie_index ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 1 w 0.5 a ## 2 1.5 1 b 0.5 a ## 3 2.5 1 b 0.5 a ## 4 3.5 1 b 0.5 a ## 5 0.125 2 w 0.125 a ## 6 0.375 2 b 0.125 a ## 7 0.625 2 b 0.125 a ## 8 0.875 2 b 0.125 a ## 9 1.12 2 w 0.125 a ## 10 1.38 2 b 0.125 a ## # … with 242 more rows Both lines_1 and lines_2 require adjustments for x and xend. Our current approach is a nested ifelse(). Rather than copy and paste that multi-line ifelse() code for all four, let’s wrap it in a compact function, which we’ll call move_over(). move_over &lt;- function(position, index){ ifelse(index == &quot;a&quot;, position, ifelse(index == &quot;b&quot;, position + 4, position + 4 * 2) ) } If you’re new to making your own R functions, check out Chapter 19 of R4DS or Chapter 14 of R Programming for Data Science. Anyway, now we’ll make our new lines_1 and lines_2 data, for which we’ll use move_over() to adjust their x and xend positions to the correct spots. ( lines_1 &lt;- tibble(x = rep((1:4), each = 4) %&gt;% rep(., times = 3), xend = ((1:4^2) / 4) %&gt;% rep(., times = 3), y = 1, yend = 2) %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 48 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 0.125 1 2 a ## 2 0.5 0.375 1 2 a ## 3 0.5 0.625 1 2 a ## 4 0.5 0.875 1 2 a ## 5 1.5 1.12 1 2 a ## 6 1.5 1.38 1 2 a ## 7 1.5 1.62 1 2 a ## 8 1.5 1.88 1 2 a ## 9 2.5 2.12 1 2 a ## 10 2.5 2.38 1 2 a ## # … with 38 more rows ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4) %&gt;% rep(., times = 3), xend = (1:4^3 / 4^2) %&gt;% rep(., times = 3), y = 2, yend = 3) %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 192 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.125 0.0312 2 3 a ## 2 0.125 0.0938 2 3 a ## 3 0.125 0.156 2 3 a ## 4 0.125 0.219 2 3 a ## 5 0.375 0.281 2 3 a ## 6 0.375 0.344 2 3 a ## 7 0.375 0.406 2 3 a ## 8 0.375 0.469 2 3 a ## 9 0.625 0.531 2 3 a ## 10 0.625 0.594 2 3 a ## # … with 182 more rows For the last data wrangling step, we add the remain indices to help us determine which parts to make semitransparent. I’m not sure of a slick way to do this, so these are the result of brute force counting. d &lt;- d %&gt;% mutate(remain = c(# `pie_index == &quot;a&quot;` rep(0:1, times = c(1, 3)), rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), # `pie_index == &quot;b&quot;` rep(0:1, each = 2), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 2), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), # `pie_index == &quot;c&quot;` rep(0:1, times = c(3, 1)), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)) ) ) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 8), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) We’re finally ready to plot our Figure 2.4. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_vline(xintercept = c(0, 4, 8), color = &quot;white&quot;, size = 2/3) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, size = draw, alpha = remain %&gt;% as.character()), shape = 21) + scale_size_continuous(range = c(3, 1.5)) + scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 12), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3.5), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() 2.1.2 Using prior information. We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25) Here’s the table in the middle of page 25. t &lt;- t %&gt;% rename(`previous counts` = `ways to produce`, `ways to produce` = `draw 1: blue`) %&gt;% select(p_1:p_4, `ways to produce`, `previous counts`) %&gt;% mutate(`new count` = `ways to produce` * `previous counts`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 ways to produce previous counts new count w w w w 0 0 0 b w w w 1 3 3 b b w w 2 8 16 b b b w 3 9 27 b b b b 4 0 0 We might update to reproduce the table a the top of page 26, like this. t &lt;- t %&gt;% select(p_1:p_4, `new count`) %&gt;% rename(`prior count` = `new count`) %&gt;% mutate(`factory count` = c(0, 3:0)) %&gt;% mutate(`new count` = `prior count` * `factory count`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 prior count factory count new count w w w w 0 0 0 b w w w 3 3 9 b b w w 16 2 32 b b b w 27 1 27 b b b b 0 0 0 To learn more about dplyr::select() and dplyr::rename(), check out Baert’s exhaustive blog post Data Wrangling Part 1: Basic to Advanced Ways to Select Columns. 2.1.3 From counts to probability. The opening sentences in this subsection are important: “It is helpful to think of this strategy as adhering to a principle of honest ignorance: When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible” (p. 26, emphasis in the original). We can define our updated plausibility as: plausibility of after seeing \\(\\propto\\) ways can produce \\(\\times\\) prior plausibility of In other words: \\[ \\text{plausibility of } p \\text{ after } D_{\\text{new}} \\propto \\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p \\] But since we have to standardize the results to get them into a probability metric, the full equation is: \\[ \\text{plausibility of } p \\text{ after } D_{\\text{new}} = \\frac{\\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p}{\\text{sum of the products}} \\] You might make the table in the middle of page 27 like this. t %&gt;% select(p_1:p_4) %&gt;% mutate(p = seq(from = 0, to = 1, by = .25), `ways to produce data` = c(0, 3, 8, 9, 0)) %&gt;% mutate(plausibility = `ways to produce data` / sum(`ways to produce data`)) ## # A tibble: 5 x 7 ## p_1 p_2 p_3 p_4 p `ways to produce data` plausibility ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 w w w w 0 0 0 ## 2 b w w w 0.25 3 0.15 ## 3 b b w w 0.5 8 0.4 ## 4 b b b w 0.75 9 0.45 ## 5 b b b b 1 0 0 We just computed the plausibilities, but here’s McElreath’s R code 2.1. ways &lt;- c(0, 3, 8, 9, 0) ways / sum(ways) ## [1] 0.00 0.15 0.40 0.45 0.00 2.2 Building a model We might save our globe-tossing data in a tibble. (d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;))) ## # A tibble: 9 x 1 ## toss ## &lt;chr&gt; ## 1 w ## 2 l ## 3 w ## 4 w ## 5 w ## 6 l ## 7 w ## 8 l ## 9 w 2.2.1 A data story. Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how come events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28, emphasis in the original) 2.2.2 Bayesian updating. Here we’ll add the cumulative number of trials, n_trials, and the cumulative number of successes, n_successes (i.e., toss == &quot;w&quot;), to the data. ( d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;)) ) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 Fair warning: We don’t learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview. sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) %&gt;% ungroup() %&gt;% mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), likelihood = dbinom(x = n_success, size = n_trials, prob = p_water), strip = str_c(&quot;n = &quot;, n_trials) ) %&gt;% # the next three lines allow us to normalize the prior and the likelihood, # putting them both in a probability metric group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% # plot! ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~strip, scales = &quot;free_y&quot;) If it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable. To learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. To learn about tidyr::expand(), go here. 2.2.3 Evaluate. It’s worth repeating the Rethinking: Deflationary statistics box, here. It may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we’d like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32) 2.3 Components of the model a likelihood function: “the number of ways each conjecture could produce an observation” one or more parameters: “the accumulated number of ways each conjecture cold produce the entire data” a prior: “the initial plausibility of each conjectured cause of the data” 2.3.1 Likelihood. If you let the count of water be \\(w\\) and the number of tosses be \\(n\\), then the binomial likelihood may be expressed as: \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Given a probability of .5, the binomial likelihood of 6 out of 9 tosses coming out water is: dbinom(x = 6, size = 9, prob = .5) ## [1] 0.1640625 McElreath suggested we change the values of prob. Let’s do so over the parameter space. tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;binomial likelihood&quot;) + theme(panel.grid = element_blank()) 2.3.2 Parameters. McElreath started off his Rethinking: Datum or parameter? box with: It is typical to conceive of data and parameters as completely different kinds of entities. Data are measures and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is fuzzy. (p. 34) For more in this topic, check out his lecture Understanding Bayesian Statistics without Frequentist Language. 2.3.3 Prior. So where do priors come from? They are engineering assumptions, chosen to help the machine learn. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior. You’ll see later in the book that priors that gently nudge the machine usually improve inference. Such priors are sometimes called regularizing or weakly informative priors. (p. 35) To learn more about “regularizing or weakly informative priors,” check out the Prior Choice Recommendations wiki from the Stan team. 2.3.3.1 Overthinking: Prior as a probability distribution McElreath said that “for a uniform prior from \\(a\\) to \\(b\\), the probability of any point in the interval is \\(1 / (b - a)\\)” (p. 35). Let’s try that out. To keep things simple, we’ll hold \\(a\\) constant while varying the values for \\(b\\). tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% mutate(prob = 1 / (b - a)) ## # A tibble: 5 x 3 ## a b prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 1 ## 2 0 1.5 0.667 ## 3 0 2 0.5 ## 4 0 3 0.333 ## 5 0 9 0.111 I like to verify things with plots. tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% expand(nesting(a, b), parameter_space = seq(from = 0, to = 9, length.out = 500)) %&gt;% mutate(prob = dunif(parameter_space, a, b), b = str_c(&quot;b = &quot;, b)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + scale_x_continuous(breaks = c(0, 1:3, 9)) + scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1), labels = c(&quot;0&quot;, &quot;1/9&quot;, &quot;1/3&quot;, &quot;1/2&quot;, &quot;2/3&quot;, &quot;1&quot;)) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) + facet_wrap(~b, ncol = 5) And as we’ll learn much later in the project, the \\(\\text{Uniform} (0, 1)\\) distribution is special in that we can also express it as the beta distribution for which \\(\\alpha = 1 \\text{ and } \\beta = 1\\). E.g., tibble(parameter_space = seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(prob = dbeta(parameter_space, 1, 1)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + coord_cartesian(ylim = 0:2) + theme(panel.grid = element_blank()) 2.3.4 Posterior. If we continue to focus on the globe tossing example, the posterior probability a toss will be water may be expressed as: \\[\\text{Pr} (p|w) = \\frac{\\text{Pr} (w|p) \\text{Pr} (p)}{\\text{Pr} (w)}\\] More generically and in words, this is: \\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Average Likelihood}}\\] 2.4 Making the model go Here’s the data wrangling for Figure 2.6. sequence_length &lt;- 1e3 d &lt;- tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% expand(probability, row = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)) %&gt;% arrange(row, probability) %&gt;% mutate(prior = ifelse(row == &quot;flat&quot;, 1, ifelse(row == &quot;stepped&quot;, rep(0:1, each = sequence_length / 2), exp(-abs(probability - .5) / .25) / ( 2 * .25))), likelihood = dbinom(x = 6, size = 9, prob = probability)) %&gt;% group_by(row) %&gt;% mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&gt;% gather(key, value, -probability, -row) %&gt;% ungroup() %&gt;% mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), row = factor(row, levels = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;))) To learn more about dplyr::arrange(), chech out R4DS, Chapter 5.3. In order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately and then recombine them with gridExtra::grid.arrange(). p1 &lt;- d %&gt;% filter(key == &quot;prior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;prior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p2 &lt;- d %&gt;% filter(key == &quot;likelihood&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;likelihood&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p3 &lt;- d %&gt;% filter(key == &quot;posterior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;posterior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) I’m not sure if it’s the same McElreath used in the text, but the formula I used for the tirangle-shaped prior is the Laplace distribution with a location of .5 and a dispersion of .25. Also, to learn all about dplyr::filter(), check out Baert’s Data Wrangling Part 3: Basic and more advanced ways to filter rows. 2.4.1 Grid approximation. We just employed grid approximation over the last figure. In order to get nice smooth lines, we computed the posterior over 1000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20. (d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% # compute likelihood at each value in grid mutate(unstd_posterior = likelihood * prior) %&gt;% # compute product of likelihood and prior mutate(posterior = unstd_posterior / sum(unstd_posterior)) # standardize the posterior, so it sums to 1 ) ## # A tibble: 20 x 5 ## p_grid prior likelihood unstd_posterior posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 0.0526 1 0.00000152 0.00000152 0.000000799 ## 3 0.105 1 0.0000819 0.0000819 0.0000431 ## 4 0.158 1 0.000777 0.000777 0.000409 ## 5 0.211 1 0.00360 0.00360 0.00189 ## 6 0.263 1 0.0112 0.0112 0.00587 ## 7 0.316 1 0.0267 0.0267 0.0140 ## 8 0.368 1 0.0529 0.0529 0.0279 ## 9 0.421 1 0.0908 0.0908 0.0478 ## 10 0.474 1 0.138 0.138 0.0728 ## 11 0.526 1 0.190 0.190 0.0999 ## 12 0.579 1 0.236 0.236 0.124 ## 13 0.632 1 0.267 0.267 0.140 ## 14 0.684 1 0.271 0.271 0.143 ## 15 0.737 1 0.245 0.245 0.129 ## 16 0.789 1 0.190 0.190 0.0999 ## 17 0.842 1 0.118 0.118 0.0621 ## 18 0.895 1 0.0503 0.0503 0.0265 ## 19 0.947 1 0.00885 0.00885 0.00466 ## 20 1 1 0 0 0 Here’s the right panel of Figure 2.7. d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Here it is with just 5 points, the left hand panel of Figure 2.7. tibble(p_grid = seq(from = 0, to = 1, length.out = 5), prior = 1) %&gt;% mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% mutate(unstd_posterior = likelihood * prior) %&gt;% mutate(posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) 2.4.2 Quadratic approximation. Apply the quadratic approximation to the globe tossing data with rethinking::map(). library(rethinking) globe_qa &lt;- rethinking::map( alist( w ~ dbinom(9, p), # binomial likelihood p ~ dunif(0, 1) # uniform prior ), data = list(w = 6)) # display summary of quadratic approximation precis(globe_qa) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.16 0.42 0.92 In preparation for Figure 2.8, here’s the model with \\(n = 18\\) and \\(n = 36\\). globe_qa_18 &lt;- rethinking::map( alist( w ~ dbinom(9 * 2, p), p ~ dunif(0, 1) ), data = list(w = 6 *2)) globe_qa_36 &lt;- rethinking::map( alist( w ~ dbinom(9 * 4, p), p ~ dunif(0, 1) ), data = list(w = 6 * 4)) precis(globe_qa_18) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.11 0.49 0.84 precis(globe_qa_36) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.08 0.54 0.79 Here’s the legwork for Figure 2.8. n_grid &lt;- 100 tibble(p_grid = seq(from = 0, to = 1, length.out = n_grid) %&gt;% rep(., times = 3), prior = 1, w = rep(c(6, 12, 24), each = n_grid), n = rep(c(9, 18, 36), each = n_grid), m = .67, s = rep(c(.16, .11, .08), each = n_grid)) %&gt;% mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %&gt;% mutate(unstd_grid_posterior = likelihood * prior, unstd_quad_posterior = dnorm(p_grid, m, s)) %&gt;% group_by(w) %&gt;% mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior), quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior), n = str_c(&quot;n = &quot;, n)) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 9&quot;, &quot;n = 18&quot;, &quot;n = 36&quot;))) %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = grid_posterior)) + geom_line(aes(y = quad_posterior), color = &quot;grey50&quot;) + labs(x = &quot;proportion water&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~n, scales = &quot;free&quot;) 2.4.3 Markov chain Monte Carlo. Since the main goal of this project is to highlight brms, we may as fit a model. This seems like an appropriately named subsection to do so. First we’ll have to load the package. library(brms) Here we’ll re-fit the last model from above wherein \\(w = 24\\) and \\(n = 36\\). globe_qa_brms &lt;- brm(data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 1, prior(beta(1, 1), class = Intercept), iter = 4000, warmup = 1000, control = list(adapt_delta = .9), seed = 4) The model output looks like so. print(globe_qa_brms) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.66 0.08 0.50 0.80 3579 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). There’s a lot going on in that output, which we’ll start to clarify in Chapter 4. For now, focus on the ‘Intercept’ line. As we’ll also learn in Chapter 4, the intercept of a regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial, \\(\\theta\\). Let’s plot the results of our model and compare them with those from rethinking::map(), above. posterior_samples(globe_qa_brms) %&gt;% mutate(n = &quot;n = 36&quot;) %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(fill = &quot;black&quot;) + labs(x = &quot;proportion water&quot;) + xlim(0, 1) + theme(panel.grid = element_blank()) + facet_wrap(~n) If you’re still confused. Cool. This is just a preview. We’ll start walking through fitting models in brms in Chapter 4 and we’ll learn a lot about regression with the binomial likelihood in Chapter 10. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.9.0 Rcpp_1.0.1 rethinking_1.59 rstan_2.18.2 StanHeaders_2.18.1 ## [6] gridExtra_2.3 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 ## [11] readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 rprojroot_1.3-2 ## [5] markdown_1.0 base64enc_0.1-3 rstudioapi_0.10 DT_0.7 ## [9] fansi_0.4.0 mvtnorm_1.0-10 lubridate_1.7.4 xml2_1.2.0 ## [13] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 zeallot_0.1.0 ## [17] bayesplot_1.7.0 jsonlite_1.6 broom_0.5.2 shiny_1.3.2 ## [21] compiler_3.6.0 httr_1.4.0 backports_1.1.4 assertthat_0.2.1 ## [25] Matrix_1.2-17 lazyeval_0.2.2 cli_1.1.0 later_0.8.0 ## [29] htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.0 igraph_1.2.4.1 ## [33] coda_0.19-2 gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [37] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-140 crosstalk_1.0.0 ## [41] xfun_0.7 ps_1.3.0 rvest_0.3.4 mime_0.7 ## [45] miniUI_0.1.1.1 pacman_0.5.1 gtools_3.8.1 MASS_7.3-51.4 ## [49] zoo_1.8-6 scales_1.0.0 colourpicker_1.0 hms_0.4.2 ## [53] promises_1.0.1 Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 ## [57] yaml_2.2.0 loo_2.1.0 stringi_1.4.3 highr_0.8 ## [61] dygraphs_1.1.1.6 pkgbuild_1.0.3 rlang_0.3.4 pkgconfig_2.0.2 ## [65] matrixStats_0.54.0 evaluate_0.14 lattice_0.20-38 rstantools_1.5.1 ## [69] htmlwidgets_1.3 labeling_0.3 processx_3.3.1 tidyselect_0.2.5 ## [73] plyr_1.8.4 magrittr_1.5 bookdown_0.11 R6_2.4.0 ## [77] generics_0.0.2 pillar_1.4.1 haven_2.1.0 withr_2.1.2 ## [81] xts_0.11-2 abind_1.4-5 modelr_0.1.4 crayon_1.3.4 ## [85] utf8_1.1.4 rmarkdown_1.13 grid_3.6.0 readxl_1.3.1 ## [89] callr_3.2.0 threejs_0.3.1 digest_0.6.19 xtable_1.8-4 ## [93] httpuv_1.5.1 stats4_3.6.0 munsell_0.5.0 shinyjs_1.0 "],
["sampling-the-imaginary.html", "Chapter 3 Sampling the Imaginary 3.1 Sampling from a grid-like approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Summary Let’s practice in brms Reference Session info", " Chapter 3 Sampling the Imaginary If you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute \\[\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}}\\] We’ll do so within a tibble. library(tidyverse) tibble(pr_positive_vampire = .95, pr_positive_mortal = .01, pr_vampire = .001) %&gt;% mutate(pr_positive = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.01 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive &lt;dbl&gt; 0.01094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 Here’s the other way of tackling the vampire problem, this time useing the frequency format. tibble(pr_vampire = 100 / 100000, pr_positive_vampire = 95 / 100, pr_positive_mortal = 99 / 99900) %&gt;% mutate(pr_positive = 95 + 999) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.000990991 ## $ pr_positive &lt;dbl&gt; 1094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 3.1 Sampling from a grid-like approximate posterior Here we use grid approximation, again, to generate samples. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows Now we’ll use the dplyr::sample_n() function to sample rows from d, saving them as sample. # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) glimpse(samples) ## Observations: 10,000 ## Variables: 4 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… We’ll plot the zigzagging left panel of Figure 3.1 with geom_line(). But before we do, we’ll need to add a variable numbering the samples. samples %&gt;% mutate(sample_number = 1:n()) %&gt;% ggplot(aes(x = sample_number, y = p_grid)) + geom_line(size = 1/10) + labs(x = &quot;sample number&quot;, y = &quot;proportion of water (p)&quot;) We’ll make the density in the right panel with geom_density(). samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;black&quot;) + coord_cartesian(xlim = 0:1) + xlab(&quot;proportion of water (p)&quot;) 3.2 Sampling to summarize “Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly now it is summarized depends upon your purpose” (p. 53). 3.2.1 Intervals of defined boundaries. To get the proportion of water less than some value of p_grid within the tidyverse, you’d first filter() by that value and then take the sum() within summarise(). d %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = sum(posterior)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.171 To learn more about dplyr::summarise() and related functions, check out Baert’s Data Wrangling Part 4: Summarizing and slicing your data and Chapter 5.6 of R4DS. If what you want is a frequency based on filtering by samples, then you might use n() within summarise(). samples %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.162 You can use &amp; within filter(), too. samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.602 3.2.2 Intervals of defined mass. We’ll create the upper two panels for Figure 3.2 with geom_line(), geom_ribbon(), and a some careful filtering. # upper left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # upper right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + # note this next line is the only difference in code from the last plot geom_ribbon(data = d %&gt;% filter(p_grid &lt; .75 &amp; p_grid &gt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ll come back for the lower two panels in a bit. Since we’ve saved our p_grid samples within the well-named samples tibble, we’ll have to index with $ within quantile. (q_80 &lt;- quantile(samples$p_grid, prob = .8)) ## 80% ## 0.763 That value will come in handy for the lower left panel of Figure 3.2, so we saved it. But anyways, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile(): samples %&gt;% select(p_grid) %&gt;% pull() %&gt;% quantile(prob = .8) ## 80% ## 0.763 And we might also use quantile() within summarise(). samples %&gt;% summarise(`80th percentile` = quantile(p_grid, p = .8)) ## # A tibble: 1 x 1 ## `80th percentile` ## &lt;dbl&gt; ## 1 0.763 Here’s the summarise() approach with two probabilities: samples %&gt;% summarise(`10th percentile` = quantile(p_grid, p = .1), `90th percentile` = quantile(p_grid, p = .9)) ## # A tibble: 1 x 2 ## `10th percentile` `90th percentile` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.452 0.814 The tydiverse approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base R quantile() shines. (q_10_and_90 &lt;- quantile(samples$p_grid, prob = c(.1, .9))) ## 10% 90% ## 0.4520 0.8141 Now we have our cutoff values saved as q_80 and q_10_and_90, we’re ready to make the bottom panels of Figure 3.2. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; q_80), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;lower 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; q_10_and_90[1] &amp; p_grid &lt; q_10_and_90[2]), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;middle 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ve already defined p_grid and prior within d, above. Here we’ll reuse them and update the rest of the columns. # here we update the `dbinom()` parameters n_success &lt;- 3 n_trials &lt;- 3 # update `d` d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(posterior)) # make the next part reproducible set.seed(3) # here&#39;s our new samples tibble ( samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) ) ## # A tibble: 10,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.716 1 0.367 0.367 ## 2 0.651 1 0.276 0.276 ## 3 0.547 1 0.164 0.164 ## 4 0.999 1 0.997 0.997 ## 5 0.99 1 0.970 0.970 ## 6 0.787 1 0.487 0.487 ## 7 0.94 1 0.831 0.831 ## 8 0.817 1 0.545 0.545 ## 9 0.955 1 0.871 0.871 ## 10 0.449 1 0.0905 0.0905 ## # … with 9,990 more rows The rethinking::PI() function works like a nice shorthand for quantile(). quantile(samples$p_grid, prob = c(.25, .75)) ## 25% 75% ## 0.709 0.935 rethinking::PI(samples$p_grid, prob = .5) ## 25% 75% ## 0.709 0.935 Now’s a good time to introduce Matthew Kay’s tidybayes package, which offers an array of convenience functions for Bayesian models of the type we’ll be working with in this project. library(tidybayes) median_qi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.843 0.709 0.935 0.5 median qi The tidybayes package offers a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals–just like we’ve been doing with quantile(). Note how the .width argument within median_qi() worked the same way the prob argument did within rethinking::PI(). With .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals. median_qi(samples$p_grid, .width = c(.5, .8, .99)) ## y ymin ymax .width .point .interval ## 1 0.843 0.709000 0.935 0.50 median qi ## 2 0.843 0.570000 0.975 0.80 median qi ## 3 0.843 0.260985 0.999 0.99 median qi The .width column in the output indexed which line presented which interval. Now let’s use the rethinking::HPDI() function to return 50% highest posterior density intervals (HPDIs). rethinking::HPDI(samples$p_grid, prob = .5) ## |0.5 0.5| ## 0.842 0.999 The reason I introduce tidybayes now is that the functions of the brms package only support percentile-based intervals of the type we computed with quantile() and median_qi(). But tidybayes also supports HPDIs. mode_hdi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.9562951 0.842 0.999 0.5 mode hdi This time we used the mode as the measure of central tendency. With this family of tidybayes functions, you specify the measure of central tendency in the prefix (i.e., mean, median, or mode) and then the type of interval you’d like (i.e., qi or hdi). If all you want are the intervals without the measure of central tendency or all that other technical information, tidybayes also offers the handy qi() and hdi() functions. qi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.709 0.935 hdi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.842 0.999 These are nice in that they yield simple numeric vectors, making them particularly useful to use as references within ggplot2. Now we have that skill, we can use it to make Figure 3.3. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + # check out our sweet `qi()` indexing geom_ribbon(data = d %&gt;% filter(p_grid &gt; qi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; qi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% Percentile Interval&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; hdi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; hdi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% HPDI&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) 3.2.3 Point estimates. We’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate. d %&gt;% arrange(desc(posterior)) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 ## 2 0.999 1 0.997 0.997 ## 3 0.998 1 0.994 0.994 ## 4 0.997 1 0.991 0.991 ## 5 0.996 1 0.988 0.988 ## 6 0.995 1 0.985 0.985 ## 7 0.994 1 0.982 0.982 ## 8 0.993 1 0.979 0.979 ## 9 0.992 1 0.976 0.976 ## 10 0.991 1 0.973 0.973 ## # … with 991 more rows To emphasize it, we can use slice() to select the top row. d %&gt;% arrange(desc(posterior)) %&gt;% slice(1) ## # A tibble: 1 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 Or we could use the handy dplyr::top_n() function. d %&gt;% select(posterior) %&gt;% top_n(n = 1) ## Selecting by posterior ## # A tibble: 1 x 1 ## posterior ## &lt;dbl&gt; ## 1 1 We can get th emode with mode_hdi() or mode_qi(). samples %&gt;% mode_hdi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.477 1 0.95 mode hdi samples %&gt;% mode_qi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.401 0.994 0.95 mode qi But if all you want is the mode itself, you can just use tidybayes::Mode(). Mode(samples$p_grid) ## [1] 0.9562951 But medians and means are typical, too. samples %&gt;% summarise(mean = mean(p_grid), median = median(p_grid)) ## # A tibble: 1 x 2 ## mean median ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 0.843 We can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble. ( point_estimates &lt;- bind_rows( samples %&gt;% mean_qi(p_grid), samples %&gt;% median_qi(p_grid), samples %&gt;% mode_qi(p_grid) ) %&gt;% select(p_grid, .point) %&gt;% # these last two columns will help us annotate mutate(x = p_grid + c(-.03, .03, -.03), y = c(.1, .25, .4)) ) ## # A tibble: 3 x 4 ## p_grid .point x y ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 mean 0.773 0.1 ## 2 0.843 median 0.873 0.25 ## 3 0.956 mode 0.926 0.4 The plot: d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$p_grid) + geom_text(data = point_estimates, aes(x = x, y = y, label = .point), angle = 90) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) As it turns out “different loss functions imply different point estimates” (p. 59, emphasis in the original). Let \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(p - d\\). If we decide \\(d = .5\\), then our expected loss will be: d %&gt;% mutate(loss = posterior * abs(0.5 - p_grid)) %&gt;% summarise(`expected loss` = sum(loss)) ## # A tibble: 1 x 1 ## `expected loss` ## &lt;dbl&gt; ## 1 78.4 What McElreath did with sapply(), we’ll do with purrr::map(). If you haven’t used it, map() is part of a family of similarly-named functions (e.g., map2()) from the purrr package, which is itself part of the tidyverse. The map() family is the tidyverse alternative to the family of apply() functions from the base R framework. You can learn more about how to use the map() family here or here or here. make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * abs(our_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } ( l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() ) ## # A tibble: 1,001 x 2 ## decision weighted_average_loss ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 201. ## 2 0.001 200. ## 3 0.002 200. ## 4 0.003 200. ## 5 0.004 199. ## 6 0.005 199. ## 7 0.006 199. ## 8 0.007 199. ## 9 0.008 198. ## 10 0.009 198. ## # … with 991 more rows Now we’re ready for the right panel of Figure 3.4. # this will help us find the x and y coordinates for the minimum value min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) We saved the exact minimum value as min_loss[1], which is 0.841. Within sampling error, this is the posterior median as depicted by our samples. samples %&gt;% summarise(posterior_median = median(p_grid)) ## # A tibble: 1 x 1 ## posterior_median ## &lt;dbl&gt; ## 1 0.843 The quadratic loss \\((d - p)^2\\) suggests we should use the mean instead. Let’s investigate. # ammend our loss function make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * (our_d - p_grid)^2) %&gt;% summarise(weighted_average_loss = sum(loss)) } # remake our `l` data l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() # update to the new minimum loss coordinates min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # update the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) Based on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.8. Within sampling error, this is the posterior mean of our samples. samples %&gt;% summarise(posterior_meaan = mean(p_grid)) ## # A tibble: 1 x 1 ## posterior_meaan ## &lt;dbl&gt; ## 1 0.803 3.3 Sampling to simulate prediction McElreath’s four good reasons for posterior simulation were: Model checking Software validation Research design Forecasting 3.3.1 Dummy data. Dummy data for the globe tossing model arise from the binomial likelihood. If you let \\(w\\) be a count of water and \\(n\\) be the number of tosses, the binomial likelihood is \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Letting \\(n = 2\\), \\(p(w) = .7\\), and \\(w_\\text{observed} = 0 \\text{ through }2\\), the denisties are: tibble(n = 2, probability = .7, w = 0:2) %&gt;% mutate(density = dbinom(w, size = n, prob = probability)) ## # A tibble: 3 x 4 ## n probability w density ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 0.7 0 0.09 ## 2 2 0.7 1 0.42 ## 3 2 0.7 2 0.490 If we’re going to simulate, we should probably set our seed. Doing so makes the results reproducible. set.seed(3) rbinom(1, size = 2, prob = .7) ## [1] 2 Here are ten reproducible draws. set.seed(3) rbinom(10, size = 2, prob = .7) ## [1] 2 1 2 2 1 1 2 2 1 1 Now generate 100,000 (i.e., 1e5) reproducible dummy observations. # how many would you like? n_draws &lt;- 1e5 set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 2, prob = .7)) d %&gt;% group_by(draws) %&gt;% count() %&gt;% mutate(proportion = n / nrow(d)) ## # A tibble: 3 x 3 ## # Groups: draws [3] ## draws n proportion ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 9000 0.09 ## 2 1 42051 0.421 ## 3 2 48949 0.489 As McElreath mused in the text, those simulated proportion values are very close to the analytically calculated values in our density column a few code blocks up. Here’s the simulation updated so \\(n = 9\\), which we plot in our version of Figure 3.5. set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 9, prob = .7)) # the histogram d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) McElreath suggested we play around with different values of size and prob. With the next block of code, we’ll simulate nine conditions. n_draws &lt;- 1e5 simulate_binom &lt;- function(n, probability){ set.seed(3) rbinom(n_draws, size = n, prob = probability) } d &lt;- tibble(n = c(3, 6, 9)) %&gt;% expand(n, probability = c(.3, .6, .9)) %&gt;% mutate(draws = map2(n, probability, simulate_binom)) %&gt;% ungroup() %&gt;% mutate(n = str_c(&quot;n = &quot;, n), probability = str_c(&quot;p = &quot;, probability)) %&gt;% unnest() head(d) ## # A tibble: 6 x 3 ## n probability draws ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 n = 3 p = 0.3 0 ## 2 n = 3 p = 0.3 2 ## 3 n = 3 p = 0.3 1 ## 4 n = 3 p = 0.3 0 ## 5 n = 3 p = 0.3 1 ## 6 n = 3 p = 0.3 1 The results look as follows: d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_grid(n ~ probability) 3.3.2 Model checking. If you’re new to applied statistics, you might be surprised how often mistakes arise. 3.3.2.1 Did the software work? Let this haunt your dreams: “There is no way to really be sure that software works correctly” (p. 64). If you’d like to dive deeper into these dark waters, check out one my favorite talks from StanCon 2018, Esther Williams in the Harold Holt Memorial Swimming Pool, by the ineffable Dan Simpson. If Simpson doesn’t end up drowning you, see Gabry and Simpson’s talk at the Royal Statistical Society 2018, Visualization in Bayesian workflow, a follow-up blog Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it, and that blog’s associated pre-print by Vehtari, Gelman, Simpson, Carpenter, and Bürkner Rank-normalization, folding, and localization: An improved Rˆ for assessing convergence of MCMC. 3.3.2.2 Is the model adequate? The implied predictions of the model are uncertain in two ways, and it’s important to be aware of both. First, there is observation uncertainty. For any unique value of the parameter \\(p\\), there is a unique implied pattern of observations that the model expects. These patterns of observations are the same gardens of forking data that you explored in the previous chapter. These patterns are also what you sampled in the previous section. There is uncertainty in the predicted observations, because even if you know \\(p\\) with certainty, you won’t know the next globe toss with certainty (unless \\(p = 0\\) or \\(p = 1\\)). Second, there is uncertainty about \\(p\\). The posterior distribution over \\(p\\) embodies this uncertainty. And since there is uncertainty about \\(p\\), there is uncertainty about everything that depends upon \\(p\\). The uncertainty in \\(p\\) will interact with the sampling variation, when we try to assess what the model tells us about outcomes. We’d like to propagate the parameter uncertainty–carry it forward–as we evaluate the implied predictions. All that is required is averaging over the posterior density for \\(p\\), while computing the predictions. For each possible value of the parameter \\(p\\), there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of \\(p\\), then you could average all of these prediction distributions together, using the posterior probabilities of each value of \\(p\\), to get a posterior predictive distribution. (p. 56, emphasis in the original) All this is depicted in Figure 3.6. To get ready to make our version, let’s first refresh our original grid approximation d. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows We can make our version of the top of Figure 3.6 with a little tricky filtering. d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), color = &quot;grey67&quot;, fill = &quot;grey67&quot;) + geom_segment(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(xend = p_grid, y = 0, yend = posterior, size = posterior), color = &quot;grey33&quot;, show.legend = F) + geom_point(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(y = posterior)) + annotate(geom = &quot;text&quot;, x = .08, y = .0025, label = &quot;Posterior probability&quot;) + scale_size_continuous(range = c(0, 1)) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0:10) / 10) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Note how we weighted the widths of the vertical lines by the posterior density. We’ll need to do a bit of wrangling before we’re ready to make the plot in the middle panel of Figure 3.6. n_draws &lt;- 1e5 simulate_binom &lt;- function(probability){ set.seed(3) rbinom(n_draws, size = 9, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) %&gt;% mutate(label = str_c(&quot;p = &quot;, probability)) head(d_small) ## # A tibble: 6 x 3 ## probability draws label ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.1 0 p = 0.1 ## 2 0.1 2 p = 0.1 ## 3 0.1 0 p = 0.1 ## 4 0.1 0 p = 0.1 ## 5 0.1 1 p = 0.1 ## 6 0.1 1 p = 0.1 Now we’re ready to plot. d_small %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Sampling distributions&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_wrap(~ label, ncol = 9) To make the plot at the bottom of Figure 3.6, we’ll redefine our samples, this time including the w variable (see the R code 3.26 block in the text). # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) glimpse(samples) ## Observations: 10,000 ## Variables: 5 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… ## $ w &lt;dbl&gt; 4, 7, 3, 3, 7, 6, 8, 2, 6, 4, 5, 5, 8, 6, 4, 6, 8, 2, 6, 9, 9, 7, 4, 8, 9, 8, … Here’s our histogram. samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = 0:9, ylim = 0:3000) + theme(panel.grid = element_blank()) In Figure 3.7, McElreath considered the longst sequence of the sampe values. We’ve been using rbinom() with the size parameter set to 9 for our simulations. E.g., rbinom(10, size = 9, prob = .6) ## [1] 7 5 6 8 7 5 6 3 3 4 Notice this collapses (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this rbinom(9, size = 1, prob = .6) ## [1] 0 1 1 1 0 0 0 0 0 would be the disaggregated version of just one of the numerals returned by rbinom() when size = 9. So let’s try simulating again with un-aggregated samples. We’ll keep adding to our samples tibble. In addition to the disaggregated draws based on the \\(p\\) values listed in p_grid, we’ll also want to add a row index for each of those p_grid values–it’ll come in handy when we plot. # make it reproducible set.seed(3) samples &lt;- samples %&gt;% mutate(iter = 1:n(), draws = purrr::map(p_grid, rbinom, n = 9, size = 1)) %&gt;% unnest(draws) glimpse(samples) ## Observations: 90,000 ## Variables: 7 ## $ p_grid &lt;dbl&gt; 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.651, 0.651, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0… ## $ posterior &lt;dbl&gt; 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, … ## $ w &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ draws &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, … The main action is in the draws column. Now we have to count the longest sequences. The base R rle() function will help with that. Consider McElreath’s sequence of tosses. tosses &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) You can plug that into rle(). rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; For our purposes, we’re interested in lengths. That tells us the length of each sequences of the same value. The 3 corresponds to our run of three ws. The max() function will help us confirm it’s the largest value. rle(tosses)$lengths %&gt;% max() ## [1] 3 Now let’s apply our method to the data and plot. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% max()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 3), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;longest run length&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Let’s look at rle() again. rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; We can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, “w” and “l”. rle(tosses)$lengths %&gt;% length() ## [1] 7 With that new trick, we’re ready to make the right panel of Figure 3.7. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% length()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 6), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of switches&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 3.4 Summary Let’s practice in brms Open brms. library(brms) In brms, we’ll fit the primary model of \\(w = 6\\) and \\(n = 9\\) much like we did at the end of the project for Chapter 2. b3.1 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 1, # this is a flat prior prior(beta(1, 1), class = Intercept), seed = 3, control = list(adapt_delta = .999)) We’ll learn more about the beta distribution in Chapter 11. But for now, here’s the posterior summary for b_Intercept, the probability of a “w”. posterior_summary(b3.1)[&quot;b_Intercept&quot;, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 0.64 0.14 0.36 0.88 As we’ll fully cover in the next chapter, Estimate is the posterior mean, the two Q columns are the quantile-based 95% intervals, and Est.Error is the posterior standard deviation. Much like the way we used the samples() function to simulate probability values, above, we can do so with fitted() within the brms framework. But we will have to specify scale = &quot;linear&quot; in order to return results in the probability metric. By default, brms::fitted() will return summary information. Since we want actual simulation draws, we’ll specify summary = F. f &lt;- fitted(b3.1, summary = F, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(&quot;p&quot;) glimpse(f) ## Observations: 4,000 ## Variables: 1 ## $ p &lt;dbl&gt; 0.6920484, 0.5559454, 0.6096088, 0.5305334, 0.4819733, 0.6724561, 0.6402367, 0.8356569,… By default, we have a generically-named vector V1 of 4000 samples. We’ll explain the defaults in later chapters. For now, notice we can view these in a density. f %&gt;% ggplot(aes(x = p)) + geom_density(fill = &quot;grey50&quot;, color = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = .08, y = 2.5, label = &quot;Posterior probability&quot;) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0, .5, 1), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Looks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of w counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6. # the simulation set.seed(3) f &lt;- f %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) # the plot f %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3), limits = c(0, 9)) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1200)) + ggtitle(&quot;Posterior predictive distribution&quot;) + theme(panel.grid = element_blank()) As you might imagine, we can use the output from fitted() to return disaggregated batches of 0s and 1s, too. And we could even use those disaggregated 0s and 1s to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I’ll leave those as exercises for the interested reader. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.9.0 Rcpp_1.0.1 tidybayes_1.1.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [7] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] rprojroot_1.3-2 ggstance_0.3.1 markdown_1.0 ## [7] rethinking_1.59 base64enc_0.1-3 rstudioapi_0.10 ## [10] rstan_2.18.2 svUnit_0.7-12 DT_0.7 ## [13] fansi_0.4.0 mvtnorm_1.0-10 lubridate_1.7.4 ## [16] xml2_1.2.0 bridgesampling_0.6-0 knitr_1.23 ## [19] shinythemes_1.1.2 zeallot_0.1.0 bayesplot_1.7.0 ## [22] jsonlite_1.6 broom_0.5.2 shiny_1.3.2 ## [25] compiler_3.6.0 httr_1.4.0 backports_1.1.4 ## [28] assertthat_0.2.1 Matrix_1.2-17 lazyeval_0.2.2 ## [31] cli_1.1.0 later_0.8.0 htmltools_0.3.6 ## [34] prettyunits_1.0.2 tools_3.6.0 igraph_1.2.4.1 ## [37] coda_0.19-2 gtable_0.3.0 glue_1.3.1 ## [40] reshape2_1.4.3 cellranger_1.1.0 vctrs_0.1.0 ## [43] nlme_3.1-140 crosstalk_1.0.0 xfun_0.7 ## [46] ps_1.3.0 rvest_0.3.4 mime_0.7 ## [49] miniUI_0.1.1.1 pacman_0.5.1 gtools_3.8.1 ## [52] MASS_7.3-51.4 zoo_1.8-6 scales_1.0.0 ## [55] colourpicker_1.0 hms_0.4.2 promises_1.0.1 ## [58] Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 ## [61] yaml_2.2.0 gridExtra_2.3 StanHeaders_2.18.1 ## [64] loo_2.1.0 stringi_1.4.3 highr_0.8 ## [67] dygraphs_1.1.1.6 pkgbuild_1.0.3 rlang_0.3.4 ## [70] pkgconfig_2.0.2 matrixStats_0.54.0 HDInterval_0.2.0 ## [73] evaluate_0.14 lattice_0.20-38 rstantools_1.5.1 ## [76] htmlwidgets_1.3 labeling_0.3 processx_3.3.1 ## [79] tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 ## [82] bookdown_0.11 R6_2.4.0 generics_0.0.2 ## [85] pillar_1.4.1 haven_2.1.0 withr_2.1.2 ## [88] xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [91] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [94] rmarkdown_1.13 grid_3.6.0 readxl_1.3.1 ## [97] callr_3.2.0 threejs_0.3.1 digest_0.6.19 ## [100] xtable_1.8-4 httpuv_1.5.1 stats4_3.6.0 ## [103] munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.0 "],
["linear-models.html", "Chapter 4 Linear Models 4.1 Why normal distributions are normal 4.2 A language for describing models 4.3 A Gaussian model of height 4.4 Adding a predictor 4.5 Polynomial regression Reference Session info", " Chapter 4 Linear Models Linear regression is the geocentric model of applied statistics. By “linear regression”, we will mean a family of simple statistical golems that attempt to learn about the mean and variance of some measurement, using an additive combination of other measurements. Like geocentrism, linear regression can usefully describe a very large variety of natural phenomena. Like geocentrism, linear is a descriptive model that corresponds to many different process models. If we read its structure too literally, we’re likely to make mistakes. But used wisely, these little linear golems continue to be useful. (p. 71) 4.1 Why normal distributions are normal After laying out his soccer field coin toss shuffle premise, McElreath wrote: It’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72) 4.1.1 Normal by addition. Here’s a way to do the simulation necessary for the plot in the top panel of Figure 4.2. library(tidyverse) # we set the seed to make the results of `runif()` reproducible. set.seed(4) pos &lt;- replicate(100, runif(16, -1, 1)) %&gt;% # here&#39;s the simulation as_tibble() %&gt;% # for data manipulation, we&#39;ll make this a tibble rbind(0, .) %&gt;% # here we add a row of zeros above the simulation results mutate(step = 0:16) %&gt;% # this adds a step index gather(key, value, -step) %&gt;% # here we convert the data to the long format mutate(person = rep(1:100, each = 17)) %&gt;% # this adds a person id index # the next two lines allow us to make cumulative sums within each person group_by(person) %&gt;% mutate(position = cumsum(value)) %&gt;% ungroup() # ungrouping allows for further data manipulation We might glimpse() at the data. glimpse(pos) ## Observations: 1,700 ## Variables: 5 ## $ step &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7… ## $ key &lt;chr&gt; &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V1&quot;, &quot;V… ## $ value &lt;dbl&gt; 0.00000000, 0.17160061, -0.98210841, -0.41252078, -0.44525008, 0.62714843, -0.47… ## $ person &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… ## $ position &lt;dbl&gt; 0.00000000, 0.17160061, -0.81050780, -1.22302857, -1.66827866, -1.04113023, -1.5… And here’s the actual plot code. ggplot(data = pos, aes(x = step, y = position, group = person)) + geom_vline(xintercept = c(4, 8, 16), linetype = 2) + geom_line(aes(color = person &lt; 2, alpha = person &lt; 2)) + scale_color_manual(values = c(&quot;skyblue4&quot;, &quot;black&quot;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(&quot;step number&quot;, breaks = c(0, 4, 8, 12, 16)) + theme(legend.position = &quot;none&quot;) Here’s the code for the bottom three plots of Figure 4.2. # Figure 4.2.a. pos %&gt;% filter(step == 4) %&gt;% ggplot(aes(x = position)) + geom_line(stat = &quot;density&quot;, color = &quot;dodgerblue1&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;4 steps&quot;) # Figure 4.2.b. pos %&gt;% filter(step == 8) %&gt;% ggplot(aes(x = position)) + geom_density(color = &quot;dodgerblue2&quot;) + coord_cartesian(xlim = -6:6) + labs(title = &quot;8 steps&quot;) # this is an intermediary step to get an SD value pos %&gt;% filter(step == 16) %&gt;% summarise(sd = sd(position)) ## # A tibble: 1 x 1 ## sd ## &lt;dbl&gt; ## 1 2.18 # Figure 4.2.c. pos %&gt;% filter(step == 16) %&gt;% ggplot(aes(x = position)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2.180408), linetype = 2) + # 2.180408 came from the previous code block geom_density(color = &quot;transparent&quot;, fill = &quot;dodgerblue3&quot;, alpha = 1/2) + coord_cartesian(xlim = -6:6) + labs(title = &quot;16 steps&quot;, y = &quot;density&quot;) While we were at it, we explored a few ways to express densities. The main action was with the geom_line(), geom_density(), and stat_function() functions. 4.1.2 Normal by multiplication. Here’s McElreath’s simple random growth rate. set.seed(4) prod(1 + runif(12, 0, 0.1)) ## [1] 1.774719 In the runif() part of that code, we generated 12 random draws from the uniform distribution with bounds \\([0, 0.1]\\). Within the prod() function, we first added 1 to each of those values and then computed their product. Consider a more explicit variant of the code. set.seed(4) tibble(a = 1, b = runif(12, 0, 0.1)) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c)) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 1.77 Same result. Rather than using base R replicate() to do this many times, let’s practice with purrr::map_dbl() instead (see here for details). set.seed(4) growth &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.1)))) ggplot(data = growth, aes(x = growth)) + geom_density() “The smaller the effect of each locus, the better this additive approximation will be” (p. 74). Let’s compare big and small. # simulate set.seed(4) samples &lt;- tibble(big = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.5))), small = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.01)))) %&gt;% # wrangle gather(distribution, samples) # plot samples %&gt;% ggplot(aes(x = samples)) + geom_density(fill = &quot;black&quot;, color = &quot;transparent&quot;) + facet_wrap(~distribution, scales = &quot;free&quot;) Yep, the small samples were more Gaussian. 4.1.3 Normal by log-multiplication. Instead of saving our tibble, we’ll just feed it directly into our plot. set.seed(4) tibble(samples = map_dbl(1:1e4, ~ log(prod(1 + runif(12, 0, 0.5))))) %&gt;% ggplot(aes(x = samples)) + geom_density(color = &quot;transparent&quot;, fill = &quot;gray33&quot;) What we did was really compact. Walking it out a bit, here’s what we all did within the second argument within map_dbl() (i.e., everything within log()). tibble(a = runif(12, 0, 0.5), b = 1) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c) %&gt;% log()) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 2.82 And based on the first argument within map_dbl(), we did that 10,000 times, after which we converted the results to a tibble and then fed those data into ggplot2. 4.1.4 Using Gaussian distributions. I really like the justifications in the following subsections. 4.1.4.1 Ontological justification. The Gaussian is a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process… (p. 75) But they can still be useful. 4.1.4.2 Epistemological justification. Another route to justifying the Gaussian as our choice of skeleton, and a route that will help us appreciate later why it is often a poor choice, is that it represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions. That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions… If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (pp. 75–76) In the Overthinking: Gaussian distribution box that follows, McElreath gave the formula. Let \\(y\\) be the criterion, \\(\\mu\\) be the mean, and \\(\\sigma\\) be the standard deviation. Then the probability density of some Gaussian value \\(y\\) is \\[p(y|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp} \\Bigg (- \\frac{(y - \\mu)^2}{2 \\sigma^2} \\Bigg)\\] 4.2 A language for describing models Our mathy ways of summarizing models will be something like \\[\\begin{align*} \\text{criterion}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta \\times \\text{predictor}_i \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\text{HalfCauchy}(0, 1) \\end{align*}\\] And as McElreath then followed up with, “If that doesn’t make much sense, good. That indicates that you are holding the right textbook” (p. 77). Welcome applied statistics! 4.2.1 Re-describing the globe tossing model. For the globe tossing model, the probability \\(p\\) of a count of water \\(w\\) based on \\(n\\) trials was \\[\\begin{align*} w &amp; \\sim \\text{Binomial}(n, p) \\\\ p &amp; \\sim \\text{Uniform}(0, 1) \\end{align*}\\] We can break McElreath’s R code 4.6 down a little bit with a tibble like so. # how many `p_grid` points would you like? n_points &lt;- 100 d &lt;- tibble(w = 6, n = 9, p_grid = seq(from = 0, to = 1, length.out = n_points)) %&gt;% mutate(prior = dunif(p_grid, 0, 1), likelihood = dbinom(w, n, p_grid)) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 x 6 ## w n p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 9 0 1 0. 0. ## 2 6 9 0.0101 1 8.65e-11 8.74e-12 ## 3 6 9 0.0202 1 5.37e- 9 5.43e-10 ## 4 6 9 0.0303 1 5.93e- 8 5.99e- 9 ## 5 6 9 0.0404 1 3.23e- 7 3.26e- 8 ## 6 6 9 0.0505 1 1.19e- 6 1.21e- 7 In case you were curious, here’s what they look like: d %&gt;% select(-w, -n) %&gt;% gather(key, value, -p_grid) %&gt;% # this line allows us to dictate the order the panels will appear in mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;))) %&gt;% ggplot(aes(x = p_grid, ymin = 0, ymax = value, fill = key)) + geom_ribbon() + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;)) + scale_y_continuous(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) + facet_wrap(~key, scales = &quot;free&quot;) The posterior is a combination of the prior and the likelihood. And when the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you’ll see that we almost never use flat priors. 4.3 A Gaussian model of height There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large \\(\\sigma\\). Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of \\(\\mu\\) and \\(\\sigma\\), and rank them by posterior plausibility. (p. 79) 4.3.1 The data. Let’s get the data from McElreath’s rethinking package. library(rethinking) data(Howell1) d &lt;- Howell1 Here we open our main statistical package, Bürkner’s brms. But before we do, we’ll need to detach the rethinking package. R will not allow users to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and brms packages are designed for similar purposes and, unsurprisingly, overlap in the names of their functions. To prevent problems, we will always make sure rethinking is detached before using brms. To learn more on the topic, see this R-bloggers post. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Go ahead and investigate the data with str(), the tidyverse analogue for which is glimpse(). d %&gt;% str() ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... Here are the height values. d %&gt;% select(height) %&gt;% head() ## height ## 1 151.765 ## 2 139.700 ## 3 136.525 ## 4 156.845 ## 5 145.415 ## 6 163.830 We can use filter() to make an adults-only data frame. d2 &lt;- d %&gt;% filter(age &gt;= 18) 4.3.1.1 Overthinking: Data frames. This probably reflects my training history, but the structure of a data frame seems natural and inherently appealing, to me. So I can’t relate to the “annoying” comment. But if you’re in the other camp, do check out either of these two data wrangling talks (here and here) by the ineffable Jenny Bryan. 4.3.1.2 Overthinking: Index magic. For more on indexing, check out chapter 9 of Roger Peng’s R Programming for Data Science or even the Subsetting subsection from R4DS. 4.3.2 The model. The likelihood for our model is \\[h_i \\sim \\text{Normal}(\\mu, \\sigma)\\] Our \\(\\mu\\) prior will be \\[\\mu \\sim \\text{Normal}(178, 20)\\] And our prior for \\(\\sigma\\) will be \\[\\sigma \\sim \\text{Uniform}(0, 50)\\] Here’s the shape of the prior for \\(\\mu\\) in \\(N(178, 20)\\). ggplot(data = tibble(x = seq(from = 100, to = 250, by = .1)), aes(x = x, y = dnorm(x, mean = 178, sd = 20))) + geom_line() + ylab(&quot;density&quot;) And here’s the ggplot2 code for our prior for \\(\\sigma\\), a uniform distribution with a minimum value of 0 and a maximum value of 50. We don’t really need the y axis when looking at the shapes of a density, so we’ll just remove it with scale_y_continuous(). tibble(x = seq(from = -10, to = 60, by = .1)) %&gt;% ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) + geom_line() + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) We can simulate from both priors at once to get a prior probability distribution of heights. n &lt;- 1e4 set.seed(4) tibble(sample_mu = rnorm(n, mean = 178, sd = 20), sample_sigma = runif(n, min = 0, max = 50)) %&gt;% mutate(x = rnorm(n, mean = sample_mu, sd = sample_sigma)) %&gt;% ggplot(aes(x = x)) + geom_density(fill = &quot;black&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(paste(&quot;Prior predictive distribution for &quot;, italic(h[i]))), x = NULL) + theme(panel.grid = element_blank()) As McElreath wrote, we’ve made a “vaguely bell-shaped density with thick tails. It is the expected distribution of heights, averaged over the prior” (p. 83). 4.3.3 Grid approximation of the posterior distribution. As McElreath explained, you’ll never use this for practical data analysis. But I found this helped me better understanding what exactly we’re doing with Bayesian estimation. So let’s play along. n &lt;- 200 d_grid &lt;- tibble(mu = seq(from = 140, to = 160, length.out = n), sigma = seq(from = 4, to = 9, length.out = n)) %&gt;% # we&#39;ll accomplish with `tidyr::expand()` what McElreath did with base R `expand.grid()` expand(mu, sigma) head(d_grid) ## # A tibble: 6 x 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 ## 2 140 4.03 ## 3 140 4.05 ## 4 140 4.08 ## 5 140 4.10 ## 6 140 4.13 d_grid contains every combination of mu and sigma across their specified values. Instead of base R sapply(), we’ll do the computateions by making a custom function which we’ll plug into purrr::map2(). grid_function &lt;- function(mu, sigma){ dnorm(d2$height, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’re ready to complete the tibble. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;% unnest() %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) head(d_grid) ## # A tibble: 6 x 7 ## mu sigma log_likelihood prior_mu prior_sigma product probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 -3813. -5.72 -3.91 -3822. 0 ## 2 140 4.03 -3778. -5.72 -3.91 -3787. 0 ## 3 140 4.05 -3743. -5.72 -3.91 -3753. 0 ## 4 140 4.08 -3709. -5.72 -3.91 -3719. 0 ## 5 140 4.10 -3676. -5.72 -3.91 -3686. 0 ## 6 140 4.13 -3644. -5.72 -3.91 -3653. 0 In the final d_grid, the probability vector contains the posterior probabilities across values of mu and sigma. We can make a contour plot with geom_contour(). d_grid %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_contour() + labs(x = expression(mu), y = expression(sigma)) + coord_cartesian(xlim = range(d_grid$mu), ylim = range(d_grid$sigma)) + theme(panel.grid = element_blank()) We’ll make our heat map with geom_raster(aes(fill = probability)). d_grid %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_raster(aes(fill = probability), interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(mu), y = expression(sigma)) + theme(panel.grid = element_blank()) 4.3.4 Sampling from the posterior. We can use dplyr::sample_n() to sample rows, with replacement, from d_grid. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) We can use gather() and then facet_warp() to plot the densities for both mu and sigma at once. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free&quot;) We’ll use the tidybayes package to compute their posterior modes and 95% HDIs. library(tidybayes) d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) ## # A tibble: 2 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.95 mode hdi ## 2 sigma 7.82 7.14 8.30 0.95 mode hdi Let’s say you wanted their posterior medians and 50% quantile-based intervals, instead. Just switch out the last line for median_qi(value, .width = .5). 4.3.4.1 Overthinking: Sample size and the normality of \\(\\sigma\\)’s posterior. Since we’ll be fitting models with brms almost exclusively from here on out, this section is largely mute. But we’ll do it anyway for the sake of practice. I’m going to break the steps up like before rather than compress the code together. Here’s d3. set.seed(4) (d3 &lt;- sample(d2$height, size = 20)) ## [1] 147.3200 154.9400 168.9100 156.8450 165.7350 151.7650 165.7350 156.2100 144.7800 154.9400 ## [11] 151.1300 147.9550 149.8600 162.5600 161.9250 164.4650 160.9852 151.7650 163.8300 149.8600 For our first step using d3, we’ll redefine d_grid. n &lt;- 200 # note we&#39;ve redefined the ranges of `mu` and `sigma` d_grid &lt;- tibble(mu = seq(from = 150, to = 170, length.out = n), sigma = seq(from = 4, to = 20, length.out = n)) %&gt;% expand(mu, sigma) Second, we’ll redefine our custom grid_function() function to operate over the height values of d3. grid_function &lt;- function(mu, sigma){ dnorm(d3, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’ll use the amended grid_function() to make the posterior. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) Did you catch our use of purrr::map2_dbl(), there, in place of purrr::map2()? It turns out that purrr::map() and purrr::map2() always return a list (see here and here). But as Phil Straforelli kindly pointed out, we can add the _dbl suffix to those functions, which will instruct the purrr package to return a double vector (i.e., a common kind of numeric vector). The advantage of that approach is we no longer need to follow our map() or map2() lines with unnest(). To learn more about the ins and outs of the map() family, check out this section from R4DS or Jenny Bryan’s purrr tutorial. Next we’ll sample_n() and plot. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) Behold the updated densities. d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales= &quot;free&quot;) Sigma’s not so Gaussian with that small \\(n\\). 4.3.5 Fitting the model with map() brm(). We won’t actually use rethinking::map()–which you should not conflate with purrr::map()–, but will jumpt straight to the primary brms modeling function, brm(). In the text, McElreath indexed his models with names like m4.1. I will largely follow that convention, but will replace the m with a b to stand for the brms package. Here’s the first model. b4.1 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 4, cores = 4, seed = 4) ## Warning: There were 187 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See ## http://mc-stan.org/misc/warnings.html#bfmi-low ## Warning: Examine the pairs() plot to diagnose sampling problems McElreath’s uniform prior for \\(\\sigma\\) was rough on brms. It took an unusually-large number of warmup iterations before the chains sampled properly. As McElreath covered in Chapter 8, HMC tends to work better when you default to a half Cauchy for \\(\\sigma\\). Here’s how to do so. b4.1_half_cauchy &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) This leads to an important point. After running model with Hamiltonian Monte Carlo (HMC), it’s a good idea to inspect the chains. As we’ll see, McElreath coverd this in Chapter 8. Here’s a typical way to do so in brms. plot(b4.1_half_cauchy) If you want detailed diagnostics for the HMC chains, call launch_shinystan(b4.1). That’ll keep you busy for a while. But anyway, the chains look good. We can reasonably trust the results. Here’s how to get the model summary of our brm() object. print(b4.1_half_cauchy) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 154.60 0.41 153.77 155.40 2814 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 7.75 0.29 7.20 8.33 3087 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The summary() function works in a similar way. You can also get a Stan-like summary with this: b4.1_half_cauchy$fit ## Inference for Stan model: 111e8cc5645009df70b928231cdbcf0c. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 154.60 0.01 0.41 153.77 154.34 154.61 154.88 155.40 2814 1 ## sigma 7.75 0.01 0.29 7.20 7.55 7.75 7.93 8.33 3087 1 ## lp__ -1227.50 0.03 1.01 -1230.23 -1227.83 -1227.18 -1226.79 -1226.54 1522 1 ## ## Samples were drawn using NUTS(diag_e) at Wed Jun 19 10:25:09 2019. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Whereas rethinking defaults to 89% intervals, using print() or summary() with brms models defaults to 95% intervals. Unless otherwise specified, I will stick with 95% intervals throughout. However, if you really want those 89% intervals, an easy way is with the prob argument within brms::summary() or brms::print(). summary(b4.1_half_cauchy, prob = .89) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat ## Intercept 154.60 0.41 153.95 155.25 2814 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-89% CI u-89% CI Eff.Sample Rhat ## sigma 7.75 0.29 7.31 8.21 3087 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Anyways, here’s the shockingly-narrow-\\(\\mu\\)-prior model. b4.2 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, .1), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 3000, warmup = 2000, chains = 4, cores = 4, seed = 4) Check the chains. plot(b4.2) I had to increase the warmup due to convergence issues. After doing so, everything looks to be on the up and up. The chains look great. And again, to learn more about these technical details, check out Chapter 8. Here’s the model summary(). summary(b4.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 3000; warmup = 2000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 177.87 0.10 177.67 178.06 3254 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 24.62 0.98 22.86 26.72 1309 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.3.6 Sampling from a map() brm() fit. brms doesn’t seem to have a convenience function that works the way vcov() does for rethinking. For example: vcov(b4.1_half_cauchy) ## Intercept ## Intercept 0.1685854 This only returns the first element in the matrix it did for rethinking. That is, it appears brms::vcov() only returns the variance/covariance matrix for the single-level \\(\\beta\\) parameters (i.e., those used to model \\(\\mu\\)). However, if you really wanted this information, you could get it after putting the HMC chains in a data frame. post &lt;- posterior_samples(b4.1_half_cauchy) head(post) ## b_Intercept sigma lp__ ## 1 153.8263 8.111452 -1228.985 ## 2 155.2687 7.331025 -1228.936 ## 3 153.7654 8.195435 -1229.588 ## 4 153.5382 8.131402 -1230.481 ## 5 153.7429 8.219945 -1229.798 ## 6 154.6138 7.182985 -1228.449 Now select() the columns containing the draws from the desired parameters and feed them into cof(). select(post, b_Intercept:sigma) %&gt;% cov() ## b_Intercept sigma ## b_Intercept 0.1685853631 0.0005206241 ## sigma 0.0005206241 0.0825989429 That was “(1) a vector of variances for the parameters and (2) a correlation matrix” for them (p. 90). Here are just the variances (i.e., the diagonal elements) and the correlation matrix. # variances select(post, b_Intercept:sigma) %&gt;% cov() %&gt;% diag() ## b_Intercept sigma ## 0.16858536 0.08259894 # correlation post %&gt;% select(b_Intercept, sigma) %&gt;% cor() ## b_Intercept sigma ## b_Intercept 1.000000000 0.004411915 ## sigma 0.004411915 1.000000000 With our post &lt;- posterior_samples(b4.1_half_cauchy) code from a few lines above, we’ve already done the brms version of what McElreath did with extract.samples() on page 90. However, what happened under the hood was different. Whereas rethinking used the mvnorm() function from the MASS package, in brms we just extracted the iterations of the HMC chains and put them in a data frame. str(post) ## &#39;data.frame&#39;: 4000 obs. of 3 variables: ## $ b_Intercept: num 154 155 154 154 154 ... ## $ sigma : num 8.11 7.33 8.2 8.13 8.22 ... ## $ lp__ : num -1229 -1229 -1230 -1230 -1230 ... Notice how our data frame, post, includes a third vector, lp__. That’s the log posterior. See the brms reference manual or the “The Log-Posterior (function and gradient)” section of the Stan Development Team’s RStan: the R interface to Stan for details. The log posterior will largely be outside of our focus in this project. The summary() function doesn’t work for brms posterior data frames quite the way precis() does for posterior data frames from the rethinking package. E.g., summary(post[, 1:2]) ## b_Intercept sigma ## Min. :153.0 Min. :6.806 ## 1st Qu.:154.3 1st Qu.:7.547 ## Median :154.6 Median :7.746 ## Mean :154.6 Mean :7.750 ## 3rd Qu.:154.9 3rd Qu.:7.935 ## Max. :156.1 Max. :8.905 Here’s one option using the transpose of a quantile() call nested within apply(), which is a very general function you can learn more about here or here. t(apply(post[, 1:2], 2, quantile, probs = c(.5, .025, .75))) ## 50% 2.5% 75% ## b_Intercept 154.605100 153.76758 154.881642 ## sigma 7.746396 7.19974 7.934555 The base R code is compact, but somewhat opaque. Here’s how to do something similar with more explicit tidyverse code. post %&gt;% select(-lp__) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% summarise(mean = mean(value), SD = sd(value), `2.5_percentile` = quantile(value, probs = .025), `97.5_percentile` = quantile(value, probs = .975)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## # A tibble: 2 x 5 ## parameter mean SD `2.5_percentile` `97.5_percentile` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 155. 0.41 154. 155. ## 2 sigma 7.75 0.290 7.2 8.33 You can always get pretty similar information by just putting the brm() fit object into posterior_summary(). posterior_summary(b4.1_half_cauchy) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.604871 0.4105915 153.76758 155.399422 ## sigma 7.749564 0.2874003 7.19974 8.333512 ## lp__ -1227.496120 1.0078995 -1230.23104 -1226.541493 And if you’re willing to drop the posterior \\(SD\\)s, you can use tidybayes::mean_qi(), too. post %&gt;% select(-lp__) %&gt;% gather(parameter) %&gt;% group_by(parameter) %&gt;% mean_qi(value) ## # A tibble: 2 x 7 ## parameter value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 155. 154. 155. 0.95 mean qi ## 2 sigma 7.75 7.20 8.33 0.95 mean qi 4.3.6.1 Overthinking: Under the hood with multivariate sampling. Again, brms::posterior_samples() is not the same as rethinking::extract.samples(). Rather than use the MASS::mvnorm(), brms takes the iterations from the HMC chains. McElreath coverd all of this in Chapter 8. You might also look at the brms reference manual or GitHub page for details. 4.3.6.2 Overthinking: Getting \\(\\sigma\\) right. There’s no need to fret about this in brms. With HMC, we are not constraining the posteriors to the multivariate normal distribution. Here’s our posterior density for \\(\\sigma\\). ggplot(data = post, aes(x = sigma)) + geom_density(size = 1/10, fill = &quot;black&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(sigma)) + theme(panel.grid = element_blank()) See? HMC handled the mild skew just fine. But sometimes you want to actually model \\(\\sigma\\), such as in the case where your variances are systematically heterogeneous. Bürkner calls these kinds of models distributional models, which you can learn more about in his vignette Estimating Distributional Models with brms. As he explained in the vignette, you actually model \\(\\text{log}(\\sigma)\\) in those instances. If you’re curious, we’ll practice with a model like this in Chapter 9. 4.4 Adding a predictor Here’s our scatter plot of weight and height. ggplot(data = d2, aes(x = weight, y = height)) + geom_point(shape = 1, size = 2) + theme_bw() + theme(panel.grid = element_blank()) 4.4.1 The linear model strategy In our new univariable model \\[\\begin{align*} h_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta x_i \\\\ \\alpha &amp; \\sim \\text{Normal}(178, 100) \\\\ \\beta &amp; \\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 50) \\end{align*}\\] 4.4.2 Fitting the model. The brms::brm() syntax doesn’t mirror the statistical notation. But here are the analogues to the exposition at the bottom of page 95. \\(h_i \\sim \\text{Normal}(\\mu_i, \\sigma)\\): family = gaussian \\(\\mu_i = \\alpha + \\beta x_i\\): height ~ 1 + weight \\(\\alpha \\sim \\text{Normal}(156, 100)\\): prior(normal(156, 100), class = Intercept \\(\\beta \\sim \\text{Normal}(0, 10)\\): prior(normal(0, 10), class = b) \\(\\sigma \\sim \\text{Uniform}(0, 50)\\): prior(uniform(0, 50), class = sigma) Thus, to add a predictor you just the + operator in the model formula. b4.3 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight, prior = c(prior(normal(156, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 41000, warmup = 40000, chains = 4, cores = 4, seed = 4) This was another example of how using a uniform prior for \\(\\sigma\\) required we use an unusually large number of warmup iterations before the HMC chains converged on the posterior. Change the prior to cauchy(0, 1) and the chains converge with no problem, resulting in much better effective samples, too. Here are the trace plots. plot(b4.3) 4.4.3 Interpreting the model fit. “One trouble with statistical models is that they are hard to understand” (p. 97). Welcome to the world of applied statistics. 4.4.3.1 Tables of estimates. With a little [] subsetting we can exclude the log posterior from the summary. posterior_summary(b4.3)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 113.8319583 1.91816993 110.2510602 117.5964329 ## b_weight 0.9060844 0.04219883 0.8223749 0.9850337 ## sigma 5.1058421 0.18979494 4.7541189 5.5007869 Again, brms doesn’t have a convenient corr = TRUE argument for plot() or summary(). But you can get that information after putting the chains in a data frame. posterior_samples(b4.3) %&gt;% select(-lp__) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight sigma ## b_Intercept 1.00 -0.99 0 ## b_weight -0.99 1.00 0 ## sigma 0.00 0.00 1 With centering, we can reduce the correlations among the parameters. d2 &lt;- d2 %&gt;% mutate(weight_c = weight - mean(weight)) Fit the weight_c model, b4.4. b4.4 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 50), class = sigma)), iter = 46000, warmup = 45000, chains = 4, cores = 4, seed = 4) plot(b4.4) posterior_summary(b4.4)[1:3, ] ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.5923964 0.2711724 154.0476798 155.1125773 ## b_weight_c 0.9043183 0.0407299 0.8262867 0.9827456 ## sigma 5.1102025 0.1963311 4.7484643 5.5003165 Like before, the uniform prior required extensive warmup iterations to produce a good posterior. This is easily fixed using a half Cauchy prior, instead. Anyways, the effective samples improved. Here’s the parameter correlation info. posterior_samples(b4.4) %&gt;% select(-lp__) %&gt;% cor() %&gt;% round(digits = 2) ## b_Intercept b_weight_c sigma ## b_Intercept 1.00 0.02 0.00 ## b_weight_c 0.02 1.00 0.03 ## sigma 0.00 0.03 1.00 See? Now all the correlations are quite low. Also, if you prefer a visual approach, you might do pairs(b4.4). 4.4.3.2 Plotting posterior inference against the data. Here is the code for Figure 4.4. Note our use of the fixef() function. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_abline(intercept = fixef(b4.3)[1], slope = fixef(b4.3)[2]) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + theme_bw() + theme(panel.grid = element_blank()) In the brms reference manual, Bürkner described the job of thefixef() function as “extract[ing] the population-level (’fixed’) effects from a brmsfit object”. If you’re new to multilevel models, it might not be clear what he meant by “population-level” or “fixed” effects. Don’t worry. That’ll all become clear starting around Chapter 12. In the meantime, just think of them as the typical regression parameters, minus \\(\\sigma\\). 4.4.3.3 Adding uncertainty around the mean. Be default, we extract all the posterior iterations with posterior_samples(). post &lt;- posterior_samples(b4.3) post %&gt;% slice(1:5) # this serves a similar function as `head()` ## b_Intercept b_weight sigma lp__ ## 1 111.0401 0.9674899 4.866805 -1083.920 ## 2 113.0507 0.9124175 5.620328 -1086.802 ## 3 114.1971 0.9031358 5.169781 -1082.515 ## 4 114.4832 0.8863125 5.119083 -1082.550 ## 5 114.4585 0.8886638 5.160552 -1082.374 Here are the four models leading up to McElreath’s Figure 4.5. To reduce my computation time, I used a half Cauchy(0, 1) prior on \\(\\sigma\\). If you are willing to wait for the warmups, switching that out for McElreath’s uniform prior should work fine as well. n &lt;- 10 b.10 &lt;- brm(data = d2 %&gt;% slice(1:n), # note our tricky use of `n` and `slice()` family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) n &lt;- 50 b.50 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) n &lt;- 150 b.150 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) n &lt;- 352 b.352 &lt;- brm(data = d2 %&gt;% slice(1:n), family = gaussian, height ~ 1 + weight, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) I’m not going to clutter up the document with all the trace plots and coefficient summaries from these four models. But here’s how to get that information. plot(b.10) print(b.10) plot(b.50) print(b.50) plot(b.150) print(b.150) plot(b.352) print(b.352) We’ll need to put the chains of each model into data frames. post10 &lt;- posterior_samples(b.10) post50 &lt;- posterior_samples(b.50) post150 &lt;- posterior_samples(b.150) post352 &lt;- posterior_samples(b.352) Here is the code for the four individual plots. p10 &lt;- ggplot(data = d2[1:10 , ], aes(x = weight, y = height)) + geom_abline(intercept = post10[1:20, 1], slope = post10[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 10&quot;) + theme_bw() + theme(panel.grid = element_blank()) p50 &lt;- ggplot(data = d2[1:50 , ], aes(x = weight, y = height)) + geom_abline(intercept = post50[1:20, 1], slope = post50[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 50&quot;) + theme_bw() + theme(panel.grid = element_blank()) p150 &lt;- ggplot(data = d2[1:150 , ], aes(x = weight, y = height)) + geom_abline(intercept = post150[1:20, 1], slope = post150[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 150&quot;) + theme_bw() + theme(panel.grid = element_blank()) p352 &lt;- ggplot(data = d2[1:352 , ], aes(x = weight, y = height)) + geom_abline(intercept = post352[1:20, 1], slope = post352[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + labs(subtitle = &quot;N = 352&quot;) + theme_bw() + theme(panel.grid = element_blank()) Note how we used the good old bracket syntax (e.g., d2[1:10 , ]) to index rows from our d2 data. With tidyverse-style syntax, we could have done slice(d2, 1:10) or d2 %&gt;% slice(1:10) instead. Anyway, we saved each of these plots as objects. With a little help of the multiplot() function we are going to arrange those plot objects into a grid in order to reproduce Figure 4.5. Behold the code for the multiplot() function: multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) { library(grid) # Make a list from the ... arguments and plotlist plots &lt;- c(list(...), plotlist) numPlots = length(plots) # If layout is NULL, then use &#39;cols&#39; to determine layout if (is.null(layout)) { # Make the panel # ncol: Number of columns of plots # nrow: Number of rows needed, calculated from # of cols layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)), ncol = cols, nrow = ceiling(numPlots/cols)) } if (numPlots==1) { print(plots[[1]]) } else { # Set up the page grid.newpage() pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout)))) # Make each plot, in the correct location for (i in 1:numPlots) { # Get the i,j matrix positions of the regions that contain this subplot matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE)) print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row, layout.pos.col = matchidx$col)) } } } We’re finally ready to use multiplot() to make Figure 4.5. multiplot(p10, p150, p50, p352, cols = 2) 4.4.3.4 Plotting regression intervals and contours. Remember, if you want to plot McElreath’s mu_at_50 with ggplot2, you’ll need to save it as a data frame or a tibble. mu_at_50 &lt;- post %&gt;% transmute(mu_at_50 = b_Intercept + b_weight * 50) head(mu_at_50) ## mu_at_50 ## 1 159.4146 ## 2 158.6715 ## 3 159.3539 ## 4 158.7988 ## 5 158.8916 ## 6 159.0464 And here is a version McElreath’s Figure 4.6 density plot. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() We’ll use mean_hdi() to get both 89% and 95% HPDIs along with the mean. mean_hdi(mu_at_50[,1], .width = c(.89, .95)) ## y ymin ymax .width .point .interval ## 1 159.1362 158.6041 159.6977 0.89 mean hdi ## 2 159.1362 158.4937 159.8326 0.95 mean hdi If you wanted to express those sweet 95% HPDIs on your density plot, you might use tidybayes::stat_pointintervalh(). Since stat_pointintervalh() also returns a point estimate, we’ll throw in the mode. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() In brms, you would use fitted() to do what McElreath accomplished with link(). mu &lt;- fitted(b4.3, summary = F) str(mu) ## num [1:4000, 1:352] 157 157 157 157 157 ... When you specify summary = F, fitted() returns a matrix of values with as many rows as there were post-warmup iterations across your HMC chains and as many columns as there were cases in your data. Because we had 4000 post-warmup iterations and \\(n\\) = 352, fitted() returned a matrix of 4000 rows and 352 vectors. If you omitted the summary = F argument, the default is TRUE and fitted() will return summary information instead. Much like rethinking’s link(), fitted() can accommodate custom predictor values with its newdata argument. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) mu &lt;- fitted(b4.3, summary = F, newdata = weight_seq) %&gt;% as_tibble() %&gt;% # here we name the columns after the `weight` values from which they were computed set_names(25:70) %&gt;% mutate(iter = 1:n()) str(mu) Anticipating ggplot2, we went ahead and converted the output to a tibble. But we might do a little more data processing with the aid of tidyr::gather(). With the gather() function, we’ll convert the data from the wide format to the long format. If you’re new to the distinction between wide and long data, you can learn more here or here. mu &lt;- mu %&gt;% gather(weight, height, -iter) %&gt;% # We might reformat `weight` to numerals mutate(weight = as.numeric(weight)) head(mu) ## # A tibble: 6 x 3 ## iter weight height ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 25 135. ## 2 2 25 136. ## 3 3 25 137. ## 4 4 25 137. ## 5 5 25 137. ## 6 6 25 137. Enough data processing. Here we reproduce McElreath’s Figure 4.7.a. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(iter &lt; 101), alpha = .1) # or prettied up a bit d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(iter &lt; 101), color = &quot;navyblue&quot;, alpha = .05) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) With fitted(), it’s quite easy to plot a regression line and its intervals. Just omit the summary = T argument. mu_summary &lt;- fitted(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% # let&#39;s tack on the `weight` values from `weight_seq` bind_cols(weight_seq) head(mu_summary) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 136. 0.886 135. 138. 25 ## 2 137. 0.846 136. 139. 26 ## 3 138. 0.806 137. 140. 27 ## 4 139. 0.767 138. 141. 28 ## 5 140. 0.727 139. 142. 29 ## 6 141. 0.688 140. 142. 30 Here it is, our analogue to Figure 4.7.b. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) And if you wanted to use intervals other than the default 95% ones, you’d enter a probs argument like this: fitted(b4.3, newdata = weight.seq, probs = c(.25, .75)). The resulting third and fourth vectors from the fitted() object would be named Q25 and Q75 instead of the default Q2.5 and Q97.5. The Q prefix stands for quantile. 4.4.3.4.1 Overthinking: How link fitted() works. Similar to rethinking::link(), brms::fitted() uses the formula from your model to compute the model expectations for a given set of predictor values. I use it a lot in this project. If you follow along, you’ll get a good handle on it. 4.4.3.5 Prediction intervals. Even though our full statistical model (omitting priors for the sake of simplicity) is \\[h_i \\sim \\text{Normal}(\\mu_i = \\alpha + \\beta x_, \\sigma)\\] we’ve only been plotting the \\(\\mu\\) part. In order to bring in the variability expressed by \\(\\sigma\\), we’ll have to switch to predict(). Much as brms::fitted() was our analogue to rethinking::link(), brms::predict() is our analogue to rethinking::sim(). We can reuse our weight_seq data from before. But in case you forgot, here’s that code again. weight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1)) The predict() code looks a lot like what we used for fitted(). pred_height &lt;- predict(b4.3, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_height %&gt;% slice(1:6) ## # A tibble: 6 x 5 ## Estimate Est.Error Q2.5 Q97.5 weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 136. 5.16 126. 147. 25 ## 2 137. 5.17 127. 148. 26 ## 3 138. 5.19 128. 148. 27 ## 4 139. 5.17 129. 149. 28 ## 5 140. 5.14 130. 150. 29 ## 6 141. 5.18 131. 151. 30 This time the summary information in our data frame is for, as McElreath put it, “simulated heights, not distributions of plausible average height, \\(\\mu\\)” (p. 108). Another way of saying that is that these simulations are the joint consequence of both \\(\\mu\\) and \\(\\sigma\\), unlike the results of fitted(), which only reflect \\(\\mu\\). Our plot for Figure 4.8: d2 %&gt;% ggplot(aes(x = weight)) + geom_ribbon(data = pred_height, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5 Polynomial regression Remember d? d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 14… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0… The quadratic is probably the most commonly used polynomial regression model. \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2\\] McElreath warned: “Fitting these models to data is easy. Interpreting them can be hard” (p. 111). Standardizing will help brm() fit the model. We might standardize our weight variable like so. d &lt;- d %&gt;% mutate(weight_s = (weight - mean(weight)) / sd(weight)) Here’s the quadratic model in brms. b4.5 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) plot(b4.5) print(b4.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_s + I(weight_s^2) ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 146.67 0.39 145.92 147.44 3206 1.00 ## weight_s 21.40 0.29 20.82 21.97 3203 1.00 ## Iweight_sE2 -8.42 0.29 -8.98 -7.85 3142 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 5.77 0.18 5.45 6.13 3939 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our quadratic plot requires new fitted()- and predict()-oriented wrangling. weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) f_quad &lt;- fitted(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p_quad &lt;- predict(b4.5, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) Behold the code for our version of Figure 4.9.a. You’ll notice how little the code changed from that for Figure 4.8, above. ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_quad, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_quad, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) From a formula perspective, the cubic model is a simple extenstion of the quadratic: \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3\\] Fit it like so. b4.6 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + I(weight_s^2) + I(weight_s^3), prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) And now we’ll fit the good old linear model. b4.7 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4) Here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.c, the cubic model. f_cub &lt;- fitted(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p_cub &lt;- predict(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_cub, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_cub, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) And here’s the fitted(), predict(), and ggplot2 code for Figure 4.9.a, the linear model. f_line &lt;- fitted(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p_line &lt;- predict(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_line, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) 4.5.0.0.1 Overthinking: Converting back to natural scale. You can apply McElreath’s conversion trick within the ggplot2 environment, too. Here it is with the cubic model. at &lt;- c(-2, -1, 0, 1, 2) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = p_line, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = f_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) + # here it is! scale_x_continuous(&quot;standardized weight converted back&quot;, breaks = at, labels = round(at * sd(d$weight) + mean(d$weight), 1)) Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] rethinking_1.59 gridExtra_2.3 tidybayes_1.1.0 brms_2.9.0 Rcpp_1.0.1 ## [6] rstan_2.18.2 StanHeaders_2.18.1 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [11] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 ## [16] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] rprojroot_1.3-2 ggstance_0.3.1 markdown_1.0 ## [7] base64enc_0.1-3 rstudioapi_0.10 svUnit_0.7-12 ## [10] DT_0.7 fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 codetools_0.2-16 ## [16] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 bayesplot_1.7.0 jsonlite_1.6 ## [22] broom_0.5.2 shiny_1.3.2 compiler_3.6.0 ## [25] httr_1.4.0 backports_1.1.4 assertthat_0.2.1 ## [28] Matrix_1.2-17 lazyeval_0.2.2 cli_1.1.0 ## [31] later_0.8.0 htmltools_0.3.6 prettyunits_1.0.2 ## [34] tools_3.6.0 igraph_1.2.4.1 coda_0.19-2 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-140 ## [43] crosstalk_1.0.0 xfun_0.7 ps_1.3.0 ## [46] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 ## [49] pacman_0.5.1 gtools_3.8.1 MASS_7.3-51.4 ## [52] zoo_1.8-6 scales_1.0.0 colourpicker_1.0 ## [55] hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [61] loo_2.1.0 stringi_1.4.3 highr_0.8 ## [64] dygraphs_1.1.1.6 pkgbuild_1.0.3 rlang_0.3.4 ## [67] pkgconfig_2.0.2 matrixStats_0.54.0 HDInterval_0.2.0 ## [70] evaluate_0.14 lattice_0.20-38 rstantools_1.5.1 ## [73] htmlwidgets_1.3 labeling_0.3 processx_3.3.1 ## [76] tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 ## [79] bookdown_0.11 R6_2.4.0 generics_0.0.2 ## [82] pillar_1.4.1 haven_2.1.0 withr_2.1.2 ## [85] xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [88] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [91] rmarkdown_1.13 readxl_1.3.1 callr_3.2.0 ## [94] threejs_0.3.1 digest_0.6.19 xtable_1.8-4 ## [97] httpuv_1.5.1 stats4_3.6.0 munsell_0.5.0 ## [100] viridisLite_0.3.0 shinyjs_1.0 "],
["multivariate-linear-models.html", "Chapter 5 Multivariate Linear Models 5.1 Spurious associations 5.2 Masked relationship 5.3 Multicollinearity 5.4 Categorical varaibles 5.5 Ordinary least squares and lm() Reference Session info", " Chapter 5 Multivariate Linear Models McElreath’s listed reasons for multivaraiable regression include: statistical control for confounds multiple causation interactions We’ll approach the first two in this chapter. Interactions are reserved for Chapter 6. 5.1 Spurious associations Load the Waffle House data. library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce Unload rethinking and load brms and, while we’re at it, the tidyverse. rm(WaffleDivorce) detach(package:rethinking, unload = T) library(brms) library(tidyverse) I’m not going to show the output, but you might go ahead and investigate the data with the typical functions. E.g., head(d) glimpse(d) Now we have our data, we can reproduce Figure 5.1. One convenient way to get the handful of sate labels into the plot was with the geom_text_repel() function from the ggrepel package. But first, we spent the last few chapters warming up with ggplot2. Going forward, each chapter will have its own plot theme. In this chapter, we’ll characterize the plots with theme_bw() + theme(panel.grid = element_rect()) and coloring based off of &quot;firebrick&quot;. # install.packages(&quot;ggrepel&quot;, depencencies = T) library(ggrepel) d %&gt;% ggplot(aes(x = WaffleHouses/Population, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, size = 1/2, color = &quot;firebrick4&quot;, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + geom_text_repel(data = d %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;OK&quot;, &quot;AR&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;NJ&quot;)), aes(label = Loc), size = 3, seed = 1042) + # this makes it reproducible scale_x_continuous(&quot;Waffle Houses per million&quot;, limits = c(0, 55)) + coord_cartesian(xlim = 0:50, ylim = 5:15) + ylab(&quot;Divorce rate&quot;) + theme_bw() + theme(panel.grid = element_blank()) With coord_map() and help from the fiftystater package (which gives us access to lat/long data for all fifty states via fifty_states), we can plot our three major variables in a map format. library(fiftystater) d %&gt;% # first we&#39;ll standardize the three variables to put them all on the same scale mutate(Divorce_z = (Divorce - mean(Divorce)) / sd(Divorce), MedianAgeMarriage_z = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage), Marriage_z = (Marriage - mean(Marriage)) / sd(Marriage), # need to make the state names lowercase to match with the map data Location = str_to_lower(Location)) %&gt;% # here we select the relevant variables and put them in the long format to facet with `facet_wrap()` select(Divorce_z:Marriage_z, Location) %&gt;% gather(key, value, -Location) %&gt;% ggplot(aes(map_id = Location)) + geom_map(aes(fill = value), map = fifty_states, color = &quot;firebrick&quot;, size = 1/15) + expand_limits(x = fifty_states$long, y = fifty_states$lat) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + scale_fill_gradient(low = &quot;#f8eaea&quot;, high = &quot;firebrick4&quot;) + coord_map() + theme_bw() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;, strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~key) One of the advantages of this visualization method is it just became clear that Nevada is missing from the WaffleDivorce data. Execute d %&gt;% distinct(Location) to see for yourself. Those missing data should motivate the skills we’ll cover in Chapter 14. But let’s get back on track. Here we’ll officially standardize the predictor, MedianAgeMarriage. d &lt;- d %&gt;% mutate(MedianAgeMarriage_s = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage)) Now we’re ready to fit the first univariable model. b5.1 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) Check the summary. print(b5.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.69 0.21 9.27 10.11 5459 1.00 ## MedianAgeMarriage_s -1.04 0.22 -1.45 -0.61 5054 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.51 0.16 1.24 1.87 4718 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll employ fitted() to make Figure 5.2.b. In preparation for fitted() we’ll make a new tibble, nd, composed of a handful of densely-packed values for our predictor, MedianAgeMarriage_s. With the newdata argument, we’ll use those values to return model-implied expected values for Divorce. # define the range of `MedianAgeMarriage_s` values we&#39;d like to feed into `fitted()` nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30)) # now use `fitted()` to get the model-implied trajectories f &lt;- fitted(b5.1, newdata = nd) %&gt;% as_tibble() %&gt;% # tack the `nd` data onto the `fitted()` results bind_cols(nd) # plot ggplot(data = f, aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = Divorce), size = 2, color = &quot;firebrick4&quot;) + ylab(&quot;Divorce&quot;) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = range(d$Divorce)) + theme_bw() + theme(panel.grid = element_blank()) Before fitting the next model, we’ll standardize Marriage. d &lt;- d %&gt;% mutate(Marriage_s = (Marriage - mean(Marriage)) / sd(Marriage)) We’re ready to fit our second univariable model. b5.2 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.69 0.25 9.21 10.17 5987 1.00 ## Marriage_s 0.64 0.25 0.15 1.12 5022 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.75 0.18 1.44 2.16 4120 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’ll wangle and plot our version of Figure 5.2.a. nd &lt;- tibble(Marriage_s = seq(from = -2.5, to = 3.5, length.out = 30)) f &lt;- fitted(b5.2, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) ggplot(data = f, aes(x = Marriage_s, y = Estimate)) + geom_smooth(aes(ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = Divorce), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(d$Marriage_s), ylim = range(d$Divorce)) + ylab(&quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) But merely comparing parameter means between different bivariate regressions is no way to decide which predictor is better Both of these predictors could provide independent value, or they could be redundant, or one could eliminate the value of the other. So we’ll build a multivariate model with the goal of measuring the partial value of each predictor. The question we want answered is: What is the predictive value of a variable, once I already know all of the other predictor variables? (p. 123, emphasis in the original) 5.1.1 Multivariate notation. Now we’ll get both predictors in there with our very first multivariable model. We can write the statistical model as \\[\\begin{align*} \\text{Divorce}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{Marriage_s}_i + \\beta_2 \\text{MedianAgeMarriage_s}_i \\\\ \\alpha &amp; \\sim \\text{Normal}(10, 10) \\\\ \\beta_1 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 10) \\end{align*}\\] It might help to read the \\(+\\) symbols as “or” and then say: A State’s divorce rate can be a function of its marriage rate or its median age at marriage. The “or” indicates independent associations, which may be purely statistical or rather causal. (p. 124, emphasis in the original) 5.1.2 Fitting the model. Much like we used the + operator to add single predictors to the intercept, we just use more + operators in the formula argument to add more predictors. Also notice we’re using the same prior prior(normal(0, 1), class = b) for both predictors. Within the brms framework, they are both of class = b. But if we wanted their priors to differ, we’d make two prior() statements and differentiate them with the coef argument. You’ll see examples of that later on. b5.3 &lt;- brm(data = d, family = gaussian, Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s, prior = c(prior(normal(10, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 9.69 0.22 9.27 10.11 5724 1.00 ## Marriage_s -0.12 0.30 -0.69 0.47 4307 1.00 ## MedianAgeMarriage_s -1.12 0.30 -1.71 -0.52 4514 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.52 0.16 1.25 1.86 5188 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The stanplot() function is an easy way to get a default coefficient plot. You just put the brmsfit object into the function. stanplot(b5.3) There are numerous ways to make a coefficient plot. Another is with the mcmc_intervals() function from the bayesplot package. A nice feature of the bayesplot package is its convenient way to alter the color scheme with the color_scheme_set() function. Here, for example, we’ll make the theme red. But note how the mcmc_intervals() function requires you to work with the posterior_samples() instead of the brmsfit object. # install.packages(&quot;bayesplot&quot;, dependencies = T) library(bayesplot) post &lt;- posterior_samples(b5.3) color_scheme_set(&quot;red&quot;) mcmc_intervals(post[, 1:4], prob = .5, point_est = &quot;median&quot;) + labs(title = &quot;My fancy bayesplot-based coefficient plot&quot;) + theme(axis.text.y = element_text(hjust = 0), axis.line.x = element_line(size = 1/4), axis.line.y = element_blank(), axis.ticks.y = element_blank()) Because bayesplot produces a ggplot2 object, the plot was adjustable with familiar ggplot2 syntax. For more ideas, check out this vignette. The tidybaes::stat_pointintervalh() function offers a third way, this time with a more ground-up ggplot2 workflow. library(tidybayes) post %&gt;% select(-lp__) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = reorder(key, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = &quot;firebrick4&quot;, alpha = 1/10) + stat_pointintervalh(point_interval = mode_hdi, .width = .95, size = 3/4, color = &quot;firebrick4&quot;) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = NULL, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), panel.grid.major.y = element_line(color = alpha(&quot;firebrick4&quot;, 1/4), linetype = 3), axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) The substantive interpretation of all those coefficient plots is: “Once we know median age at marriage for a State, there is little or no additive predictive power in also knowing the rate of marriage in that State” (p. 126, emphasis in the original). 5.1.3 Plotting multivariate posteriors. McElreath’s prose is delightfully deflationary. “There is a huge literature detailing a variety of plotting techniques that all attempt to help one understand multiple linear regression. None of these techniques is suitable for all jobs, and most do not generalize beyond linear regression” (p. 126). Now you’re inspired, let’s learn three: Predictor residual plots Counterfactual plots Posterior prediction plots 5.1.3.1 Predictor residual plots. To get ready to make our residual plots, we’ll predict Marriage_s with MedianAgeMarriage_s. b5.4 &lt;- brm(data = d, family = gaussian, Marriage_s ~ 1 + MedianAgeMarriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Marriage_s ~ 1 + MedianAgeMarriage_s ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -0.00 0.10 -0.21 0.20 6246 1.00 ## MedianAgeMarriage_s -0.71 0.10 -0.91 -0.52 5928 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.72 0.07 0.59 0.88 5243 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With fitted(), we compute the expected values for each state (with the exception of Nevada). Since the MedianAgeMarriage_s values for each state are in the date we used to fit the model, we’ll omit the newdata argument. f &lt;- fitted(b5.4) %&gt;% as_tibble() %&gt;% bind_cols(d) head(f) ## # A tibble: 6 x 19 ## Estimate Est.Error Q2.5 Q97.5 Location Loc Population MedianAgeMarria… Marriage Marriage.SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.432 0.119 0.196 0.664 Alabama AL 4.78 25.3 20.2 1.27 ## 2 0.489 0.123 0.243 0.729 Alaska AK 0.71 25.2 26 2.93 ## 3 0.145 0.105 -0.0622 0.352 Arizona AZ 6.33 25.8 20.3 0.98 ## 4 1.01 0.173 0.664 1.34 Arkansas AR 2.92 24.3 26.4 1.7 ## 5 -0.429 0.120 -0.664 -0.194 Califor… CA 37.2 26.8 19.1 0.39 ## 6 0.202 0.107 -0.00633 0.411 Colorado CO 5.03 25.7 23.5 1.24 ## # … with 9 more variables: Divorce &lt;dbl&gt;, Divorce.SE &lt;dbl&gt;, WaffleHouses &lt;int&gt;, South &lt;int&gt;, ## # Slaves1860 &lt;int&gt;, Population1860 &lt;int&gt;, PropSlaves1860 &lt;dbl&gt;, MedianAgeMarriage_s &lt;dbl&gt;, ## # Marriage_s &lt;dbl&gt; After a little data processing, we can make Figure 5.3. f %&gt;% ggplot(aes(x = MedianAgeMarriage_s, y = Marriage_s)) + geom_point(size = 2, shape = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(xend = MedianAgeMarriage_s, yend = Estimate), size = 1/4) + geom_line(aes(y = Estimate), color = &quot;firebrick4&quot;) + coord_cartesian(ylim = range(d$Marriage_s)) + theme_bw() + theme(panel.grid = element_blank()) We get the residuals with the well-named residuals() function. Much like with brms::fitted(), brms::residuals() returns a four-vector matrix with the number of rows equal to the number of observations in the original data (by default, anyway). The vectors have the familiar names: Estimate, Est.Error, Q2.5, and Q97.5. See the brms reference manual for details. With our residuals in hand, we just need a little more data processing to make Figure 5.4.a. r &lt;- residuals(b5.4) %&gt;% # to use this in ggplot2, we need to make it a tibble or data frame as_tibble() %&gt;% bind_cols(d) # for the annotation at the top text &lt;- tibble(Estimate = c(- 0.5, 0.5), Divorce = 14.1, label = c(&quot;slower&quot;, &quot;faster&quot;)) # plot r %&gt;% ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(&quot;Marriage rate residuals&quot;, limits = c(-2, 2)) + coord_cartesian(xlim = range(r$Estimate), ylim = c(6, 14.1)) + theme_bw() + theme(panel.grid = element_blank()) To get the MedianAgeMarriage_s residuals, we have to fit the corresponding model first. b5.4b &lt;- brm(data = d, family = gaussian, MedianAgeMarriage_s ~ 1 + Marriage_s, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) And now we’ll get the new batch of residuals, do a little data processing, and make a plot corresponding to Figure 5.4.b. text &lt;- tibble(Estimate = c(- 0.7, 0.5), Divorce = 14.1, label = c(&quot;younger&quot;, &quot;older&quot;)) residuals(b5.4b) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Estimate, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text(data = text, aes(label = label)) + scale_x_continuous(&quot;Age of marriage residuals&quot;, limits = c(-2, 3)) + coord_cartesian(xlim = range(r$Estimate), ylim = c(6, 14.1)) + theme_bw() + theme(panel.grid = element_blank()) 5.1.3.2 Counterfactual plots. A second sort of inferential plot displays the implied predictions of the model. I call these plots counterfactual, because they can be produced for any values of the predictor variable you like, even unobserved or impossible combinations like very high median age of marriage and very high marriage rate. There are no States with this combination, but in a counterfactual plot, you can ask the model for a prediction for such a State. (p. 129, emphasis in the original) Making Figure 5.5.a requires a little more data wrangling than before. # we need new `nd` data nd &lt;- tibble(Marriage_s = seq(from = -3, to = 3, length.out = 30), MedianAgeMarriage_s = mean(d$MedianAgeMarriage_s)) fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # since `fitted()` and `predict()` name their intervals the same way, # we&#39;ll need to `rename()` them to keep them straight rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% # note how we&#39;re just nesting the `predict()` code right inside `bind_cols()` bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% # since we only need the intervals, we&#39;ll use `transmute()` rather than `mutate()` transmute(p_ll = Q2.5, p_ul = Q97.5), # now tack on the `nd` data nd) %&gt;% # we&#39;re finally ready to plot ggplot(aes(x = Marriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = f_ll, ymax = f_ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + coord_cartesian(xlim = range(d$Marriage_s), ylim = c(6, 14)) + labs(subtitle = &quot;Counterfactual plot for which\\nMedianAgeMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) We follow the same process for Figure 5.5.b. # new data nd &lt;- tibble(MedianAgeMarriage_s = seq(from = -3, to = 3.5, length.out = 30), Marriage_s = mean(d$Marriage_s)) # `fitted()` + `predict()` fitted(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3, newdata = nd) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5), nd ) %&gt;% # plot ggplot(aes(x = MedianAgeMarriage_s, y = Estimate)) + geom_ribbon(aes(ymin = p_ll, ymax = p_ul), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = f_ll, ymax = f_ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + coord_cartesian(xlim = range(d$MedianAgeMarriage_s), ylim = c(6, 14)) + labs(subtitle = &quot;Counterfactual plot for which\\nMarriage_s = 0&quot;, y = &quot;Divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) A tension with such plots, however, lies in their counterfactual nature. In the small world of the model, it is possible to change median age of marriage without also changing the marriage rate. But is this also possible in the large world of reality? Probably not… …If our goal is to intervene in the world, there may not be any realistic way to manipulate each predictor without also manipulating the others. This is a serious obstacle to applied science, whether you are an ecologist, an economist, or an epidemiologist [or a psychologist] (p. 131) 5.1.3.3 Posterior prediction plots. “In addition to understanding the estimates, it’s important to check the model fit against the observed data” (p. 131). For more on the topic, check out Gabry and colleagues’ Visualization in Bayesian workflow or Simpson’s related blog post Touch me, I want to feel your data. In this version of Figure 5.6.a, the thin lines are the 95% intervals and the thicker lines are +/- the posterior \\(SD\\), both of which are returned when you use fitted(). fitted(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Divorce, y = Estimate)) + geom_abline(linetype = 2, color = &quot;grey50&quot;, size = .5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 3/4) + geom_linerange(aes(ymin = Q2.5, ymax = Q97.5), size = 1/4, color = &quot;firebrick4&quot;) + geom_linerange(aes(ymin = Estimate - Est.Error, ymax = Estimate + Est.Error), size = 1/2, color = &quot;firebrick4&quot;) + # Note our use of the dot placeholder, here: https://magrittr.tidyverse.org/reference/pipe.html geom_text(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;UT&quot;)), aes(label = Loc), hjust = 0, nudge_x = - 0.65) + labs(x = &quot;Observed divorce&quot;, y = &quot;Predicted divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) In order to make Figure 5.6.b, we need to clarify the relationships among fitted(), predict(), and residuals(). Here’s my attempt in a table. tibble(`brms function` = c(&quot;fitted&quot;, &quot;predict&quot;, &quot;residual&quot;), mean = c(&quot;same as the data&quot;, &quot;same as the data&quot;, &quot;in a deviance-score metric&quot;), scale = c(&quot;excludes sigma&quot;, &quot;includes sigma&quot;, &quot;excludes sigma&quot;)) %&gt;% knitr::kable() brms function mean scale fitted same as the data excludes sigma predict same as the data includes sigma residual in a deviance-score metric excludes sigma Hopefully this clarifies that if we want to incorporate the prediction interval in a deviance metric, we’ll need to first use predict() and then subtract the intervals from their corresponding Divorce values in the data. residuals(b5.3) %&gt;% as_tibble() %&gt;% rename(f_ll = Q2.5, f_ul = Q97.5) %&gt;% bind_cols( predict(b5.3) %&gt;% as_tibble() %&gt;% transmute(p_ll = Q2.5, p_ul = Q97.5), d ) %&gt;% # here we put our `predict()` intervals into a deviance metric mutate(p_ll = Divorce - p_ll, p_ul = Divorce - p_ul) %&gt;% # now plot! ggplot(aes(x = reorder(Loc, Estimate), y = Estimate)) + geom_hline(yintercept = 0, size = 1/2, color = &quot;firebrick4&quot;, alpha = 1/10) + geom_pointrange(aes(ymin = f_ll, ymax = f_ul), size = 2/5, shape = 20, color = &quot;firebrick4&quot;) + geom_segment(aes(y = Estimate - Est.Error, yend = Estimate + Est.Error, x = Loc, xend = Loc), size = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(y = p_ll, yend = p_ul, x = Loc, xend = Loc), size = 3, color = &quot;firebrick4&quot;, alpha = 1/10) + labs(x = NULL, y = NULL) + coord_flip(ylim = c(-6, 5)) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Compared to the last couple plots, Figure 5.6.c is pretty simple. residuals(b5.3) %&gt;% as_tibble() %&gt;% bind_cols(d) %&gt;% mutate(wpc = WaffleHouses / Population) %&gt;% ggplot(aes(x = wpc, y = Estimate)) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, size = 1/2, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;AR&quot;, &quot;MS&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;ID&quot;)), aes(label = Loc), seed = 5.6) + scale_x_continuous(&quot;Waffles per capita&quot;, limits = c(0, 45)) + coord_cartesian(xlim = range(0, 40)) + ylab(&quot;Divorce error&quot;) + theme_bw() + theme(panel.grid = element_blank()) More McElreath inspiration: “No matter how many predictors you’ve already included in a regression, it’s still possible to find spurious correlations with the remaining variation” (p. 134). To keep our deflation train going, it’s worthwhile to repeat the message in McElreath’s Rethinking: Stats, huh, yeah what is it good for? box. Often people want statistical modeling to do things that statistical modeling cannot do. For example, we’d like to know whether an effect is real or rather spurious. Unfortunately, modeling merely quantifies uncertainty in the precise way that the model understands the problem. Usually answers to large world questions about truth and causation depend upon information not included in the model. For example, any observed correlation between an outcome and predictor could be eliminated or reversed once another predictor is added to the model. But if we cannot think of another predictor, we might never notice this. Therefore all statistical models are vulnerable to and demand critique, regardless of the precision of their estimates and apparent accuracy of their predictions. (p. 134) 5.1.3.4 Overthinking: Simulating spurious association. n &lt;- 100 # number of cases set.seed(5) # setting the seed makes the results reproducible d &lt;- tibble(x_real = rnorm(n), # x_real as Gaussian with mean 0 and SD 1 (i.e., the defaults) x_spur = rnorm(n, x_real), # x_spur as Gaussian with mean = x_real y = rnorm(n, x_real)) # y as Gaussian with mean = x_real Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) We may as well fit a model. brm(data = d, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) %&gt;% fixef() %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.00 0.10 -0.19 0.19 ## x_real 0.98 0.15 0.69 1.28 ## x_spur 0.06 0.09 -0.12 0.24 5.2 Masked relationship Let’s load those tasty milk data. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) You might inspect the data like this. d %&gt;% select(kcal.per.g, mass, neocortex.perc) %&gt;% pairs(col = &quot;firebrick4&quot;) By just looking at that mess, do you think you could describe the associations of mass and neocortex.perc with the criterion, kcal.per.g? I couldn’t. It’s a good thing we have math. McElreath has us start of with a simple univaraible milk model. b5.5 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + neocortex.perc, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) The uniform prior was difficult on Stan. After playing around a bit, I just switched to a unit-scale half Cauchy. Similar to the rethinking example in the text, brms warned that “Rows containing NAs were excluded from the model.” This isn’t necessarily a problem; the model fit just fine. But we should be ashamed of ourselves and look eagerly forward to Chapter 14 where we’ll learn how to do better. To compliment how McElreath removed cases with missing values on our variables of interest with Base R complete.cases(), here we’ll do so with tidyr::drop_na() and a little help with ends_with(). dcc &lt;- d %&gt;% drop_na(ends_with(&quot;_s&quot;)) But anyway, let’s inspect the parameter summary. print(b5.5, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc ## Data: d (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.343 0.568 -0.752 1.462 5552 1.001 ## neocortex.perc 0.005 0.008 -0.012 0.021 5549 1.001 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.192 0.040 0.132 0.285 2969 1.001 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Did you notice now we set digits = 3 within print() much the way McElreath set digits=3 within precis()? To get the brms answer to what McElreath did with coef(), we’ll use the fixef() function. fixef(b5.5)[2] * (76 - 55) ## [1] 0.09807436 Yes, indeed, “that’s less than 0.1 kilocalories” (p. 137). Just for kicks, we’ll superimpose 50% intervals atop 95% intervals for the next few plots. Here’s Figure 5.7, top left. nd &lt;- tibble(neocortex.perc = 54:80) fitted(b5.5, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) Do note the probs argument in the fitted() code, above. Let’s make the log_mass variable. dcc &lt;- dcc %&gt;% mutate(log_mass = log(mass)) Now we use log_mass as the new sole predictor. b5.6 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, control = list(adapt_delta = 0.9), seed = 5) print(b5.6, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.705 0.060 0.586 0.821 5121 1.000 ## log_mass -0.032 0.024 -0.080 0.018 4577 1.000 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.183 0.037 0.128 0.270 3816 1.001 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, top right. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30)) fitted(b5.6, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) Finally, we’re ready to fit with both predictors included in the “joint model.” Here’s the statistical formula \\[\\begin{align*} \\text{kcal.per.g}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{neocortex.perc}_i + \\beta_2 \\text{log}(\\text{mass}_i) \\\\ \\alpha &amp; \\sim \\text{Normal}(0, 100) \\\\ \\beta_1 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\text{Uniform}(0, 1) \\end{align*}\\] Note, the HMC chains required a longer warmup period and a higher adapt_delta setting for the model to converge properly. Life will be much better once we ditch the uniform prior for good. b5.7 &lt;- brm(data = dcc, family = gaussian, kcal.per.g ~ 1 + neocortex.perc + log_mass, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 1), class = sigma)), iter = 4000, warmup = 2000, chains = 4, cores = 4, control = list(adapt_delta = 0.999), seed = 5) print(b5.7, digits = 3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + neocortex.perc + log_mass ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 4000; warmup = 2000; thin = 1; ## total post-warmup samples = 8000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept -1.091 0.573 -2.229 0.048 3138 1.001 ## neocortex.perc 0.028 0.009 0.010 0.046 3048 1.001 ## log_mass -0.097 0.027 -0.150 -0.042 3074 1.003 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.139 0.029 0.095 0.210 3583 1.002 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.7, bottom left. nd &lt;- tibble(neocortex.perc = 54:80 %&gt;% as.double(), log_mass = mean(dcc$log_mass)) b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) And make Figure 5.7, bottom right. nd &lt;- tibble(log_mass = seq(from = -2.5, to = 5, length.out = 30), neocortex.perc = mean(dcc$neocortex.perc)) b5.7 %&gt;% fitted(newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$log_mass), ylim = range(dcc$kcal.per.g)) + ylab(&quot;kcal.per.g&quot;) + theme_bw() + theme(panel.grid = element_blank()) What [this regression model did was] ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model [asked] if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we statistically account for both. (pp. 140–141, emphasis in the original) 5.2.0.1 Overthinking: Simulating a masking relationship. n &lt;- 100 # number of cases rho &lt;- .7 # correlation between x_pos and x_neg set.seed(5) # setting the seed makes the results reproducible d &lt;- tibble(x_pos = rnorm(n), # x_pos as a standard Gaussian x_neg = rnorm(n, rho * x_pos, sqrt(1 - rho^2)), # x_neg correlated with x_pos y = rnorm(n, x_pos - x_neg)) # y equally associated with x_pos and x_neg Here are the quick pairs() plots. pairs(d, col = &quot;firebrick4&quot;) Here we fit the models with a little help from the update() function. b5.O.both &lt;- brm(data = d, family = gaussian, y ~ 1 + x_pos + x_neg, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 1), class = b), prior(cauchy(0, 1), class = sigma)), seed = 5) b5.O.pos &lt;- update(b5.O.both, formula = y ~ 1 + x_pos) b5.O.neg &lt;- update(b5.O.both, formula = y ~ 1 + x_neg) Compare the coefficients. fixef(b5.O.pos) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.01 0.12 -0.25 0.23 ## x_pos 0.26 0.13 0.01 0.51 fixef(b5.O.neg) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.01 0.12 -0.22 0.25 ## x_neg -0.29 0.11 -0.50 -0.06 fixef(b5.O.both) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.00 0.10 -0.19 0.18 ## x_pos 0.97 0.15 0.69 1.25 ## x_neg -0.90 0.13 -1.16 -0.64 5.3 Multicollinearity Multicollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will say that a very large range of parameter values are plausible, from tiny associations to massive ones, even if all of the variables are in reality strongly associated with the outcome. This frustrating phenomenon arises from the details of how statistical control works. So once you understand multicollinearity, you will better understand [multivariable] models in general. (pp. 141–142) 5.3.1 Multicollinear legs. Let’s simulate some leg data. n &lt;- 100 set.seed(5) d &lt;- tibble(height = rnorm(n, mean = 10, sd = 2), leg_prop = runif(n, min = 0.4, max = 0.5)) %&gt;% mutate(leg_left = leg_prop * height + rnorm(n, mean = 0, sd = 0.02), leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02)) leg_left and leg_right are highly correlated. d %&gt;% select(leg_left:leg_right) %&gt;% cor() %&gt;% round(digits = 4) ## leg_left leg_right ## leg_left 1.0000 0.9996 ## leg_right 0.9996 1.0000 Have you ever even seen a \\(\\rho = .9996\\) correlation, before? Here it is in a plot. d %&gt;% ggplot(aes(x = leg_left, y = leg_right)) + geom_point(alpha = 1/2, color = &quot;firebrick4&quot;) + theme_bw() + theme(panel.grid = element_blank()) Here’s our attempt to predict height with both legs. b5.8 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left + leg_right, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) Let’s inspect the damage. print(b5.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left + leg_right ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.79 0.29 1.21 2.36 5531 1.00 ## leg_left 0.68 2.10 -3.37 4.81 1757 1.00 ## leg_right 1.17 2.10 -2.98 5.22 1762 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.61 0.04 0.53 0.70 2934 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That ‘Est.Error’ column isn’t looking too good. But it’s easy to miss that, which is why McElreath suggested “a graphical view of the [output] is more useful because it displays the posterior [estimates] and [intervals] in a way that allows us with a glance to see that something has gone wrong here” (p. 143). Here’s our coefficient plot using brms::stanplot() with a little help from bayesplot::color_scheme_set(). color_scheme_set(&quot;red&quot;) stanplot(b5.8, type = &quot;intervals&quot;, prob = .5, prob_outer = .95, point_est = &quot;median&quot;) + labs(title = &quot;The coefficient plot for the two-leg model&quot;, subtitle = &quot;Holy smokes; look at the widths of those betas!&quot;) + theme_bw() + theme(text = element_text(size = 14), panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Now you can use the brms::stanplot() function without explicitly loading the bayesplot package. But loading bayesplot allows you to set the color scheme with color_scheme_set(). This is perhaps the simplest way to plot the bivariate posterior of our two predictor coefficients, Figure 6.2.a. pairs(b5.8, pars = parnames(b5.8)[2:3]) If you’d like a nicer and more focused attempt, you might have to revert to the posterior_samples() function and a little ggplot2 code. post &lt;- posterior_samples(b5.8) post %&gt;% ggplot(aes(x = b_leg_left, y = b_leg_right)) + geom_point(color = &quot;firebrick&quot;, alpha = 1/10, size = 1/3) + theme_bw() + theme(panel.grid = element_blank()) While we’re at it, you can make a similar plot with the mcmc_scatter() function. post %&gt;% mcmc_scatter(pars = c(&quot;b_leg_left&quot;, &quot;b_leg_right&quot;), size = 1/3, alpha = 1/10) + theme_bw() + theme(panel.grid = element_blank()) But wow, those coefficients look about as highly correlated as the predictors, just with the reversed sign. post %&gt;% select(b_leg_left:b_leg_right) %&gt;% cor() ## b_leg_left b_leg_right ## b_leg_left 1.0000000 -0.9995444 ## b_leg_right -0.9995444 1.0000000 On page 165, McElreath clarified that “from the computer’s perspective, this model is simply:” \\[\\begin{align*} y_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + (\\beta_1 + \\beta_2) x_i \\end{align*}\\] Accordingly, here’s the posterior of the sum of the two regression coefficients, Figure 6.2.b. We’ll use tidybayes::geom_halfeyeh() to both plot the density and mark off the posterior median and percentile-based 95% probability intervals at its base. library(tidybayes) post %&gt;% ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) + geom_halfeyeh(fill = &quot;firebrick&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Sum the multicollinear coefficients&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;) + theme_bw() + theme(panel.grid = element_blank()) Now we fit the model after ditching one of the leg lengths. b5.9 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 1.79 0.29 1.22 2.36 5928 1.00 ## leg_left 1.84 0.06 1.72 1.96 6270 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.60 0.04 0.53 0.69 6693 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That posterior \\(SD\\) looks much better. Compare this density to the one in Figure 6.1.b. posterior_samples(b5.9) %&gt;% ggplot(aes(x = b_leg_left, y = 0)) + geom_halfeyeh(fill = &quot;firebrick&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Just one coefficient needed&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;, x = &quot;only b_leg_left, this time&quot;) + theme_bw() + theme(panel.grid = element_blank()) When two predictor variables are very strongly correlated, including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such a case. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. (p. 145, emphasis in the original) 5.3.2 Multicollinear milk. Multicollinearity arises in real data, too. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = TRUE) library(brms) We’ll follow the text and fit the two univariable models, first. Note our use of the update() function. # `kcal.per.g` regressed on `perc.fat` b5.10 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + perc.fat, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) # `kcal.per.g` regressed on `perc.lactose` b5.11 &lt;- update(b5.10, newdata = d, formula = kcal.per.g ~ 1 + perc.lactose) Compare the coefficients. posterior_summary(b5.10) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.301 0.039 0.225 0.375 ## b_perc.fat 0.010 0.001 0.008 0.012 ## sigma 0.079 0.011 0.061 0.106 ## lp__ 24.056 1.268 20.703 25.479 posterior_summary(b5.11) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.166 0.047 1.074 1.258 ## b_perc.lactose -0.011 0.001 -0.012 -0.009 ## sigma 0.067 0.010 0.051 0.089 ## lp__ 28.791 1.284 25.403 30.282 If you’d like to get just the 95% intervals similar to the way McElreath reported them in the prose on page 146, you might use the handy posterior_interval() function. posterior_interval(b5.10)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## 0.008 0.012 posterior_interval(b5.11)[2, ] %&gt;% round(digits = 3) ## 2.5% 97.5% ## -0.012 -0.009 Now “watch what happens when we place both predictor varaibles in the same regression model” (p. 146). b5.12 &lt;- update(b5.11, newdata = d, formula = kcal.per.g ~ 1 + perc.fat + perc.lactose) posterior_summary(b5.12) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 1.007 0.225 0.570 1.443 ## b_perc.fat 0.002 0.003 -0.003 0.007 ## b_perc.lactose -0.009 0.003 -0.014 -0.003 ## sigma 0.068 0.010 0.052 0.091 ## lp__ 27.656 1.477 23.973 29.524 You can make custom pairs plots with GGalley, which will also compute the point estimates for the bivariate correlations. Here’s a default plot. #install.packages(&quot;GGally&quot;, dependencies = T) library(GGally) ggpairs(data = d, columns = c(3:4, 6)) + theme(panel.grid = element_blank()) But you can customize these, too. E.g., my_diag &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;firebrick4&quot;, size = 0) } my_lower &lt;- function(data, mapping, ...){ ggplot(data = data, mapping = mapping) + geom_smooth(method = &quot;lm&quot;, color = &quot;firebrick4&quot;, size = 1/3, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(color = &quot;firebrick&quot;, alpha = .8, size = 1/4) } # then plug those custom functions into `ggpairs()` ggpairs(data = d, columns = c(3:4, 6), diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + theme_bw() + theme(strip.background = element_rect(fill = &quot;white&quot;, color = &quot;white&quot;), axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank()) Our two predictor “variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g, but neither helps much once you already know the other” (p. 148, emphasis in the original). You can really see that on the lower two scatter plots. You’ll note the ggpairs() plot also showed the Pearson’s correlation coefficients, se we don’t need to use the cor() function like McElreath did in the text. In the next section, we’ll run the simulation necessary for our version of Figure 5.10. 5.3.2.1 Overthinking: Simulating collinearity. First we’ll get the data and define the functions. You’ll note I’ve defined my sim_coll() a little differently from sim.coll() in the text. I’ve omitted rep.sim.coll() as an independent function altogether, but computed similar summary information with the summarise() code at the bottom of the block. sim_coll &lt;- function(seed, rho){ set.seed(seed) d &lt;- d %&gt;% mutate(x = rnorm(n(), mean = perc.fat * rho, sd = sqrt((1 - rho^2) * var(perc.fat)))) m &lt;- lm(kcal.per.g ~ perc.fat + x, data = d) sqrt(diag(vcov(m)))[2] # parameter SD } # how many simulations per `rho`-value would you like? n_seed &lt;- 100 # how many `rho`-values from 0 to .99 would you like to evaluate the process over? n_rho &lt;- 30 d &lt;- tibble(seed = 1:n_seed) %&gt;% expand(seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2_dbl(seed, rho, sim_coll)) %&gt;% group_by(rho) %&gt;% # we&#39;ll `summarise()` our output by the mean and 95% intervals summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) We’ve added 95% interval bands to our version of Figure 5.10. d %&gt;% ggplot(aes(x = rho, y = mean)) + geom_smooth(aes(ymin = ll, ymax = ul), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + labs(x = expression(rho), y = &quot;parameter SD&quot;) + coord_cartesian(ylim = c(0, .0072)) + theme_bw() + theme(panel.grid = element_blank()) Did you notice we used the base R lm() function to fit the models? As McElreath rightly pointed out, lm() presumes flat priors. Proper Bayesian modeling could improve on that. But then we’d have to wait for a whole lot of HMC chains to run and until our personal computers or the algorithms we use to fit our Bayesian models become orders of magnitude faster, we just don’t have time for that. 5.3.3 Post-treatment bias. It helped me understand the next example by mapping out the sequence of events McElreath described in the second paragraph: seed and sprout plants measure heights apply different antifungal soil treatments (i.e., the experimental manipulation) measure (a) the heights and (b) the presence of fungus Based on the design, let’s simulate our data. n &lt;- 100 set.seed(5) d &lt;- tibble(h0 = rnorm(n, mean = 10, sd = 2), treatment = rep(0:1, each = n / 2), fungus = rbinom(n, size = 1, prob = .5 - treatment * 0.4), h1 = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1)) We’ll use head() to peek at the data. d %&gt;% head() ## # A tibble: 6 x 4 ## h0 treatment fungus h1 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 8.32 0 0 14.3 ## 2 12.8 0 0 18.5 ## 3 7.49 0 1 8.97 ## 4 10.1 0 1 12.9 ## 5 13.4 0 1 14.8 ## 6 8.79 0 1 12.0 These data + the model were rough on Stan, at first, which spat out warnings about divergent transitions. The model ran fine after setting warmup = 1000 and adapt_delta = 0.99. b5.13 &lt;- brm(data = d, family = gaussian, h1 ~ 1 + h0 + treatment + fungus, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.99), seed = 5) print(b5.13) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ 1 + h0 + treatment + fungus ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 5.59 0.57 4.50 6.68 2685 1.00 ## h0 0.94 0.05 0.83 1.04 3336 1.00 ## treatment 0.04 0.22 -0.40 0.48 689 1.01 ## fungus -2.77 0.26 -3.29 -2.26 1790 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.01 0.08 0.88 1.17 3146 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now fit the model after excluding fungus, our post-treatment variable. b5.14 &lt;- update(b5.13, formula = h1 ~ 1 + h0 + treatment) print(b5.14) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ h0 + treatment ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 5.49 0.84 3.83 7.11 1637 1.00 ## h0 0.82 0.08 0.67 0.98 1659 1.00 ## treatment 1.04 0.31 0.44 1.69 1080 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 1.53 0.11 1.33 1.76 3090 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). “Now the impact of treatment is strong and positive, as it should be” (p. 152). In this case, there were really two outcomes. The first was the one we modeled, the height at the end of the experiment (i.e., h1). The second outcome, which was clearly related to h1, was the presence of fungus, captured by our binomial variable fungus. If you wanted to model that, you’d fit a logistic regression model, which we’ll learn about in Chapter 10. 5.4 Categorical varaibles Many readers will already know that variables like this, routinely called factors, can easily be included in linear models. But what is not widely understood is how these variables are included in a model… Knowing how the machine works removes a lot of this difficulty. (p. 153, emphasis in the original) 5.4.1 Binary categories. Reload the Howell1 data. library(rethinking) data(Howell1) d &lt;- Howell1 Unload rethinking and load brms. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Just in case you forgot what these data were like: d %&gt;% glimpse() ## Observations: 544 ## Variables: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 14… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0… Let’s fit the first height model with the male dummy. Note. The uniform prior McElreath used in the text in conjunction with the brms::brm() function seemed to cause problems for the HMC chains, here. After experimenting with start values, increasing warmup, and increasing adapt_delta, switching out the uniform prior did the trick. Anticipating Chapter 8, I recommend you use a weakly-regularizing half Cauchy for \\(\\sigma\\). b5.15 &lt;- brm(data = d, family = gaussian, height ~ 1 + male, prior = c(prior(normal(178, 100), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) Check the summary. print(b5.15) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + male ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 134.83 1.61 131.65 137.94 5668 1.00 ## male 7.32 2.25 2.98 11.76 5642 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 27.38 0.85 25.80 29.12 5179 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our samples from the posterior are already in the HMC iterations. All we need to do is put them in a data frame and then put them to work. post &lt;- posterior_samples(b5.15) post %&gt;% transmute(male_height = b_Intercept + b_male) %&gt;% mean_qi(.width = .89) ## male_height .lower .upper .width .point .interval ## 1 142.1473 139.5225 144.7803 0.89 mean qi You can also do this with fitted(). nd &lt;- tibble(male = 1) fitted(b5.15, newdata = nd) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 142.1473 1.653697 138.9162 145.3928 And you could even plot. fitted(b5.15, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = V1, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied male heights&quot;, x = expression(alpha + beta[&quot;male&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.1.1 Overthinking: Re-parameterizing the model. The reparameterized model follows the form \\[\\begin{align*} \\text{height}_i &amp; \\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_\\text{female} (1 - \\text{male}_i) + \\alpha_\\text{male} \\text{male}_i \\end{align*}\\] So then a female dummy would satisfy the condition \\(\\text{female}_i = (1 - \\text{male}_i)\\). Let’s make that dummy. d &lt;- d %&gt;% mutate(female = 1 - male) Everyone has their own idiosyncratic way of coding. One of my quirks is I always explicitly specify a model’s intercept following the form y ~ 1 + x, where y is the criterion, x stands for the predictors, and 1 is the intercept. You don’t have to do this, of course. You could just code y ~ x to get the same results. The brm() function assumes you want that intercept. One of the reasons I like the verbose version is it reminds me to think about the intercept and to include it in my priors. Another nice feature is that is helps me make sense of the code for this model: height ~ 0 + male + female. When we replace … ~ 1 + … with … ~ 0 + …, we tell brm() to remove the intercept. Removing the intercept allows us to include ALL levels of a given categorical variable in our model. In this case, we’ve expressed sex as two dummies, female and male. Taking out the intercept lets us put both dummies into the formula. b5.15b &lt;- brm(data = d, family = gaussian, height ~ 0 + male + female, prior = c(prior(normal(178, 100), class = b), prior(cauchy(0, 2), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.15b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 0 + male + female ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## male 142.35 1.71 139.10 145.69 5738 1.00 ## female 134.66 1.62 131.47 137.86 6261 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 27.37 0.81 25.80 29.02 6113 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If we wanted the formal difference score from such a model, we’d subtract. posterior_samples(b5.15b) %&gt;% transmute(dif = b_male - b_female) %&gt;% ggplot(aes(x = dif, y = 0)) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Model-implied difference score&quot;, x = expression(alpha[&quot;male&quot;] - alpha[&quot;female&quot;])) + theme_bw() + theme(panel.grid = element_blank()) 5.4.2 Many categories. When there are more than two categories, you’ll need more than one dummy variable. Here’s the general rule: To include \\(k\\) categories in a linear model, you require \\(k - 1\\) dummy variables. Each dummy variable indicates, with the value 1, a unique category. The category with no dummy variable assigned to it ends up again as the “intercept” category. (p. 155) We’ll practice with milk. library(rethinking) data(milk) d &lt;- milk Unload rethinking and load brms. rm(milk) detach(package:rethinking, unload = T) library(brms) With the tidyverse, we can peek at clade with distinct() in the place of base R unique(). d %&gt;% distinct(clade) ## clade ## 1 Strepsirrhine ## 2 New World Monkey ## 3 Old World Monkey ## 4 Ape As clade has 4 categories, let’s use ifelse() to convert these to 4 dummy variables. d &lt;- d %&gt;% mutate(clade_nwm = ifelse(clade == &quot;New World Monkey&quot;, 1, 0), clade_owm = ifelse(clade == &quot;Old World Monkey&quot;, 1, 0), clade_s = ifelse(clade == &quot;Strepsirrhine&quot;, 1, 0), clade_ape = ifelse(clade == &quot;Ape&quot;, 1, 0)) Now we’ll fit the model with three of the four dummies. In this model, clade_ape is the reference category captured by the intercept. b5.16 &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s, prior = c(prior(normal(.6, 10), class = Intercept), prior(normal(0, 1), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.16) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.55 0.04 0.47 0.63 4726 1.00 ## clade_nwm 0.17 0.06 0.04 0.29 5016 1.00 ## clade_owm 0.24 0.07 0.11 0.37 5343 1.00 ## clade_s -0.04 0.07 -0.18 0.10 5072 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.13 0.02 0.10 0.17 5006 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we grab the chains, our draws from the posterior. post &lt;- b5.16 %&gt;% posterior_samples() head(post) ## b_Intercept b_clade_nwm b_clade_owm b_clade_s sigma lp__ ## 1 0.5487975 0.1392064 0.1810537 0.02934529 0.1216194 9.441813 ## 2 0.5896205 0.1061920 0.1626172 -0.09996722 0.1198356 10.165838 ## 3 0.4816118 0.2560447 0.3437558 0.07247933 0.1280294 9.086081 ## 4 0.5135556 0.1592885 0.2999321 -0.08033590 0.1449220 8.694863 ## 5 0.5629218 0.1486115 0.2150750 0.07140982 0.1343563 8.399936 ## 6 0.5407192 0.1986738 0.2156752 0.08817498 0.1188004 8.190390 You might compute averages for each category and summarizing the results with the transpose of base R’s apply() function, rounding to two digits of precision. post$mu_ape &lt;- post$b_Intercept post$mu_nwm &lt;- post$b_Intercept + post$b_clade_nwm post$mu_owm &lt;- post$b_Intercept + post$b_clade_owm post$mu_s &lt;- post$b_Intercept + post$b_clade_s round(t(apply(post[ ,7:10], 2, quantile, c(.5, .025, .975))), digits = 2) ## 50% 2.5% 97.5% ## mu_ape 0.55 0.47 0.63 ## mu_nwm 0.71 0.63 0.80 ## mu_owm 0.79 0.68 0.89 ## mu_s 0.51 0.39 0.62 Here’s a more tidyverse sort of way to get the same thing, but this time with means and HPDIs via the tidybayes::mean_hdi() function. post %&gt;% transmute(mu_ape = b_Intercept, mu_nwm = b_Intercept + b_clade_nwm, mu_owm = b_Intercept + b_clade_owm, mu_s = b_Intercept + b_clade_s) %&gt;% gather() %&gt;% group_by(key) %&gt;% mean_hdi() %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu_ape 0.55 0.46 0.63 0.95 mean hdi ## 2 mu_nwm 0.71 0.62 0.8 0.95 mean hdi ## 3 mu_owm 0.79 0.68 0.89 0.95 mean hdi ## 4 mu_s 0.51 0.39 0.62 0.95 mean hdi You could also summarize with fitted(). nd &lt;- tibble(clade_nwm = c(1, 0, 0, 0), clade_owm = c(0, 1, 0, 0), clade_s = c(0, 0, 1, 0), primate = c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;)) fitted(b5.16, newdata = nd, summary = F) %&gt;% as_tibble() %&gt;% gather() %&gt;% mutate(primate = rep(c(&quot;New World Monkey&quot;, &quot;Old World Monkey&quot;, &quot;Strepsirrhine&quot;, &quot;Ape&quot;), each = n() / 4)) %&gt;% ggplot(aes(x = value, y = reorder(primate, value))) + geom_halfeyeh(fill = &quot;firebrick4&quot;, point_interval = median_qi, .width = .95) + labs(x = &quot;kcal.per.g&quot;, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) And there are multiple ways to compute summary statistics for the difference between NWM and OWM, too. # base R quantile(post$mu_nwm - post$mu_owm, probs = c(.5, .025, .975)) ## 50% 2.5% 97.5% ## -0.07276730 -0.20668455 0.07040724 # tidyverse + tidybayes post %&gt;% transmute(dif = mu_nwm - mu_owm) %&gt;% median_qi() ## dif .lower .upper .width .point .interval ## 1 -0.0727673 -0.2066846 0.07040724 0.95 median qi 5.4.3 Adding regular predictor variables. If we wanted to fit the model including perc.fat as an additional predictor, the basic statistical formula would be \\[ \\mu_i = \\alpha + \\beta_\\text{clade_nwm} \\text{clade_nwm}_i + \\beta_\\text{clade_owm} \\text{clade_owm}_i + \\beta_\\text{clade_s} \\text{clade_s}_i + \\beta_\\text{perc.fat} \\text{perc.fat}_i \\] The corresponding formula argument within brm() would be kcal.per.g ~ 1 + clade_nwm + clade_owm + clade_s + perc.fat. 5.4.4 Another approach: Unique intercepts. Using the code below, there’s no need to transform d$clade into d$clade_id. The advantage of this approach is the indices in the model summary are more descriptive than a[1] through a[4]. b5.16_alt &lt;- brm(data = d, family = gaussian, kcal.per.g ~ 0 + clade, prior = c(prior(normal(.6, 10), class = b), prior(uniform(0, 10), class = sigma)), iter = 2000, warmup = 500, chains = 4, cores = 4, seed = 5) print(b5.16_alt) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g ~ 0 + clade ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 500; thin = 1; ## total post-warmup samples = 6000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## cladeApe 0.55 0.04 0.46 0.63 7204 1.00 ## cladeNewWorldMonkey 0.71 0.04 0.63 0.80 6634 1.00 ## cladeOldWorldMonkey 0.79 0.05 0.68 0.89 7429 1.00 ## cladeStrepsirrhine 0.51 0.06 0.39 0.62 7142 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 0.13 0.02 0.10 0.18 4651 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). See? This is much easier than trying to remember which one was which in an arbitrary numeric index. 5.5 Ordinary least squares and lm() Since this section centers on the frequentist lm() function, I’m going to largely ignore it. A couple things, though. You’ll note how the brms package uses the lm()-like design formula syntax. Although not as pedagogical as the more formal rethinking syntax, it has the advantage of cohering with the popular lme4 syntax for multilevel models. Also, on page 161 McElreath clarified that one cannot use the I() syntax with his rethinking package. Not so with brms. The I() syntax works just fine with brms::brm(). We’ve already made use of it in the “Polynomial regression” section of Chapter 4. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] grid parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] rethinking_1.59 GGally_1.4.0 bayesplot_1.7.0 tidybayes_1.1.0 gridExtra_2.3 ## [6] fiftystater_1.0.1 ggrepel_0.8.1 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [11] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 tidyverse_1.2.1 ## [16] brms_2.9.0 Rcpp_1.0.1 rstan_2.18.2 StanHeaders_2.18.1 ggplot2_3.1.1 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.1.4 plyr_1.8.4 ## [4] igraph_1.2.4.1 lazyeval_0.2.2 svUnit_0.7-12 ## [7] crosstalk_1.0.0 rstantools_1.5.1 inline_0.3.15 ## [10] digest_0.6.19 htmltools_0.3.6 rsconnect_0.8.13 ## [13] fansi_0.4.0 magrittr_1.5 modelr_0.1.4 ## [16] matrixStats_0.54.0 xts_0.11-2 prettyunits_1.0.2 ## [19] colorspace_1.4-1 rvest_0.3.4 haven_2.1.0 ## [22] xfun_0.7 callr_3.2.0 crayon_1.3.4 ## [25] jsonlite_1.6 zeallot_0.1.0 zoo_1.8-6 ## [28] glue_1.3.1 gtable_0.3.0 pkgbuild_1.0.3 ## [31] maps_3.3.0 abind_1.4-5 scales_1.0.0 ## [34] mvtnorm_1.0-10 miniUI_0.1.1.1 viridisLite_0.3.0 ## [37] xtable_1.8-4 HDInterval_0.2.0 ggstance_0.3.1 ## [40] mapproj_1.2.6 stats4_3.6.0 DT_0.7 ## [43] htmlwidgets_1.3 httr_1.4.0 threejs_0.3.1 ## [46] RColorBrewer_1.1-2 arrayhelpers_1.0-20160527 reshape_0.8.8 ## [49] pkgconfig_2.0.2 loo_2.1.0 utf8_1.1.4 ## [52] tidyselect_0.2.5 labeling_0.3 rlang_0.3.4 ## [55] reshape2_1.4.3 later_0.8.0 munsell_0.5.0 ## [58] cellranger_1.1.0 tools_3.6.0 cli_1.1.0 ## [61] generics_0.0.2 pacman_0.5.1 broom_0.5.2 ## [64] ggridges_0.5.1 evaluate_0.14 yaml_2.2.0 ## [67] processx_3.3.1 knitr_1.23 nlme_3.1-140 ## [70] mime_0.7 xml2_1.2.0 compiler_3.6.0 ## [73] shinythemes_1.1.2 rstudioapi_0.10 stringi_1.4.3 ## [76] highr_0.8 ps_1.3.0 Brobdingnag_1.2-6 ## [79] lattice_0.20-38 Matrix_1.2-17 markdown_1.0 ## [82] shinyjs_1.0 vctrs_0.1.0 pillar_1.4.1 ## [85] bridgesampling_0.6-0 httpuv_1.5.1 R6_2.4.0 ## [88] bookdown_0.11 promises_1.0.1 codetools_0.2-16 ## [91] colourpicker_1.0 MASS_7.3-51.4 gtools_3.8.1 ## [94] assertthat_0.2.1 rprojroot_1.3-2 withr_2.1.2 ## [97] shinystan_2.5.0 hms_0.4.2 coda_0.19-2 ## [100] rmarkdown_1.13 shiny_1.3.2 lubridate_1.7.4 ## [103] base64enc_0.1-3 dygraphs_1.1.1.6 "]
]
