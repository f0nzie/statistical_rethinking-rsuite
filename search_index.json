[
["index.html", "Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.0.1 This is a love letter Why this? My assumptions about you How to use and understand this project You can do this, too We have updates", " Statistical Rethinking with brms, ggplot2, and the tidyverse version 1.0.1 A Solomon Kurz 2019-06-19 This is a love letter I love McElreath’s Statistical Rethinking text. It’s the entry-level textbook for applied researchers I spent years looking for. McElreath’s freely-available lectures on the book are really great, too. However, I prefer using Bürkner’s brms package when doing Bayesian regression in R. It’s just spectacular. I also prefer plotting with Wickham’s ggplot2, and coding with functions and principles from the tidyverse, which you might learn about here or here. So, this project is an attempt to reexpress the code in McElreath’s textbook. His models are re-fit with brms, the figures are reproduced or reimagined with ggplot2, and the general data wrangling code now predominantly follows the tidyverse style. Why this? I’m not a statistician and I have no formal background in computer science. Though I benefited from a suite of statistics courses in grad school, a large portion of my training has been outside of the classroom, working with messy real-world data, and searching online for help. One of the great resources I happened on was idre, the UCLA Institute for Digital Education, which offers an online portfolio of richly annotated textbook examples. Their online tutorials are among the earliest inspirations for this project. We need more resources like them. With that in mind, one of the strengths of McElreath’s text is its thorough integration with the rethinking package. The rethinking package is a part of the R ecosystem, which is great because R is free and open source. And McElreath has made the source code for rethinking publically available, too. Since he completed his text, many other packages have been developed to help users of the R ecosystem interface with Stan. Of those alternative packages, I think Bürkner’s brms is the best for general-purpose Bayesian data analysis. It’s flexible, uses reasonably-approachable syntax, has sensible defaults, and offers a vast array of post-processing convenience functions. And brms has only gotten better over time. To my knowledge, there are no textbooks on the market that highlight the brms package, which seems like an evil worth correcting. In addition, McElreath’s data wrangling code is based in the base R style and he made most of his figures with base R plots. Though there are benefits to sticking close to base R functions (e.g., less dependencies leading to a lower likelihood that your code will break in the future), there are downsides. For beginners, base R functions can be difficult both to learn and to read. Happily, in recent years Hadley Wickham and others have been developing a group of packages collectively called the tidyverse. These tidyverse packages (e.g., dplyr, tidyr, purrr) were developed according to an underlying philosophy and they are designed to work together coherently and seamlessly. Though not all within the R community share this opinion, I am among those who think the tydyverse style of coding is generally easier to learn and sufficiently powerful that these packages can accommodate the bulk of your data needs. I also find tydyverse-style syntax easier to read. And of course, the widely-used ggplot2 package is part of the tidyverse, too. To be clear, students can get a great education in both Bayesian statistics and programming in R with McElreath’s text just the way it is. Just go slow, work through all the examples, and read the text closely. It’s a pedagogical boon. I could not have done better or even closely so. But what I can offer is a parallel introduction on how to fit the statistical models with the ever-improving and already-quite-impressive brms package. I can throw in examples of how to perform other operations according to the ethic of the tidyverse. And I can also offer glimpses of some of the other great packages in the R + Stan ecosystem, such as loo, bayesplot, and tidybayes. My assumptions about you If you’re looking at this project, I’m guessing you’re either a graduate student, a post-graduate academic, or a researcher of some sort. So I’m presuming you have at least a 101-level foundation in statistics. If you’re rusty, consider checking out Legler and Roback’s free bookdown text, Broadening Your Statistical Horizons before diving into Statistical Rethinking. I’m also assuming you understand the rudiments of R and have at least a vague idea about what the tidyverse is. If you’re totally new to R, consider starting with Peng’s R Programming for Data Science. And the best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s R for Data Science, which I extensively link to throughout this project. That said, you do not need to be totally fluent in statistics or R. Otherwise why would you need this project, anyway? IMO, the most important things are curiosity, a willingness to try, and persistent tinkering. I love this stuff. Hopefully you will, too. How to use and understand this project This project is not meant to stand alone. It’s a supplement to McElreath’s Statistical Rethinking text. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, some of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project will sometimes be blank or omitted, though I do highlight some of the important points in quotes and prose of my own. So I imagine students might reference this project as they progress through McElreath’s text. I also imagine working data analysts might use this project in conjunction with the text as they flip to the specific sections that seem relevant to solving their data challenges. I reproduce the bulk of the figures in the text, too. The plots in the first few chapters are the closest to those in the text. However, I’m passionate about data visualization and like to play around with color palettes, formatting templates, and other conventions quite a bit. As a result, the plots in each chapter have their own look and feel. For more on some of these topics, check out chapters 3, 7, and 28 in R4DS, Healy’s Data Visualization: A practical introduction, or Wilke’s Fundamentals of Data Visualization. In this project, I use a handful of formatting conventions gleaned from R4DS, The tidyverse style guide, and R Markdown: The Definitive Guide. R code blocks and their output appear in a gray background. E.g., 2 + 2 == 5 ## [1] FALSE Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit the package a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidybayes::mode_hdi()). R objects, such as data or function arguments, are in typewriter font atop gray backgrounds (e.g., chimpanzees, .width = .5). You can detect hyperlinks by their typical blue-colored font. In the text, McElreath indexed his models with names like m4.1 (i.e., the first model of Chapter 4). I primarily followed that convention, but replaced the m with a b to stand for the brms package. You can do this, too This project is powered by Yihui Xie’s bookdown package, which makes it easy to turn R markdown files into HTML, PDF, and EPUB. Go here to learn more about bookdown. While you’re at it, also check out Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide. And if you’re unacquainted with GitHub, check out Jenny Bryan’s Happy Git and GitHub for the useR. I’ve even blogged about what it was like putting together the first version of this project. The source code of the project is available here. We have updates I released the initial 0.9.0 version of this project in September 26, 2018. In April 19, 2019 came the 1.0.0 version. Some of the major changes were: All models were refit with the current official version of brms, 2.8.0. Adopting the seed argument within the brm() function made the model results more reproducible. The loo package was updated. As a consequence, our workflow for the WAIC and LOO changed, too. I improved the brms alternative to McElreath’s coeftab() function. I made better use of the tidyverse, especially some of the purrr functions. Particularly in the later chapters, there’s a greater emphasis on functions from the tidybayes package. Chapter 11 contains the updated brms 2.8.0 workflow for making custom distributions, using the beta-binomial model as the example. Chapter 12 received a new bonus section contrasting different methods for working with multilevel posteriors. Chapter 14 received a new bonus section introducing Bayesian meta-analysis and linking it to multilevel and measurement-error models. With the help of others within the community, I corrected many typos and streamlined some of the code (e.g., dropped an unnecessary use of the mi() function in section 14.2.1) And in some cases, I corrected sections that were just plain wrong (e.g., some of my initial attempts in section 3.3 were incorrect). In response to some reader requests, we finally have a PDF version! Making that happen required some formatting adjustments, resulting in version 1.0.1. Noteworthy changes include: Major revisions to the LaTeX syntax underlying many of the in-text equations (e.g., dropping the “eqnarray” environment for &quot;align*&quot;) Adjusting some of the image syntax Updating the reference for the Bayesian \\(R^2\\) (Gelman, Goodrich, Gabry, &amp; Vehtari, 2018) Though we’re into version 1.0.1, there’s room for improvement. There are still two models that need work. The current solution for model 10.6 is wrong, which I try to make clear in the prose. It also appears that the Gaussian process model from section 13.4 is off. Both models are beyond my current skill set and friendly suggestions are welcome. In addition to modeling concerns, typos may yet be looming and I’m sure there are places where the code could be made more streamlined, more elegant, or just more in-line with the tidyverse style. Which is all to say, I hope to release better and more useful updates in the future. Before we move on, I’d like to thank the following for their helpful contributions: Paul-Christian Bürkner (@paul-buerkner), Andrew Collier (@datawookie), Jeff Hammerbacher (@hammer), Matthew Kay (@mjskay), TJ Mahr (@tjmahr), Stijn Masschelein (@stijnmasschelein), Colin Quirk (@colinquirk), Rishi Sadhir (@RishiSadhir), Richard Torkar (@torkar), Aki Vehtari (@avehtari). "],
["the-golem-of-prague.html", "Chapter 1 The Golem of Prague 1 Statistical golems Reference Session info", " Chapter 1 The Golem of Prague Figure 1.1: Rabbi Loew and Golem by Mikoláš Aleš, 1899 As he opened the chapter, McElreath told us that ultimately Judah was forced to destroy the golem, as its combination of extraordinary power with clumsiness eventually led to innocent deaths. Wiping away one letter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned the robot. 1 Statistical golems Scientists also make golems. Our golems rarely have physical form, but they too are often made of clay, living in silicon as computer code. These golems are scientific model. But these golems have real effects on the world, through the predictions they make and the intuitions they challenge or inspire. A concern with truth enlivens these models, but just like a golem or a modern robot, scientific models are neither true nor false, neither prophets nor charlatans. Rather they are constructs engineered for some purpose. These constructs are incredibly powerful, dutifully conducting their programmed calculations. (p. 1, emphasis in the original) There are a lot of great points, themes, methods, and factoids in this text. For me, one of the most powerful themes interlaced throughout the pages is how we should be skeptical of our models. Yes, learn Bayes. Pour over this book. Fit models until late into the night. But please don’t fall into blind love with their elegance and power. If we all knew what we were doing, there’d be no need for science. For more wise deflation along these lines, do check out A personal essay on Bayes factors, Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection and Science, statistics and the problem of “pretty good inference”, a blog, paper and talk by the inimitable Danielle Navarro. Anyway, McElreath left us no code or figures to translate in this chapter. But before you skip off to the next one, why not invest a little time soaking in this chapter’s material by watching McElreath present it? He’s an engaging speaker and the material in his online lectures does not entirely overlap with that in the text. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.1 bookdown_0.11 digest_0.6.19 crayon_1.3.4 ## [5] rprojroot_1.3-2 backports_1.1.4 pacman_0.5.1 magrittr_1.5 ## [9] evaluate_0.14 highr_0.8 stringi_1.4.3 rstudioapi_0.10 ## [13] rmarkdown_1.13 tools_3.6.0 stringr_1.4.0 xfun_0.7 ## [17] yaml_2.2.0 compiler_3.6.0 htmltools_0.3.6 knitr_1.23 "],
["small-worlds-and-large-worlds.html", "Chapter 2 Small Worlds and Large Worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model 2.4 Making the model go Reference Session info", " Chapter 2 Small Worlds and Large Worlds A while back The Oatmeal put together an infographic on Christopher Columbus. I’m no historian and cannot vouch for its accuracy, so make of it what you will. McElreath described the thrust of this chapter this way: In this chapter, you will begin to build Bayesian models. The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (p. 20) Indeed. 2.1 The garden of forking data Gelman and Loken wrote a great paper by this name. 2.1.1 Counting possibilities. Throughout this project, we’ll use the tidyverse for data wrangling. library(tidyverse) If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in section 5.6.1 from Grolemund and Wickham’s R for Data Science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2. Other than the pipe, the other big thing to be aware of is tibbles. For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. And importantly, tibbles are just special types of data frames. So whenever we talk about data frames, we’re also talking about tibbles. For more on the topic, check out R4SD, Chapter 10. So, if we’re willing to code the marbles as 0 = “white” 1 = “blue”, we can arrange the possibility data in a tibble as follows. d &lt;- tibble(p_1 = 0, p_2 = rep(1:0, times = c(1, 3)), p_3 = rep(1:0, times = c(2, 2)), p_4 = rep(1:0, times = c(3, 1)), p_5 = 1) head(d) ## # A tibble: 4 x 5 ## p_1 p_2 p_3 p_4 p_5 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1 1 1 1 ## 2 0 0 1 1 1 ## 3 0 0 0 1 1 ## 4 0 0 0 0 1 You might depict the possibility data in a plot. d %&gt;% gather() %&gt;% mutate(x = rep(1:4, times = 5), possibility = rep(1:5, each = 4)) %&gt;% ggplot(aes(x = x, y = possibility, fill = value %&gt;% as.character())) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(.75, 4.25), ylim = c(.75, 5.25)) + theme(legend.position = &quot;none&quot;) As a quick aside, check out Suzan Baert’s blog post Data Wrangling Part 2: Transforming your columns into the right shape for an extensive discussion on dplyr::mutate() and dplyr::gather(). Here’s the basic structure of the possibilities per marble draw. tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) %&gt;% knitr::kable() draw marbles possibilities 1 4 4 2 4 16 3 4 64 If you walk that out a little, you can structure the data required to approach Figure 2.2. ( d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3)), fill = rep(c(&quot;b&quot;, &quot;w&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2))) ) ## # A tibble: 84 x 3 ## position draw fill ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 b ## 2 2 1 w ## 3 3 1 w ## 4 4 1 w ## 5 0.25 2 b ## 6 0.5 2 w ## 7 0.75 2 w ## 8 1 2 w ## 9 1.25 2 b ## 10 1.5 2 w ## # … with 74 more rows See what I did there with the parentheses? If you assign a value to an object in R (e.g., dog &lt;- 1) and just hit return, nothing will immediately pop up in the console. You have to actually execute dog before R will return 1. But if you wrap the code within parentheses (e.g., (dog &lt;- 1)), R will perform the assignment and return the value as if you had executed dog. But we digress. Here’s the initial plot. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) To my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles. # these will connect the dots from the first and second draws ( lines_1 &lt;- tibble(x = rep((1:4), each = 4), xend = ((1:4^2) / 4), y = 1, yend = 2) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1 2 ## 2 1 0.5 1 2 ## 3 1 0.75 1 2 ## 4 1 1 1 2 ## 5 2 1.25 1 2 ## 6 2 1.5 1 2 ## 7 2 1.75 1 2 ## 8 2 2 1 2 ## 9 3 2.25 1 2 ## 10 3 2.5 1 2 ## 11 3 2.75 1 2 ## 12 3 3 1 2 ## 13 4 3.25 1 2 ## 14 4 3.5 1 2 ## 15 4 3.75 1 2 ## 16 4 4 1 2 # these will connect the dots from the second and third draws ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4), xend = (1:4^3) / (4^2), y = 2, yend = 3) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.25 0.0625 2 3 ## 2 0.25 0.125 2 3 ## 3 0.25 0.188 2 3 ## 4 0.25 0.25 2 3 ## 5 0.5 0.312 2 3 ## 6 0.5 0.375 2 3 ## 7 0.5 0.438 2 3 ## 8 0.5 0.5 2 3 ## 9 0.75 0.562 2 3 ## 10 0.75 0.625 2 3 ## # … with 54 more rows We can use the lines_1 and lines_2 data in the plot with two geom_segment() functions. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that. d &lt;- d %&gt;% mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) d ## # A tibble: 84 x 4 ## position draw fill denominator ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.5 1 b 0.5 ## 2 1.5 1 w 0.5 ## 3 2.5 1 w 0.5 ## 4 3.5 1 w 0.5 ## 5 0.125 2 b 0.125 ## 6 0.375 2 w 0.125 ## 7 0.625 2 w 0.125 ## 8 0.875 2 w 0.125 ## 9 1.12 2 b 0.125 ## 10 1.38 2 w 0.125 ## # … with 74 more rows We’ll follow the same logic for the lines_1 and lines_2 data. ( lines_1 &lt;- lines_1 %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 0.125 1 2 ## 2 0.5 0.375 1 2 ## 3 0.5 0.625 1 2 ## 4 0.5 0.875 1 2 ## 5 1.5 1.12 1 2 ## 6 1.5 1.38 1 2 ## 7 1.5 1.62 1 2 ## 8 1.5 1.88 1 2 ## 9 2.5 2.12 1 2 ## 10 2.5 2.38 1 2 ## 11 2.5 2.62 1 2 ## 12 2.5 2.88 1 2 ## 13 3.5 3.12 1 2 ## 14 3.5 3.38 1 2 ## 15 3.5 3.62 1 2 ## 16 3.5 3.88 1 2 ( lines_2 &lt;- lines_2 %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.0312 2 3 ## 2 0.125 0.0938 2 3 ## 3 0.125 0.156 2 3 ## 4 0.125 0.219 2 3 ## 5 0.375 0.281 2 3 ## 6 0.375 0.344 2 3 ## 7 0.375 0.406 2 3 ## 8 0.375 0.469 2 3 ## 9 0.625 0.531 2 3 ## 10 0.625 0.594 2 3 ## # … with 54 more rows Now the plot’s looking closer. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_y_continuous(breaks = 1:3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + theme(panel.grid.minor = element_blank(), legend.position = &quot;none&quot;) For the final step, we’ll use coord_polar() to change the coordinate system, giving the plot a mandala-like feel. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 4) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() To make our version of Figure 2.3, we’ll have to add an index to tell us which paths remain logically valid after each choice. We’ll call the index remain. lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0:1, times = c(1, 3)), rep(0, times = 4 * 3))) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d &lt;- d %&gt;% mutate(remain = c(rep(1:0, times = c(1, 3)), rep(0:1, times = c(1, 3)), rep(0, times = 4 * 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) # finally, the plot: d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, alpha = remain %&gt;% as.character()), shape = 21, size = 4) + # it&#39;s the alpha parameter that makes elements semitransparent scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() Letting “w” = a white dot and “b” = a blue dot, we might recreate the table in the middle of page 23 like so. # if we make two custom functions, here, it will simplify the code within `mutate()`, below n_blue &lt;- function(x){ rowSums(x == &quot;b&quot;) } n_white &lt;- function(x){ rowSums(x == &quot;w&quot;) } t &lt;- # for the first four columns, `p_` indexes position tibble(p_1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), p_2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), p_3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), p_4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(`draw 1: blue` = n_blue(.), `draw 2: white` = n_white(.), `draw 3: blue` = n_blue(.)) %&gt;% mutate(`ways to produce` = `draw 1: blue` * `draw 2: white` * `draw 3: blue`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 draw 1: blue draw 2: white draw 3: blue ways to produce w w w w 0 4 0 0 b w w w 1 3 1 3 b b w w 2 2 2 8 b b b w 3 1 3 9 b b b b 4 0 4 0 We’ll need new data for Figure 2.4. Here’s the initial primary data, d. d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3))) ( d &lt;- d %&gt;% bind_rows( d, d ) %&gt;% # here are the fill colors mutate(fill = c(rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), each = 2) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 1)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)))) %&gt;% # now we need to shift the positions over in accordance with draw, like before mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(position = ifelse(pie_index == &quot;a&quot;, position, ifelse(pie_index == &quot;b&quot;, position + 4, position + 4 * 2))) ) ## # A tibble: 252 x 5 ## position draw fill denominator pie_index ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 1 w 0.5 a ## 2 1.5 1 b 0.5 a ## 3 2.5 1 b 0.5 a ## 4 3.5 1 b 0.5 a ## 5 0.125 2 w 0.125 a ## 6 0.375 2 b 0.125 a ## 7 0.625 2 b 0.125 a ## 8 0.875 2 b 0.125 a ## 9 1.12 2 w 0.125 a ## 10 1.38 2 b 0.125 a ## # … with 242 more rows Both lines_1 and lines_2 require adjustments for x and xend. Our current approach is a nested ifelse(). Rather than copy and paste that multi-line ifelse() code for all four, let’s wrap it in a compact function, which we’ll call move_over(). move_over &lt;- function(position, index){ ifelse(index == &quot;a&quot;, position, ifelse(index == &quot;b&quot;, position + 4, position + 4 * 2) ) } If you’re new to making your own R functions, check out Chapter 19 of R4DS or Chapter 14 of R Programming for Data Science. Anyway, now we’ll make our new lines_1 and lines_2 data, for which we’ll use move_over() to adjust their x and xend positions to the correct spots. ( lines_1 &lt;- tibble(x = rep((1:4), each = 4) %&gt;% rep(., times = 3), xend = ((1:4^2) / 4) %&gt;% rep(., times = 3), y = 1, yend = 2) %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 48 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 0.125 1 2 a ## 2 0.5 0.375 1 2 a ## 3 0.5 0.625 1 2 a ## 4 0.5 0.875 1 2 a ## 5 1.5 1.12 1 2 a ## 6 1.5 1.38 1 2 a ## 7 1.5 1.62 1 2 a ## 8 1.5 1.88 1 2 a ## 9 2.5 2.12 1 2 a ## 10 2.5 2.38 1 2 a ## # … with 38 more rows ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4) %&gt;% rep(., times = 3), xend = (1:4^3 / 4^2) %&gt;% rep(., times = 3), y = 2, yend = 3) %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 192 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.125 0.0312 2 3 a ## 2 0.125 0.0938 2 3 a ## 3 0.125 0.156 2 3 a ## 4 0.125 0.219 2 3 a ## 5 0.375 0.281 2 3 a ## 6 0.375 0.344 2 3 a ## 7 0.375 0.406 2 3 a ## 8 0.375 0.469 2 3 a ## 9 0.625 0.531 2 3 a ## 10 0.625 0.594 2 3 a ## # … with 182 more rows For the last data wrangling step, we add the remain indices to help us determine which parts to make semitransparent. I’m not sure of a slick way to do this, so these are the result of brute force counting. d &lt;- d %&gt;% mutate(remain = c(# `pie_index == &quot;a&quot;` rep(0:1, times = c(1, 3)), rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), # `pie_index == &quot;b&quot;` rep(0:1, each = 2), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 2), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), # `pie_index == &quot;c&quot;` rep(0:1, times = c(3, 1)), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)) ) ) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 8), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) We’re finally ready to plot our Figure 2.4. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_vline(xintercept = c(0, 4, 8), color = &quot;white&quot;, size = 2/3) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, size = draw, alpha = remain %&gt;% as.character()), shape = 21) + scale_size_continuous(range = c(3, 1.5)) + scale_alpha_manual(values = c(1/10, 1)) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 12), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3.5), breaks = NULL) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) + coord_polar() 2.1.2 Using prior information. We may have prior information about the relative plausibility of each conjecture. This prior information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Or we might want to act as if we had prior information, so we can build conservatism into the analysis. Whatever the source, it would help to have a way to use prior information. Luckily there is a natural solution: Just multiply the prior count by the new count. (p. 25) Here’s the table in the middle of page 25. t &lt;- t %&gt;% rename(`previous counts` = `ways to produce`, `ways to produce` = `draw 1: blue`) %&gt;% select(p_1:p_4, `ways to produce`, `previous counts`) %&gt;% mutate(`new count` = `ways to produce` * `previous counts`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 ways to produce previous counts new count w w w w 0 0 0 b w w w 1 3 3 b b w w 2 8 16 b b b w 3 9 27 b b b b 4 0 0 We might update to reproduce the table a the top of page 26, like this. t &lt;- t %&gt;% select(p_1:p_4, `new count`) %&gt;% rename(`prior count` = `new count`) %&gt;% mutate(`factory count` = c(0, 3:0)) %&gt;% mutate(`new count` = `prior count` * `factory count`) t %&gt;% knitr::kable() p_1 p_2 p_3 p_4 prior count factory count new count w w w w 0 0 0 b w w w 3 3 9 b b w w 16 2 32 b b b w 27 1 27 b b b b 0 0 0 To learn more about dplyr::select() and dplyr::rename(), check out Baert’s exhaustive blog post Data Wrangling Part 1: Basic to Advanced Ways to Select Columns. 2.1.3 From counts to probability. The opening sentences in this subsection are important: “It is helpful to think of this strategy as adhering to a principle of honest ignorance: When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible” (p. 26, emphasis in the original). We can define our updated plausibility as: plausibility of after seeing \\(\\propto\\) ways can produce \\(\\times\\) prior plausibility of In other words: \\[ \\text{plausibility of } p \\text{ after } D_{\\text{new}} \\propto \\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p \\] But since we have to standardize the results to get them into a probability metric, the full equation is: \\[ \\text{plausibility of } p \\text{ after } D_{\\text{new}} = \\frac{\\text{ ways } p \\text{ can produce } D_{\\text{new}} \\times \\text{ prior plausibility of } p}{\\text{sum of the products}} \\] You might make the table in the middle of page 27 like this. t %&gt;% select(p_1:p_4) %&gt;% mutate(p = seq(from = 0, to = 1, by = .25), `ways to produce data` = c(0, 3, 8, 9, 0)) %&gt;% mutate(plausibility = `ways to produce data` / sum(`ways to produce data`)) ## # A tibble: 5 x 7 ## p_1 p_2 p_3 p_4 p `ways to produce data` plausibility ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 w w w w 0 0 0 ## 2 b w w w 0.25 3 0.15 ## 3 b b w w 0.5 8 0.4 ## 4 b b b w 0.75 9 0.45 ## 5 b b b b 1 0 0 We just computed the plausibilities, but here’s McElreath’s R code 2.1. ways &lt;- c(0, 3, 8, 9, 0) ways / sum(ways) ## [1] 0.00 0.15 0.40 0.45 0.00 2.2 Building a model We might save our globe-tossing data in a tibble. (d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;))) ## # A tibble: 9 x 1 ## toss ## &lt;chr&gt; ## 1 w ## 2 l ## 3 w ## 4 w ## 5 w ## 6 l ## 7 w ## 8 l ## 9 w 2.2.1 A data story. Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how come events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28, emphasis in the original) 2.2.2 Bayesian updating. Here we’ll add the cumulative number of trials, n_trials, and the cumulative number of successes, n_successes (i.e., toss == &quot;w&quot;), to the data. ( d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;)) ) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 Fair warning: We don’t learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview. sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) %&gt;% ungroup() %&gt;% mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), likelihood = dbinom(x = n_success, size = n_trials, prob = p_water), strip = str_c(&quot;n = &quot;, n_trials) ) %&gt;% # the next three lines allow us to normalize the prior and the likelihood, # putting them both in a probability metric group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% # plot! ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~strip, scales = &quot;free_y&quot;) If it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable. To learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. To learn about tidyr::expand(), go here. 2.2.3 Evaluate. It’s worth repeating the Rethinking: Deflationary statistics box, here. It may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we’d like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32) 2.3 Components of the model a likelihood function: “the number of ways each conjecture could produce an observation” one or more parameters: “the accumulated number of ways each conjecture cold produce the entire data” a prior: “the initial plausibility of each conjectured cause of the data” 2.3.1 Likelihood. If you let the count of water be \\(w\\) and the number of tosses be \\(n\\), then the binomial likelihood may be expressed as: \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Given a probability of .5, the binomial likelihood of 6 out of 9 tosses coming out water is: dbinom(x = 6, size = 9, prob = .5) ## [1] 0.1640625 McElreath suggested we change the values of prob. Let’s do so over the parameter space. tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;binomial likelihood&quot;) + theme(panel.grid = element_blank()) 2.3.2 Parameters. McElreath started off his Rethinking: Datum or parameter? box with: It is typical to conceive of data and parameters as completely different kinds of entities. Data are measures and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is fuzzy. (p. 34) For more in this topic, check out his lecture Understanding Bayesian Statistics without Frequentist Language. 2.3.3 Prior. So where do priors come from? They are engineering assumptions, chosen to help the machine learn. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior. You’ll see later in the book that priors that gently nudge the machine usually improve inference. Such priors are sometimes called regularizing or weakly informative priors. (p. 35) To learn more about “regularizing or weakly informative priors,” check out the Prior Choice Recommendations wiki from the Stan team. 2.3.3.1 Overthinking: Prior as a probability distribution McElreath said that “for a uniform prior from \\(a\\) to \\(b\\), the probability of any point in the interval is \\(1 / (b - a)\\)” (p. 35). Let’s try that out. To keep things simple, we’ll hold \\(a\\) constant while varying the values for \\(b\\). tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% mutate(prob = 1 / (b - a)) ## # A tibble: 5 x 3 ## a b prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 1 ## 2 0 1.5 0.667 ## 3 0 2 0.5 ## 4 0 3 0.333 ## 5 0 9 0.111 I like to verify things with plots. tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% expand(nesting(a, b), parameter_space = seq(from = 0, to = 9, length.out = 500)) %&gt;% mutate(prob = dunif(parameter_space, a, b), b = str_c(&quot;b = &quot;, b)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + scale_x_continuous(breaks = c(0, 1:3, 9)) + scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1), labels = c(&quot;0&quot;, &quot;1/9&quot;, &quot;1/3&quot;, &quot;1/2&quot;, &quot;2/3&quot;, &quot;1&quot;)) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) + facet_wrap(~b, ncol = 5) And as we’ll learn much later in the project, the \\(\\text{Uniform} (0, 1)\\) distribution is special in that we can also express it as the beta distribution for which \\(\\alpha = 1 \\text{ and } \\beta = 1\\). E.g., tibble(parameter_space = seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(prob = dbeta(parameter_space, 1, 1)) %&gt;% ggplot(aes(x = parameter_space, ymin = 0, ymax = prob)) + geom_ribbon() + coord_cartesian(ylim = 0:2) + theme(panel.grid = element_blank()) 2.3.4 Posterior. If we continue to focus on the globe tossing example, the posterior probability a toss will be water may be expressed as: \\[\\text{Pr} (p|w) = \\frac{\\text{Pr} (w|p) \\text{Pr} (p)}{\\text{Pr} (w)}\\] More generically and in words, this is: \\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Average Likelihood}}\\] 2.4 Making the model go Here’s the data wrangling for Figure 2.6. sequence_length &lt;- 1e3 d &lt;- tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% expand(probability, row = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)) %&gt;% arrange(row, probability) %&gt;% mutate(prior = ifelse(row == &quot;flat&quot;, 1, ifelse(row == &quot;stepped&quot;, rep(0:1, each = sequence_length / 2), exp(-abs(probability - .5) / .25) / ( 2 * .25))), likelihood = dbinom(x = 6, size = 9, prob = probability)) %&gt;% group_by(row) %&gt;% mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&gt;% gather(key, value, -probability, -row) %&gt;% ungroup() %&gt;% mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), row = factor(row, levels = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;))) To learn more about dplyr::arrange(), chech out R4DS, Chapter 5.3. In order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately and then recombine them with gridExtra::grid.arrange(). p1 &lt;- d %&gt;% filter(key == &quot;prior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;prior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p2 &lt;- d %&gt;% filter(key == &quot;likelihood&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;likelihood&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) p3 &lt;- d %&gt;% filter(key == &quot;posterior&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;posterior&quot;) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1) library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) I’m not sure if it’s the same McElreath used in the text, but the formula I used for the tirangle-shaped prior is the Laplace distribution with a location of .5 and a dispersion of .25. Also, to learn all about dplyr::filter(), check out Baert’s Data Wrangling Part 3: Basic and more advanced ways to filter rows. 2.4.1 Grid approximation. We just employed grid approximation over the last figure. In order to get nice smooth lines, we computed the posterior over 1000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20. (d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% # compute likelihood at each value in grid mutate(unstd_posterior = likelihood * prior) %&gt;% # compute product of likelihood and prior mutate(posterior = unstd_posterior / sum(unstd_posterior)) # standardize the posterior, so it sums to 1 ) ## # A tibble: 20 x 5 ## p_grid prior likelihood unstd_posterior posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 0.0526 1 0.00000152 0.00000152 0.000000799 ## 3 0.105 1 0.0000819 0.0000819 0.0000431 ## 4 0.158 1 0.000777 0.000777 0.000409 ## 5 0.211 1 0.00360 0.00360 0.00189 ## 6 0.263 1 0.0112 0.0112 0.00587 ## 7 0.316 1 0.0267 0.0267 0.0140 ## 8 0.368 1 0.0529 0.0529 0.0279 ## 9 0.421 1 0.0908 0.0908 0.0478 ## 10 0.474 1 0.138 0.138 0.0728 ## 11 0.526 1 0.190 0.190 0.0999 ## 12 0.579 1 0.236 0.236 0.124 ## 13 0.632 1 0.267 0.267 0.140 ## 14 0.684 1 0.271 0.271 0.143 ## 15 0.737 1 0.245 0.245 0.129 ## 16 0.789 1 0.190 0.190 0.0999 ## 17 0.842 1 0.118 0.118 0.0621 ## 18 0.895 1 0.0503 0.0503 0.0265 ## 19 0.947 1 0.00885 0.00885 0.00466 ## 20 1 1 0 0 0 Here’s the right panel of Figure 2.7. d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Here it is with just 5 points, the left hand panel of Figure 2.7. tibble(p_grid = seq(from = 0, to = 1, length.out = 5), prior = 1) %&gt;% mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% mutate(unstd_posterior = likelihood * prior) %&gt;% mutate(posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) 2.4.2 Quadratic approximation. Apply the quadratic approximation to the globe tossing data with rethinking::map(). library(rethinking) globe_qa &lt;- rethinking::map( alist( w ~ dbinom(9, p), # binomial likelihood p ~ dunif(0, 1) # uniform prior ), data = list(w = 6)) # display summary of quadratic approximation precis(globe_qa) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.16 0.42 0.92 In preparation for Figure 2.8, here’s the model with \\(n = 18\\) and \\(n = 36\\). globe_qa_18 &lt;- rethinking::map( alist( w ~ dbinom(9 * 2, p), p ~ dunif(0, 1) ), data = list(w = 6 *2)) globe_qa_36 &lt;- rethinking::map( alist( w ~ dbinom(9 * 4, p), p ~ dunif(0, 1) ), data = list(w = 6 * 4)) precis(globe_qa_18) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.11 0.49 0.84 precis(globe_qa_36) ## Mean StdDev 5.5% 94.5% ## p 0.67 0.08 0.54 0.79 Here’s the legwork for Figure 2.8. n_grid &lt;- 100 tibble(p_grid = seq(from = 0, to = 1, length.out = n_grid) %&gt;% rep(., times = 3), prior = 1, w = rep(c(6, 12, 24), each = n_grid), n = rep(c(9, 18, 36), each = n_grid), m = .67, s = rep(c(.16, .11, .08), each = n_grid)) %&gt;% mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %&gt;% mutate(unstd_grid_posterior = likelihood * prior, unstd_quad_posterior = dnorm(p_grid, m, s)) %&gt;% group_by(w) %&gt;% mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior), quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior), n = str_c(&quot;n = &quot;, n)) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 9&quot;, &quot;n = 18&quot;, &quot;n = 36&quot;))) %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = grid_posterior)) + geom_line(aes(y = quad_posterior), color = &quot;grey50&quot;) + labs(x = &quot;proportion water&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~n, scales = &quot;free&quot;) 2.4.3 Markov chain Monte Carlo. Since the main goal of this project is to highlight brms, we may as fit a model. This seems like an appropriately named subsection to do so. First we’ll have to load the package. library(brms) Here we’ll re-fit the last model from above wherein \\(w = 24\\) and \\(n = 36\\). globe_qa_brms &lt;- brm(data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 1, prior(beta(1, 1), class = Intercept), iter = 4000, warmup = 1000, control = list(adapt_delta = .9), seed = 4) The model output looks like so. print(globe_qa_brms) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 0.66 0.08 0.50 0.80 3579 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). There’s a lot going on in that output, which we’ll start to clarify in Chapter 4. For now, focus on the ‘Intercept’ line. As we’ll also learn in Chapter 4, the intercept of a regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial, \\(\\theta\\). Let’s plot the results of our model and compare them with those from rethinking::map(), above. posterior_samples(globe_qa_brms) %&gt;% mutate(n = &quot;n = 36&quot;) %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(fill = &quot;black&quot;) + labs(x = &quot;proportion water&quot;) + xlim(0, 1) + theme(panel.grid = element_blank()) + facet_wrap(~n) If you’re still confused. Cool. This is just a preview. We’ll start walking through fitting models in brms in Chapter 4 and we’ll learn a lot about regression with the binomial likelihood in Chapter 10. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.9.0 Rcpp_1.0.1 rethinking_1.59 rstan_2.18.2 StanHeaders_2.18.1 ## [6] gridExtra_2.3 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 purrr_0.3.2 ## [11] readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 rprojroot_1.3-2 ## [5] markdown_1.0 base64enc_0.1-3 rstudioapi_0.10 DT_0.7 ## [9] fansi_0.4.0 mvtnorm_1.0-10 lubridate_1.7.4 xml2_1.2.0 ## [13] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 zeallot_0.1.0 ## [17] bayesplot_1.7.0 jsonlite_1.6 broom_0.5.2 shiny_1.3.2 ## [21] compiler_3.6.0 httr_1.4.0 backports_1.1.4 assertthat_0.2.1 ## [25] Matrix_1.2-17 lazyeval_0.2.2 cli_1.1.0 later_0.8.0 ## [29] htmltools_0.3.6 prettyunits_1.0.2 tools_3.6.0 igraph_1.2.4.1 ## [33] coda_0.19-2 gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [37] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-140 crosstalk_1.0.0 ## [41] xfun_0.7 ps_1.3.0 rvest_0.3.4 mime_0.7 ## [45] miniUI_0.1.1.1 pacman_0.5.1 gtools_3.8.1 MASS_7.3-51.4 ## [49] zoo_1.8-6 scales_1.0.0 colourpicker_1.0 hms_0.4.2 ## [53] promises_1.0.1 Brobdingnag_1.2-6 inline_0.3.15 shinystan_2.5.0 ## [57] yaml_2.2.0 loo_2.1.0 stringi_1.4.3 highr_0.8 ## [61] dygraphs_1.1.1.6 pkgbuild_1.0.3 rlang_0.3.4 pkgconfig_2.0.2 ## [65] matrixStats_0.54.0 evaluate_0.14 lattice_0.20-38 rstantools_1.5.1 ## [69] htmlwidgets_1.3 labeling_0.3 processx_3.3.1 tidyselect_0.2.5 ## [73] plyr_1.8.4 magrittr_1.5 bookdown_0.11 R6_2.4.0 ## [77] generics_0.0.2 pillar_1.4.1 haven_2.1.0 withr_2.1.2 ## [81] xts_0.11-2 abind_1.4-5 modelr_0.1.4 crayon_1.3.4 ## [85] utf8_1.1.4 rmarkdown_1.13 grid_3.6.0 readxl_1.3.1 ## [89] callr_3.2.0 threejs_0.3.1 digest_0.6.19 xtable_1.8-4 ## [93] httpuv_1.5.1 stats4_3.6.0 munsell_0.5.0 shinyjs_1.0 "],
["sampling-the-imaginary.html", "Chapter 3 Sampling the Imaginary 3.1 Sampling from a grid-like approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Summary Let’s practice in brms Reference Session info", " Chapter 3 Sampling the Imaginary If you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute \\[\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}}\\] We’ll do so within a tibble. library(tidyverse) tibble(pr_positive_vampire = .95, pr_positive_mortal = .01, pr_vampire = .001) %&gt;% mutate(pr_positive = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.01 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive &lt;dbl&gt; 0.01094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 Here’s the other way of tackling the vampire problem, this time useing the frequency format. tibble(pr_vampire = 100 / 100000, pr_positive_vampire = 95 / 100, pr_positive_mortal = 99 / 99900) %&gt;% mutate(pr_positive = 95 + 999) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %&gt;% glimpse() ## Observations: 1 ## Variables: 5 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.000990991 ## $ pr_positive &lt;dbl&gt; 1094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 3.1 Sampling from a grid-like approximate posterior Here we use grid approximation, again, to generate samples. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows Now we’ll use the dplyr::sample_n() function to sample rows from d, saving them as sample. # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) glimpse(samples) ## Observations: 10,000 ## Variables: 4 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… We’ll plot the zigzagging left panel of Figure 3.1 with geom_line(). But before we do, we’ll need to add a variable numbering the samples. samples %&gt;% mutate(sample_number = 1:n()) %&gt;% ggplot(aes(x = sample_number, y = p_grid)) + geom_line(size = 1/10) + labs(x = &quot;sample number&quot;, y = &quot;proportion of water (p)&quot;) We’ll make the density in the right panel with geom_density(). samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;black&quot;) + coord_cartesian(xlim = 0:1) + xlab(&quot;proportion of water (p)&quot;) 3.2 Sampling to summarize “Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly now it is summarized depends upon your purpose” (p. 53). 3.2.1 Intervals of defined boundaries. To get the proportion of water less than some value of p_grid within the tidyverse, you’d first filter() by that value and then take the sum() within summarise(). d %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = sum(posterior)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.171 To learn more about dplyr::summarise() and related functions, check out Baert’s Data Wrangling Part 4: Summarizing and slicing your data and Chapter 5.6 of R4DS. If what you want is a frequency based on filtering by samples, then you might use n() within summarise(). samples %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.162 You can use &amp; within filter(), too. samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.602 3.2.2 Intervals of defined mass. We’ll create the upper two panels for Figure 3.2 with geom_line(), geom_ribbon(), and a some careful filtering. # upper left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # upper right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + # note this next line is the only difference in code from the last plot geom_ribbon(data = d %&gt;% filter(p_grid &lt; .75 &amp; p_grid &gt; .5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ll come back for the lower two panels in a bit. Since we’ve saved our p_grid samples within the well-named samples tibble, we’ll have to index with $ within quantile. (q_80 &lt;- quantile(samples$p_grid, prob = .8)) ## 80% ## 0.763 That value will come in handy for the lower left panel of Figure 3.2, so we saved it. But anyways, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile(): samples %&gt;% select(p_grid) %&gt;% pull() %&gt;% quantile(prob = .8) ## 80% ## 0.763 And we might also use quantile() within summarise(). samples %&gt;% summarise(`80th percentile` = quantile(p_grid, p = .8)) ## # A tibble: 1 x 1 ## `80th percentile` ## &lt;dbl&gt; ## 1 0.763 Here’s the summarise() approach with two probabilities: samples %&gt;% summarise(`10th percentile` = quantile(p_grid, p = .1), `90th percentile` = quantile(p_grid, p = .9)) ## # A tibble: 1 x 2 ## `10th percentile` `90th percentile` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.452 0.814 The tydiverse approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base R quantile() shines. (q_10_and_90 &lt;- quantile(samples$p_grid, prob = c(.1, .9))) ## 10% 90% ## 0.4520 0.8141 Now we have our cutoff values saved as q_80 and q_10_and_90, we’re ready to make the bottom panels of Figure 3.2. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &lt; q_80), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;lower 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; q_10_and_90[1] &amp; p_grid &lt; q_10_and_90[2]), aes(ymin = 0, ymax = posterior)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;middle 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) We’ve already defined p_grid and prior within d, above. Here we’ll reuse them and update the rest of the columns. # here we update the `dbinom()` parameters n_success &lt;- 3 n_trials &lt;- 3 # update `d` d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(posterior)) # make the next part reproducible set.seed(3) # here&#39;s our new samples tibble ( samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) ) ## # A tibble: 10,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.716 1 0.367 0.367 ## 2 0.651 1 0.276 0.276 ## 3 0.547 1 0.164 0.164 ## 4 0.999 1 0.997 0.997 ## 5 0.99 1 0.970 0.970 ## 6 0.787 1 0.487 0.487 ## 7 0.94 1 0.831 0.831 ## 8 0.817 1 0.545 0.545 ## 9 0.955 1 0.871 0.871 ## 10 0.449 1 0.0905 0.0905 ## # … with 9,990 more rows The rethinking::PI() function works like a nice shorthand for quantile(). quantile(samples$p_grid, prob = c(.25, .75)) ## 25% 75% ## 0.709 0.935 rethinking::PI(samples$p_grid, prob = .5) ## 25% 75% ## 0.709 0.935 Now’s a good time to introduce Matthew Kay’s tidybayes package, which offers an array of convenience functions for Bayesian models of the type we’ll be working with in this project. library(tidybayes) median_qi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.843 0.709 0.935 0.5 median qi The tidybayes package offers a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals–just like we’ve been doing with quantile(). Note how the .width argument within median_qi() worked the same way the prob argument did within rethinking::PI(). With .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals. median_qi(samples$p_grid, .width = c(.5, .8, .99)) ## y ymin ymax .width .point .interval ## 1 0.843 0.709000 0.935 0.50 median qi ## 2 0.843 0.570000 0.975 0.80 median qi ## 3 0.843 0.260985 0.999 0.99 median qi The .width column in the output indexed which line presented which interval. Now let’s use the rethinking::HPDI() function to return 50% highest posterior density intervals (HPDIs). rethinking::HPDI(samples$p_grid, prob = .5) ## |0.5 0.5| ## 0.842 0.999 The reason I introduce tidybayes now is that the functions of the brms package only support percentile-based intervals of the type we computed with quantile() and median_qi(). But tidybayes also supports HPDIs. mode_hdi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.9562951 0.842 0.999 0.5 mode hdi This time we used the mode as the measure of central tendency. With this family of tidybayes functions, you specify the measure of central tendency in the prefix (i.e., mean, median, or mode) and then the type of interval you’d like (i.e., qi or hdi). If all you want are the intervals without the measure of central tendency or all that other technical information, tidybayes also offers the handy qi() and hdi() functions. qi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.709 0.935 hdi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.842 0.999 These are nice in that they yield simple numeric vectors, making them particularly useful to use as references within ggplot2. Now we have that skill, we can use it to make Figure 3.3. # lower left panel d %&gt;% ggplot(aes(x = p_grid)) + # check out our sweet `qi()` indexing geom_ribbon(data = d %&gt;% filter(p_grid &gt; qi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; qi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% Percentile Interval&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = d %&gt;% filter(p_grid &gt; hdi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; hdi(samples$p_grid, .width = .5)[2]), aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;50% HPDI&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) 3.2.3 Point estimates. We’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate. d %&gt;% arrange(desc(posterior)) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 ## 2 0.999 1 0.997 0.997 ## 3 0.998 1 0.994 0.994 ## 4 0.997 1 0.991 0.991 ## 5 0.996 1 0.988 0.988 ## 6 0.995 1 0.985 0.985 ## 7 0.994 1 0.982 0.982 ## 8 0.993 1 0.979 0.979 ## 9 0.992 1 0.976 0.976 ## 10 0.991 1 0.973 0.973 ## # … with 991 more rows To emphasize it, we can use slice() to select the top row. d %&gt;% arrange(desc(posterior)) %&gt;% slice(1) ## # A tibble: 1 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 Or we could use the handy dplyr::top_n() function. d %&gt;% select(posterior) %&gt;% top_n(n = 1) ## Selecting by posterior ## # A tibble: 1 x 1 ## posterior ## &lt;dbl&gt; ## 1 1 We can get th emode with mode_hdi() or mode_qi(). samples %&gt;% mode_hdi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.477 1 0.95 mode hdi samples %&gt;% mode_qi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.401 0.994 0.95 mode qi But if all you want is the mode itself, you can just use tidybayes::Mode(). Mode(samples$p_grid) ## [1] 0.9562951 But medians and means are typical, too. samples %&gt;% summarise(mean = mean(p_grid), median = median(p_grid)) ## # A tibble: 1 x 2 ## mean median ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 0.843 We can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble. ( point_estimates &lt;- bind_rows( samples %&gt;% mean_qi(p_grid), samples %&gt;% median_qi(p_grid), samples %&gt;% mode_qi(p_grid) ) %&gt;% select(p_grid, .point) %&gt;% # these last two columns will help us annotate mutate(x = p_grid + c(-.03, .03, -.03), y = c(.1, .25, .4)) ) ## # A tibble: 3 x 4 ## p_grid .point x y ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 mean 0.773 0.1 ## 2 0.843 median 0.873 0.25 ## 3 0.956 mode 0.926 0.4 The plot: d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$p_grid) + geom_text(data = point_estimates, aes(x = x, y = y, label = .point), angle = 90) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) As it turns out “different loss functions imply different point estimates” (p. 59, emphasis in the original). Let \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(p - d\\). If we decide \\(d = .5\\), then our expected loss will be: d %&gt;% mutate(loss = posterior * abs(0.5 - p_grid)) %&gt;% summarise(`expected loss` = sum(loss)) ## # A tibble: 1 x 1 ## `expected loss` ## &lt;dbl&gt; ## 1 78.4 What McElreath did with sapply(), we’ll do with purrr::map(). If you haven’t used it, map() is part of a family of similarly-named functions (e.g., map2()) from the purrr package, which is itself part of the tidyverse. The map() family is the tidyverse alternative to the family of apply() functions from the base R framework. You can learn more about how to use the map() family here or here or here. make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * abs(our_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } ( l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() ) ## # A tibble: 1,001 x 2 ## decision weighted_average_loss ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 201. ## 2 0.001 200. ## 3 0.002 200. ## 4 0.003 200. ## 5 0.004 199. ## 6 0.005 199. ## 7 0.006 199. ## 8 0.007 199. ## 9 0.008 198. ## 10 0.009 198. ## # … with 991 more rows Now we’re ready for the right panel of Figure 3.4. # this will help us find the x and y coordinates for the minimum value min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) We saved the exact minimum value as min_loss[1], which is 0.841. Within sampling error, this is the posterior median as depicted by our samples. samples %&gt;% summarise(posterior_median = median(p_grid)) ## # A tibble: 1 x 1 ## posterior_median ## &lt;dbl&gt; ## 1 0.843 The quadratic loss \\((d - p)^2\\) suggests we should use the mean instead. Let’s investigate. # ammend our loss function make_loss &lt;- function(our_d){ d %&gt;% mutate(loss = posterior * (our_d - p_grid)^2) %&gt;% summarise(weighted_average_loss = sum(loss)) } # remake our `l` data l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest() # update to the new minimum loss coordinates min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # update the plot l %&gt;% ggplot(aes(x = decision)) + geom_ribbon(aes(ymin = 0, ymax = weighted_average_loss), fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) Based on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.8. Within sampling error, this is the posterior mean of our samples. samples %&gt;% summarise(posterior_meaan = mean(p_grid)) ## # A tibble: 1 x 1 ## posterior_meaan ## &lt;dbl&gt; ## 1 0.803 3.3 Sampling to simulate prediction McElreath’s four good reasons for posterior simulation were: Model checking Software validation Research design Forecasting 3.3.1 Dummy data. Dummy data for the globe tossing model arise from the binomial likelihood. If you let \\(w\\) be a count of water and \\(n\\) be the number of tosses, the binomial likelihood is \\[\\text{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}\\] Letting \\(n = 2\\), \\(p(w) = .7\\), and \\(w_\\text{observed} = 0 \\text{ through }2\\), the denisties are: tibble(n = 2, probability = .7, w = 0:2) %&gt;% mutate(density = dbinom(w, size = n, prob = probability)) ## # A tibble: 3 x 4 ## n probability w density ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 0.7 0 0.09 ## 2 2 0.7 1 0.42 ## 3 2 0.7 2 0.490 If we’re going to simulate, we should probably set our seed. Doing so makes the results reproducible. set.seed(3) rbinom(1, size = 2, prob = .7) ## [1] 2 Here are ten reproducible draws. set.seed(3) rbinom(10, size = 2, prob = .7) ## [1] 2 1 2 2 1 1 2 2 1 1 Now generate 100,000 (i.e., 1e5) reproducible dummy observations. # how many would you like? n_draws &lt;- 1e5 set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 2, prob = .7)) d %&gt;% group_by(draws) %&gt;% count() %&gt;% mutate(proportion = n / nrow(d)) ## # A tibble: 3 x 3 ## # Groups: draws [3] ## draws n proportion ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 9000 0.09 ## 2 1 42051 0.421 ## 3 2 48949 0.489 As McElreath mused in the text, those simulated proportion values are very close to the analytically calculated values in our density column a few code blocks up. Here’s the simulation updated so \\(n = 9\\), which we plot in our version of Figure 3.5. set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 9, prob = .7)) # the histogram d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) McElreath suggested we play around with different values of size and prob. With the next block of code, we’ll simulate nine conditions. n_draws &lt;- 1e5 simulate_binom &lt;- function(n, probability){ set.seed(3) rbinom(n_draws, size = n, prob = probability) } d &lt;- tibble(n = c(3, 6, 9)) %&gt;% expand(n, probability = c(.3, .6, .9)) %&gt;% mutate(draws = map2(n, probability, simulate_binom)) %&gt;% ungroup() %&gt;% mutate(n = str_c(&quot;n = &quot;, n), probability = str_c(&quot;p = &quot;, probability)) %&gt;% unnest() head(d) ## # A tibble: 6 x 3 ## n probability draws ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 n = 3 p = 0.3 0 ## 2 n = 3 p = 0.3 2 ## 3 n = 3 p = 0.3 1 ## 4 n = 3 p = 0.3 0 ## 5 n = 3 p = 0.3 1 ## 6 n = 3 p = 0.3 1 The results look as follows: d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_grid(n ~ probability) 3.3.2 Model checking. If you’re new to applied statistics, you might be surprised how often mistakes arise. 3.3.2.1 Did the software work? Let this haunt your dreams: “There is no way to really be sure that software works correctly” (p. 64). If you’d like to dive deeper into these dark waters, check out one my favorite talks from StanCon 2018, Esther Williams in the Harold Holt Memorial Swimming Pool, by the ineffable Dan Simpson. If Simpson doesn’t end up drowning you, see Gabry and Simpson’s talk at the Royal Statistical Society 2018, Visualization in Bayesian workflow, a follow-up blog Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it, and that blog’s associated pre-print by Vehtari, Gelman, Simpson, Carpenter, and Bürkner Rank-normalization, folding, and localization: An improved Rˆ for assessing convergence of MCMC. 3.3.2.2 Is the model adequate? The implied predictions of the model are uncertain in two ways, and it’s important to be aware of both. First, there is observation uncertainty. For any unique value of the parameter \\(p\\), there is a unique implied pattern of observations that the model expects. These patterns of observations are the same gardens of forking data that you explored in the previous chapter. These patterns are also what you sampled in the previous section. There is uncertainty in the predicted observations, because even if you know \\(p\\) with certainty, you won’t know the next globe toss with certainty (unless \\(p = 0\\) or \\(p = 1\\)). Second, there is uncertainty about \\(p\\). The posterior distribution over \\(p\\) embodies this uncertainty. And since there is uncertainty about \\(p\\), there is uncertainty about everything that depends upon \\(p\\). The uncertainty in \\(p\\) will interact with the sampling variation, when we try to assess what the model tells us about outcomes. We’d like to propagate the parameter uncertainty–carry it forward–as we evaluate the implied predictions. All that is required is averaging over the posterior density for \\(p\\), while computing the predictions. For each possible value of the parameter \\(p\\), there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of \\(p\\), then you could average all of these prediction distributions together, using the posterior probabilities of each value of \\(p\\), to get a posterior predictive distribution. (p. 56, emphasis in the original) All this is depicted in Figure 3.6. To get ready to make our version, let’s first refresh our original grid approximation d. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows We can make our version of the top of Figure 3.6 with a little tricky filtering. d %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), color = &quot;grey67&quot;, fill = &quot;grey67&quot;) + geom_segment(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(xend = p_grid, y = 0, yend = posterior, size = posterior), color = &quot;grey33&quot;, show.legend = F) + geom_point(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(y = posterior)) + annotate(geom = &quot;text&quot;, x = .08, y = .0025, label = &quot;Posterior probability&quot;) + scale_size_continuous(range = c(0, 1)) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0:10) / 10) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Note how we weighted the widths of the vertical lines by the posterior density. We’ll need to do a bit of wrangling before we’re ready to make the plot in the middle panel of Figure 3.6. n_draws &lt;- 1e5 simulate_binom &lt;- function(probability){ set.seed(3) rbinom(n_draws, size = 9, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) %&gt;% mutate(label = str_c(&quot;p = &quot;, probability)) head(d_small) ## # A tibble: 6 x 3 ## probability draws label ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.1 0 p = 0.1 ## 2 0.1 2 p = 0.1 ## 3 0.1 0 p = 0.1 ## 4 0.1 0 p = 0.1 ## 5 0.1 1 p = 0.1 ## 6 0.1 1 p = 0.1 Now we’re ready to plot. d_small %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Sampling distributions&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_wrap(~ label, ncol = 9) To make the plot at the bottom of Figure 3.6, we’ll redefine our samples, this time including the w variable (see the R code 3.26 block in the text). # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% sample_n(size = n_samples, weight = posterior, replace = T) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) glimpse(samples) ## Observations: 10,000 ## Variables: 5 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, … ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708… ## $ w &lt;dbl&gt; 4, 7, 3, 3, 7, 6, 8, 2, 6, 4, 5, 5, 8, 6, 4, 6, 8, 2, 6, 9, 9, 7, 4, 8, 9, 8, … Here’s our histogram. samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = 0:9, ylim = 0:3000) + theme(panel.grid = element_blank()) In Figure 3.7, McElreath considered the longst sequence of the sampe values. We’ve been using rbinom() with the size parameter set to 9 for our simulations. E.g., rbinom(10, size = 9, prob = .6) ## [1] 7 5 6 8 7 5 6 3 3 4 Notice this collapses (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this rbinom(9, size = 1, prob = .6) ## [1] 0 1 1 1 0 0 0 0 0 would be the disaggregated version of just one of the numerals returned by rbinom() when size = 9. So let’s try simulating again with un-aggregated samples. We’ll keep adding to our samples tibble. In addition to the disaggregated draws based on the \\(p\\) values listed in p_grid, we’ll also want to add a row index for each of those p_grid values–it’ll come in handy when we plot. # make it reproducible set.seed(3) samples &lt;- samples %&gt;% mutate(iter = 1:n(), draws = purrr::map(p_grid, rbinom, n = 9, size = 1)) %&gt;% unnest(draws) glimpse(samples) ## Observations: 90,000 ## Variables: 7 ## $ p_grid &lt;dbl&gt; 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.651, 0.651, 0… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ likelihood &lt;dbl&gt; 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0… ## $ posterior &lt;dbl&gt; 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, … ## $ w &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ draws &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, … The main action is in the draws column. Now we have to count the longest sequences. The base R rle() function will help with that. Consider McElreath’s sequence of tosses. tosses &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) You can plug that into rle(). rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; For our purposes, we’re interested in lengths. That tells us the length of each sequences of the same value. The 3 corresponds to our run of three ws. The max() function will help us confirm it’s the largest value. rle(tosses)$lengths %&gt;% max() ## [1] 3 Now let’s apply our method to the data and plot. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% max()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 3), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;longest run length&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Let’s look at rle() again. rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; We can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, “w” and “l”. rle(tosses)$lengths %&gt;% length() ## [1] 7 With that new trick, we’re ready to make the right panel of Figure 3.7. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% length()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 6), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of switches&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 3.4 Summary Let’s practice in brms Open brms. library(brms) In brms, we’ll fit the primary model of \\(w = 6\\) and \\(n = 9\\) much like we did at the end of the project for Chapter 2. b3.1 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 1, # this is a flat prior prior(beta(1, 1), class = Intercept), seed = 3, control = list(adapt_delta = .999)) We’ll learn more about the beta distribution in Chapter 11. But for now, here’s the posterior summary for b_Intercept, the probability of a “w”. posterior_summary(b3.1)[&quot;b_Intercept&quot;, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 0.64 0.14 0.36 0.88 As we’ll fully cover in the next chapter, Estimate is the posterior mean, the two Q columns are the quantile-based 95% intervals, and Est.Error is the posterior standard deviation. Much like the way we used the samples() function to simulate probability values, above, we can do so with fitted() within the brms framework. But we will have to specify scale = &quot;linear&quot; in order to return results in the probability metric. By default, brms::fitted() will return summary information. Since we want actual simulation draws, we’ll specify summary = F. f &lt;- fitted(b3.1, summary = F, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(&quot;p&quot;) glimpse(f) ## Observations: 4,000 ## Variables: 1 ## $ p &lt;dbl&gt; 0.6920484, 0.5559454, 0.6096088, 0.5305334, 0.4819733, 0.6724561, 0.6402367, 0.8356569,… By default, we have a generically-named vector V1 of 4000 samples. We’ll explain the defaults in later chapters. For now, notice we can view these in a density. f %&gt;% ggplot(aes(x = p)) + geom_density(fill = &quot;grey50&quot;, color = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = .08, y = 2.5, label = &quot;Posterior probability&quot;) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0, .5, 1), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Looks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of w counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6. # the simulation set.seed(3) f &lt;- f %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) # the plot f %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3), limits = c(0, 9)) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1200)) + ggtitle(&quot;Posterior predictive distribution&quot;) + theme(panel.grid = element_blank()) As you might imagine, we can use the output from fitted() to return disaggregated batches of 0s and 1s, too. And we could even use those disaggregated 0s and 1s to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I’ll leave those as exercises for the interested reader. Reference McElreath, R. (2016). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman &amp; Hall/CRC Press. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 18.04.2 LTS ## ## Matrix products: default ## BLAS: /opt/R/3.6.0/lib/R/lib/libRblas.so ## LAPACK: /opt/R/3.6.0/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C LC_TIME=en_US.UTF-8 ## [4] LC_COLLATE=en_US.UTF-8 LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C LC_ADDRESS=C ## [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] rethinking_1.59 rstan_2.18.2 StanHeaders_2.18.1 gridExtra_2.3 brms_2.9.0 ## [6] Rcpp_1.0.1 tidybayes_1.1.0 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.1 ## [11] purrr_0.3.2 readr_1.3.1 tidyr_0.8.3 tibble_2.1.3 ggplot2_3.1.1 ## [16] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 rsconnect_0.8.13 ## [4] rprojroot_1.3-2 ggstance_0.3.1 markdown_1.0 ## [7] base64enc_0.1-3 rstudioapi_0.10 svUnit_0.7-12 ## [10] DT_0.7 fansi_0.4.0 mvtnorm_1.0-10 ## [13] lubridate_1.7.4 xml2_1.2.0 codetools_0.2-16 ## [16] bridgesampling_0.6-0 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 bayesplot_1.7.0 jsonlite_1.6 ## [22] broom_0.5.2 shiny_1.3.2 compiler_3.6.0 ## [25] httr_1.4.0 backports_1.1.4 assertthat_0.2.1 ## [28] Matrix_1.2-17 lazyeval_0.2.2 cli_1.1.0 ## [31] later_0.8.0 htmltools_0.3.6 prettyunits_1.0.2 ## [34] tools_3.6.0 igraph_1.2.4.1 coda_0.19-2 ## [37] gtable_0.3.0 glue_1.3.1 reshape2_1.4.3 ## [40] cellranger_1.1.0 vctrs_0.1.0 nlme_3.1-140 ## [43] crosstalk_1.0.0 xfun_0.7 ps_1.3.0 ## [46] rvest_0.3.4 mime_0.7 miniUI_0.1.1.1 ## [49] pacman_0.5.1 gtools_3.8.1 MASS_7.3-51.4 ## [52] zoo_1.8-6 scales_1.0.0 colourpicker_1.0 ## [55] hms_0.4.2 promises_1.0.1 Brobdingnag_1.2-6 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [61] loo_2.1.0 stringi_1.4.3 highr_0.8 ## [64] dygraphs_1.1.1.6 pkgbuild_1.0.3 rlang_0.3.4 ## [67] pkgconfig_2.0.2 matrixStats_0.54.0 HDInterval_0.2.0 ## [70] evaluate_0.14 lattice_0.20-38 rstantools_1.5.1 ## [73] htmlwidgets_1.3 labeling_0.3 processx_3.3.1 ## [76] tidyselect_0.2.5 plyr_1.8.4 magrittr_1.5 ## [79] bookdown_0.11 R6_2.4.0 generics_0.0.2 ## [82] pillar_1.4.1 haven_2.1.0 withr_2.1.2 ## [85] xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [88] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [91] rmarkdown_1.13 grid_3.6.0 readxl_1.3.1 ## [94] callr_3.2.0 threejs_0.3.1 digest_0.6.19 ## [97] xtable_1.8-4 httpuv_1.5.1 stats4_3.6.0 ## [100] munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.0 "]
]
